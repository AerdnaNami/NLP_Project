{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50954c1e718c44218ed25540e18a82db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ee23aa9aca5483fbd729252a3458b54",
              "IPY_MODEL_b732f4fa88574c0ba5cfbdf00fba252a",
              "IPY_MODEL_a8e05b42cc2440008d87d82d2213fbd9"
            ],
            "layout": "IPY_MODEL_c2ed9e3e38484620b079b3960ee41272"
          }
        },
        "5ee23aa9aca5483fbd729252a3458b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c36fae75f9e4e6990ae8cdcd5bbd8a9",
            "placeholder": "​",
            "style": "IPY_MODEL_da01f12084b54ae29c002b9fe7bacd95",
            "value": "README.md: 100%"
          }
        },
        "b732f4fa88574c0ba5cfbdf00fba252a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7adc5cdfdf54827b31c598b44e4b837",
            "max": 9639,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6fd9bf8e1ef4f16bfe52c6e3f60e0b8",
            "value": 9639
          }
        },
        "a8e05b42cc2440008d87d82d2213fbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba04064598014ecc836bb9a3974060c7",
            "placeholder": "​",
            "style": "IPY_MODEL_b651e26a10f54d48b1a5334f692ccccb",
            "value": " 9.64k/9.64k [00:00&lt;00:00, 108kB/s]"
          }
        },
        "c2ed9e3e38484620b079b3960ee41272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c36fae75f9e4e6990ae8cdcd5bbd8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da01f12084b54ae29c002b9fe7bacd95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7adc5cdfdf54827b31c598b44e4b837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fd9bf8e1ef4f16bfe52c6e3f60e0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba04064598014ecc836bb9a3974060c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b651e26a10f54d48b1a5334f692ccccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb67edf9e3294ed982ef77b1858a289b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85f64bf6a9b94882ac1348b415787fe8",
              "IPY_MODEL_c0b61f7ea6114412ad7e45e29d5e822f",
              "IPY_MODEL_627aaa4a1f8a4c688aa8accdfd59953b"
            ],
            "layout": "IPY_MODEL_4a0900f5a75e4839bc82f9ef15eec778"
          }
        },
        "85f64bf6a9b94882ac1348b415787fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73da26e4774640999268ed2c047838a2",
            "placeholder": "​",
            "style": "IPY_MODEL_c84034311c88422793674d0310353fc8",
            "value": "qasper.py: 100%"
          }
        },
        "c0b61f7ea6114412ad7e45e29d5e822f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0754e3c2506646c3a7dacde4dce8e7b9",
            "max": 5950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f78aa1c4bacb4939a299622b289181e4",
            "value": 5950
          }
        },
        "627aaa4a1f8a4c688aa8accdfd59953b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce27af0d76f4db0b82cccef004eacc8",
            "placeholder": "​",
            "style": "IPY_MODEL_8e32730d699c41569191036a3dde7a9c",
            "value": " 5.95k/5.95k [00:00&lt;00:00, 45.8kB/s]"
          }
        },
        "4a0900f5a75e4839bc82f9ef15eec778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73da26e4774640999268ed2c047838a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84034311c88422793674d0310353fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0754e3c2506646c3a7dacde4dce8e7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78aa1c4bacb4939a299622b289181e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ce27af0d76f4db0b82cccef004eacc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e32730d699c41569191036a3dde7a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acaa8bf3d02d4002ac620e0983ea9a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_750f5c3536b342c08becad6123e9a482",
              "IPY_MODEL_f642251a6f8d46a3af592c1875b54e44",
              "IPY_MODEL_466a79af5fb84ae58f2c88f1d4f4508d"
            ],
            "layout": "IPY_MODEL_668ffde7216f47dabbd4d8822e23b099"
          }
        },
        "750f5c3536b342c08becad6123e9a482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63420932dd1f48b498156b0f87a607c0",
            "placeholder": "​",
            "style": "IPY_MODEL_666c8ede8d794cbda8299f64ffa795b9",
            "value": "0000.parquet: 100%"
          }
        },
        "f642251a6f8d46a3af592c1875b54e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af50f2545b244baa5bc3552df937c1a",
            "max": 14374550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a0e13a526734211a14daafdb235150e",
            "value": 14374550
          }
        },
        "466a79af5fb84ae58f2c88f1d4f4508d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f6afb88cea4347a54a056023beb02b",
            "placeholder": "​",
            "style": "IPY_MODEL_1d9946260dae4e6daf3892785744d873",
            "value": " 14.4M/14.4M [00:00&lt;00:00, 19.9MB/s]"
          }
        },
        "668ffde7216f47dabbd4d8822e23b099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63420932dd1f48b498156b0f87a607c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666c8ede8d794cbda8299f64ffa795b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4af50f2545b244baa5bc3552df937c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a0e13a526734211a14daafdb235150e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65f6afb88cea4347a54a056023beb02b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9946260dae4e6daf3892785744d873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e0a03faab02426892f304e6857704e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_130dbd8b16cd4c30a7a282fa43d2f289",
              "IPY_MODEL_989cf6f786ee435099b6b1e3b3c85b2b",
              "IPY_MODEL_5841e3364b6d426f86f48eeaecc99eca"
            ],
            "layout": "IPY_MODEL_0db4beda1c6a4d7a82889406b779d234"
          }
        },
        "130dbd8b16cd4c30a7a282fa43d2f289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec73454f16945ee87cae7358f8c3cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_a496352a0cf049608c4de2c590614742",
            "value": "0000.parquet: 100%"
          }
        },
        "989cf6f786ee435099b6b1e3b3c85b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c7fa56c23954544837422581f2afff6",
            "max": 4749127,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dce9c010bcd04d76bd1ca209fb03bc8b",
            "value": 4749127
          }
        },
        "5841e3364b6d426f86f48eeaecc99eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399ce36d2cbd44dd8061a1e309e9708e",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf3a9c1a5b941df87a78e08860625fe",
            "value": " 4.75M/4.75M [00:00&lt;00:00, 40.6MB/s]"
          }
        },
        "0db4beda1c6a4d7a82889406b779d234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec73454f16945ee87cae7358f8c3cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a496352a0cf049608c4de2c590614742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c7fa56c23954544837422581f2afff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce9c010bcd04d76bd1ca209fb03bc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "399ce36d2cbd44dd8061a1e309e9708e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf3a9c1a5b941df87a78e08860625fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "424c2b94287645589204bc9818890428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bd2c52cd2bf4d72a5daf9b5a6f2400d",
              "IPY_MODEL_c2df925a8691422f863766841c3afde3",
              "IPY_MODEL_2bc47a6cefdd4d4bad3a5f6c47416974"
            ],
            "layout": "IPY_MODEL_7e6de5fb4a034022be5eaab21c394437"
          }
        },
        "5bd2c52cd2bf4d72a5daf9b5a6f2400d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f3852ad7e6f4557864697171ede7a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_bb0fc07abf9d43b69959d0ee16f67eac",
            "value": "0000.parquet: 100%"
          }
        },
        "c2df925a8691422f863766841c3afde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda5f75aa337474ca7820b1c633cd4d7",
            "max": 7074873,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17d2841c29b2499598eb067cd81422a4",
            "value": 7074873
          }
        },
        "2bc47a6cefdd4d4bad3a5f6c47416974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5d8fe3b75f34dbea0e157e84b573a24",
            "placeholder": "​",
            "style": "IPY_MODEL_e449d6f969be4818804626c6df2db250",
            "value": " 7.07M/7.07M [00:00&lt;00:00, 60.9MB/s]"
          }
        },
        "7e6de5fb4a034022be5eaab21c394437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3852ad7e6f4557864697171ede7a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb0fc07abf9d43b69959d0ee16f67eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda5f75aa337474ca7820b1c633cd4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d2841c29b2499598eb067cd81422a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5d8fe3b75f34dbea0e157e84b573a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e449d6f969be4818804626c6df2db250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21488077c2c04b7c8feb1a38c0aa0974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d4fda123b6c4f28a35ddc4e924d5a29",
              "IPY_MODEL_e46ce7cc708d4a3aa3171d78eec49237",
              "IPY_MODEL_2d7eacdf1fab4d7b85c6c6378011a84c"
            ],
            "layout": "IPY_MODEL_a5d4c246ba284f59990522196fe56c8b"
          }
        },
        "2d4fda123b6c4f28a35ddc4e924d5a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7e459e1efb5481a9400d3d7d008d895",
            "placeholder": "​",
            "style": "IPY_MODEL_a81a94e5cc294233bdeaf9dd9141d9fa",
            "value": "Generating train split: 100%"
          }
        },
        "e46ce7cc708d4a3aa3171d78eec49237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b54ceaa287e4bf4979091d3cc68641a",
            "max": 888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa6fcd5202ef4781ae748e188c96a6ef",
            "value": 888
          }
        },
        "2d7eacdf1fab4d7b85c6c6378011a84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db8b27111f55452080a7435c8b55080e",
            "placeholder": "​",
            "style": "IPY_MODEL_7c9d2be257a0447aa6695dd0046a73b7",
            "value": " 888/888 [00:00&lt;00:00, 1514.64 examples/s]"
          }
        },
        "a5d4c246ba284f59990522196fe56c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e459e1efb5481a9400d3d7d008d895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81a94e5cc294233bdeaf9dd9141d9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b54ceaa287e4bf4979091d3cc68641a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6fcd5202ef4781ae748e188c96a6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db8b27111f55452080a7435c8b55080e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9d2be257a0447aa6695dd0046a73b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73bb155352214ac59f37f4af84a242b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e66ca3e125754d22a86534c48359f93b",
              "IPY_MODEL_1b666afca4b747b1a29cef08acd0ec5d",
              "IPY_MODEL_43d347b432c24e39844952adfe4bdaca"
            ],
            "layout": "IPY_MODEL_f93e4cb7c6844019a88f83cd094ee8f1"
          }
        },
        "e66ca3e125754d22a86534c48359f93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a105cdfd1e14ba7a74cd9016a5ffc02",
            "placeholder": "​",
            "style": "IPY_MODEL_9f99480e938142b7b68cf4955be403ed",
            "value": "Generating validation split: 100%"
          }
        },
        "1b666afca4b747b1a29cef08acd0ec5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116e969642a841d8ad18e5b31f97ce23",
            "max": 281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40423376ce8f4719ab919349078c2eaf",
            "value": 281
          }
        },
        "43d347b432c24e39844952adfe4bdaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b82121f00654dad92195093ec7b2ab4",
            "placeholder": "​",
            "style": "IPY_MODEL_ed19a01ea3754391bfd94c0851b6389a",
            "value": " 281/281 [00:00&lt;00:00, 1176.71 examples/s]"
          }
        },
        "f93e4cb7c6844019a88f83cd094ee8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a105cdfd1e14ba7a74cd9016a5ffc02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f99480e938142b7b68cf4955be403ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116e969642a841d8ad18e5b31f97ce23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40423376ce8f4719ab919349078c2eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b82121f00654dad92195093ec7b2ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed19a01ea3754391bfd94c0851b6389a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd8ba9aa8e644272a641d53753848159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ada6fa99ec99420d9979d92a9ba946d2",
              "IPY_MODEL_87b5c2dceada4f7cb48b94f323aa0e76",
              "IPY_MODEL_c3ad5d8089ef4b4196c040537826ccc6"
            ],
            "layout": "IPY_MODEL_dc7e5f43b1614104b4bca76263aa7018"
          }
        },
        "ada6fa99ec99420d9979d92a9ba946d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738827a95eaf4a158c7a74f916d02c08",
            "placeholder": "​",
            "style": "IPY_MODEL_d013bb4232bd41f2b8b0bf6fb3733d8f",
            "value": "Generating test split: 100%"
          }
        },
        "87b5c2dceada4f7cb48b94f323aa0e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a54394e5f1540a097650a325f489adc",
            "max": 416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5ff1986b65e4d859b9f0d291f013010",
            "value": 416
          }
        },
        "c3ad5d8089ef4b4196c040537826ccc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47fe4fba6ff145dab2c0b7e6e1ee7e1a",
            "placeholder": "​",
            "style": "IPY_MODEL_4b5756a36c9f49febbed66fe040336cb",
            "value": " 416/416 [00:00&lt;00:00, 904.62 examples/s]"
          }
        },
        "dc7e5f43b1614104b4bca76263aa7018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738827a95eaf4a158c7a74f916d02c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d013bb4232bd41f2b8b0bf6fb3733d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a54394e5f1540a097650a325f489adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ff1986b65e4d859b9f0d291f013010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47fe4fba6ff145dab2c0b7e6e1ee7e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5756a36c9f49febbed66fe040336cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Retriever:\n",
        "1. Build TF-IDF and BM25 matrix based on training set\n",
        "2. Test on test set\n",
        "3. Evaluate: Recall@k, Precision@k"
      ],
      "metadata": {
        "id": "aQGIC76wNO7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBTQjYIiOr63",
        "outputId": "1e042e16-cf64-4a3b-c09d-a8c05b9dd5df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"allenai/qasper\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "50954c1e718c44218ed25540e18a82db",
            "5ee23aa9aca5483fbd729252a3458b54",
            "b732f4fa88574c0ba5cfbdf00fba252a",
            "a8e05b42cc2440008d87d82d2213fbd9",
            "c2ed9e3e38484620b079b3960ee41272",
            "3c36fae75f9e4e6990ae8cdcd5bbd8a9",
            "da01f12084b54ae29c002b9fe7bacd95",
            "c7adc5cdfdf54827b31c598b44e4b837",
            "b6fd9bf8e1ef4f16bfe52c6e3f60e0b8",
            "ba04064598014ecc836bb9a3974060c7",
            "b651e26a10f54d48b1a5334f692ccccb",
            "fb67edf9e3294ed982ef77b1858a289b",
            "85f64bf6a9b94882ac1348b415787fe8",
            "c0b61f7ea6114412ad7e45e29d5e822f",
            "627aaa4a1f8a4c688aa8accdfd59953b",
            "4a0900f5a75e4839bc82f9ef15eec778",
            "73da26e4774640999268ed2c047838a2",
            "c84034311c88422793674d0310353fc8",
            "0754e3c2506646c3a7dacde4dce8e7b9",
            "f78aa1c4bacb4939a299622b289181e4",
            "2ce27af0d76f4db0b82cccef004eacc8",
            "8e32730d699c41569191036a3dde7a9c",
            "acaa8bf3d02d4002ac620e0983ea9a3c",
            "750f5c3536b342c08becad6123e9a482",
            "f642251a6f8d46a3af592c1875b54e44",
            "466a79af5fb84ae58f2c88f1d4f4508d",
            "668ffde7216f47dabbd4d8822e23b099",
            "63420932dd1f48b498156b0f87a607c0",
            "666c8ede8d794cbda8299f64ffa795b9",
            "4af50f2545b244baa5bc3552df937c1a",
            "2a0e13a526734211a14daafdb235150e",
            "65f6afb88cea4347a54a056023beb02b",
            "1d9946260dae4e6daf3892785744d873",
            "7e0a03faab02426892f304e6857704e8",
            "130dbd8b16cd4c30a7a282fa43d2f289",
            "989cf6f786ee435099b6b1e3b3c85b2b",
            "5841e3364b6d426f86f48eeaecc99eca",
            "0db4beda1c6a4d7a82889406b779d234",
            "7ec73454f16945ee87cae7358f8c3cb9",
            "a496352a0cf049608c4de2c590614742",
            "9c7fa56c23954544837422581f2afff6",
            "dce9c010bcd04d76bd1ca209fb03bc8b",
            "399ce36d2cbd44dd8061a1e309e9708e",
            "6bf3a9c1a5b941df87a78e08860625fe",
            "424c2b94287645589204bc9818890428",
            "5bd2c52cd2bf4d72a5daf9b5a6f2400d",
            "c2df925a8691422f863766841c3afde3",
            "2bc47a6cefdd4d4bad3a5f6c47416974",
            "7e6de5fb4a034022be5eaab21c394437",
            "8f3852ad7e6f4557864697171ede7a3a",
            "bb0fc07abf9d43b69959d0ee16f67eac",
            "bda5f75aa337474ca7820b1c633cd4d7",
            "17d2841c29b2499598eb067cd81422a4",
            "c5d8fe3b75f34dbea0e157e84b573a24",
            "e449d6f969be4818804626c6df2db250",
            "21488077c2c04b7c8feb1a38c0aa0974",
            "2d4fda123b6c4f28a35ddc4e924d5a29",
            "e46ce7cc708d4a3aa3171d78eec49237",
            "2d7eacdf1fab4d7b85c6c6378011a84c",
            "a5d4c246ba284f59990522196fe56c8b",
            "a7e459e1efb5481a9400d3d7d008d895",
            "a81a94e5cc294233bdeaf9dd9141d9fa",
            "1b54ceaa287e4bf4979091d3cc68641a",
            "fa6fcd5202ef4781ae748e188c96a6ef",
            "db8b27111f55452080a7435c8b55080e",
            "7c9d2be257a0447aa6695dd0046a73b7",
            "73bb155352214ac59f37f4af84a242b0",
            "e66ca3e125754d22a86534c48359f93b",
            "1b666afca4b747b1a29cef08acd0ec5d",
            "43d347b432c24e39844952adfe4bdaca",
            "f93e4cb7c6844019a88f83cd094ee8f1",
            "4a105cdfd1e14ba7a74cd9016a5ffc02",
            "9f99480e938142b7b68cf4955be403ed",
            "116e969642a841d8ad18e5b31f97ce23",
            "40423376ce8f4719ab919349078c2eaf",
            "2b82121f00654dad92195093ec7b2ab4",
            "ed19a01ea3754391bfd94c0851b6389a",
            "dd8ba9aa8e644272a641d53753848159",
            "ada6fa99ec99420d9979d92a9ba946d2",
            "87b5c2dceada4f7cb48b94f323aa0e76",
            "c3ad5d8089ef4b4196c040537826ccc6",
            "dc7e5f43b1614104b4bca76263aa7018",
            "738827a95eaf4a158c7a74f916d02c08",
            "d013bb4232bd41f2b8b0bf6fb3733d8f",
            "6a54394e5f1540a097650a325f489adc",
            "f5ff1986b65e4d859b9f0d291f013010",
            "47fe4fba6ff145dab2c0b7e6e1ee7e1a",
            "4b5756a36c9f49febbed66fe040336cb"
          ]
        },
        "id": "s5JrNUGBM8UL",
        "outputId": "fcd765df-e6d6-4ac4-ffa0-61d718b4de56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/9.64k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50954c1e718c44218ed25540e18a82db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "qasper.py:   0%|          | 0.00/5.95k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb67edf9e3294ed982ef77b1858a289b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0000.parquet:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acaa8bf3d02d4002ac620e0983ea9a3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0000.parquet:   0%|          | 0.00/4.75M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e0a03faab02426892f304e6857704e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0000.parquet:   0%|          | 0.00/7.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "424c2b94287645589204bc9818890428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/888 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21488077c2c04b7c8feb1a38c0aa0974"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/281 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73bb155352214ac59f37f4af84a242b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/416 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd8ba9aa8e644272a641d53753848159"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_qa_dataframe(ds, se):\n",
        "    # Lists to store the data\n",
        "    titles = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    list_of_full_papers = []\n",
        "    relevant_paragraphs = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for id in range(len(ds[se])):\n",
        "        paragraphs = []\n",
        "        title = ds[se]['title'][id]\n",
        "        for section in ds[se][id]['full_text']['paragraphs']:\n",
        "            for paragraph in section:\n",
        "                paragraphs.append(paragraph)\n",
        "        # Iterate through questions for each document\n",
        "        for i in range(len(ds[se][id]['qas']['question'])):\n",
        "            # Iterate through answers for each question\n",
        "            for j in range(len(ds[se][id]['qas']['answers'][i]['answer'])):\n",
        "                # Check if there are extractive spans\n",
        "                if len(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans']) > 0:\n",
        "                    titles.append(title)\n",
        "                    questions.append(ds[se][id]['qas']['question'][i])\n",
        "                    answers.append(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans'])\n",
        "                    list_of_full_papers.append(paragraphs)\n",
        "\n",
        "\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'title': titles,\n",
        "        'question': questions,\n",
        "        'answer': answers,\n",
        "        'full_paper': list_of_full_papers,\n",
        "\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage example:\n",
        "se = 'train'\n",
        "df = create_qa_dataframe(ds, se)"
      ],
      "metadata": {
        "id": "PCLzXr1RRXrH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6G6WpGhRfRPC",
        "outputId": "cb6d14fb-ea72-43fb-8a54-9cd315df8577"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  title  \\\n",
              "0     Minimally Supervised Learning of Affective Eve...   \n",
              "1     Minimally Supervised Learning of Affective Eve...   \n",
              "2     Minimally Supervised Learning of Affective Eve...   \n",
              "3     PO-EMO: Conceptualization, Annotation, and Mod...   \n",
              "4     PO-EMO: Conceptualization, Annotation, and Mod...   \n",
              "...                                                 ...   \n",
              "1358  Back to the Future -- Sequential Alignment of ...   \n",
              "1359  Analyzing Language Learned by an Active Questi...   \n",
              "1360  Deep Semi-Supervised Learning with Linguistica...   \n",
              "1361  Deep Semi-Supervised Learning with Linguistica...   \n",
              "1362  Open-Vocabulary Semantic Parsing with both Dis...   \n",
              "\n",
              "                                               question  \\\n",
              "0                             What is the seed lexicon?   \n",
              "1     What are labels available in dataset for super...   \n",
              "2            How large is raw corpus used for training?   \n",
              "3           How is the annotation experiment evaluated?   \n",
              "4           What are the aesthetic emotions formalized?   \n",
              "...                                                 ...   \n",
              "1358  What are three challenging tasks authors evalu...   \n",
              "1359  What is the difference in findings of Buck et ...   \n",
              "1360  What is the unsupervised task in the final layer?   \n",
              "1361                How many supervised tasks are used?   \n",
              "1362                   What knowledge base do they use?   \n",
              "\n",
              "                                                 answer  \\\n",
              "0     [seed lexicon consists of positive and negativ...   \n",
              "1                                  [negative, positive]   \n",
              "2                               [100 million sentences]   \n",
              "3     [confusion matrices of labels between annotators]   \n",
              "4     [feelings of suspense experienced in narrative...   \n",
              "...                                                 ...   \n",
              "1358  [paper acceptance prediction, Named Entity Rec...   \n",
              "1359  [AQA diverges from well structured language in...   \n",
              "1360                                [Language Modeling]   \n",
              "1361                                              [two]   \n",
              "1362                                         [Freebase]   \n",
              "\n",
              "                                             full_paper  \n",
              "0     [Affective events BIBREF0 are events that typi...  \n",
              "1     [Affective events BIBREF0 are events that typi...  \n",
              "2     [Affective events BIBREF0 are events that typi...  \n",
              "3     [1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...  \n",
              "4     [1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...  \n",
              "...                                                 ...  \n",
              "1358  [As time passes, language usage changes. For e...  \n",
              "1359  [ BIBREF0 propose a reinforcement learning fra...  \n",
              "1360  [It is natural to think of NLP tasks existing ...  \n",
              "1361  [It is natural to think of NLP tasks existing ...  \n",
              "1362  [Semantic parsing is the task of mapping a phr...  \n",
              "\n",
              "[1363 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79084575-6e6c-4c04-995d-11570362da45\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>full_paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>What is the seed lexicon?</td>\n",
              "      <td>[seed lexicon consists of positive and negativ...</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>What are labels available in dataset for super...</td>\n",
              "      <td>[negative, positive]</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>How large is raw corpus used for training?</td>\n",
              "      <td>[100 million sentences]</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PO-EMO: Conceptualization, Annotation, and Mod...</td>\n",
              "      <td>How is the annotation experiment evaluated?</td>\n",
              "      <td>[confusion matrices of labels between annotators]</td>\n",
              "      <td>[1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PO-EMO: Conceptualization, Annotation, and Mod...</td>\n",
              "      <td>What are the aesthetic emotions formalized?</td>\n",
              "      <td>[feelings of suspense experienced in narrative...</td>\n",
              "      <td>[1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>Back to the Future -- Sequential Alignment of ...</td>\n",
              "      <td>What are three challenging tasks authors evalu...</td>\n",
              "      <td>[paper acceptance prediction, Named Entity Rec...</td>\n",
              "      <td>[As time passes, language usage changes. For e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>Analyzing Language Learned by an Active Questi...</td>\n",
              "      <td>What is the difference in findings of Buck et ...</td>\n",
              "      <td>[AQA diverges from well structured language in...</td>\n",
              "      <td>[ BIBREF0 propose a reinforcement learning fra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>Deep Semi-Supervised Learning with Linguistica...</td>\n",
              "      <td>What is the unsupervised task in the final layer?</td>\n",
              "      <td>[Language Modeling]</td>\n",
              "      <td>[It is natural to think of NLP tasks existing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>Deep Semi-Supervised Learning with Linguistica...</td>\n",
              "      <td>How many supervised tasks are used?</td>\n",
              "      <td>[two]</td>\n",
              "      <td>[It is natural to think of NLP tasks existing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>Open-Vocabulary Semantic Parsing with both Dis...</td>\n",
              "      <td>What knowledge base do they use?</td>\n",
              "      <td>[Freebase]</td>\n",
              "      <td>[Semantic parsing is the task of mapping a phr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1363 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79084575-6e6c-4c04-995d-11570362da45')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79084575-6e6c-4c04-995d-11570362da45 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79084575-6e6c-4c04-995d-11570362da45');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a37ed12-3005-474b-9776-98d1cb0202fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a37ed12-3005-474b-9776-98d1cb0202fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a37ed12-3005-474b-9776-98d1cb0202fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9bbb0aa5-14e6-4384-bda1-c374a289ebdc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9bbb0aa5-14e6-4384-bda1-c374a289ebdc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1363,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 659,\n        \"samples\": [\n          \"Deep Contextualized Acoustic Representations For Semi-Supervised Speech Recognition\",\n          \"\\\"What is Relevant in a Text Document?\\\": An Interpretable Machine Learning Approach\",\n          \"Mapping (Dis-)Information Flow about the MH17 Plane Crash\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1255,\n        \"samples\": [\n          \"What are multilingual models that were outperformed in performed experiment?\",\n          \"what pitfalls are mentioned in the paper?\",\n          \"How many comments were used?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_paper\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_qa_dataframe(ds, se):\n",
        "    # Lists to store the data\n",
        "    titles = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    list_of_full_papers = []\n",
        "    relevant_paragraphs = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for id in range(len(ds[se])):\n",
        "        paragraphs = []\n",
        "        title = ds[se]['title'][id]\n",
        "        for section in ds[se][id]['full_text']['paragraphs']:\n",
        "            for paragraph in section:\n",
        "                paragraphs.append(paragraph)\n",
        "\n",
        "        # Iterate through questions for each document\n",
        "        for i in range(len(ds[se][id]['qas']['question'])):\n",
        "            # Iterate through answers for each question\n",
        "            for j in range(len(ds[se][id]['qas']['answers'][i]['answer'])):\n",
        "                # Check if there are extractive spans\n",
        "                if len(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans']) > 0:\n",
        "                    answer_span = ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans'][0]\n",
        "\n",
        "                    # Find the paragraph containing the answer\n",
        "                    #relevant_para = None\n",
        "                    #for para in paragraphs:\n",
        "                        #if answer_span in para:\n",
        "                            #relevant_para = para\n",
        "                            #break\n",
        "\n",
        "                    titles.append(title)\n",
        "                    questions.append(ds[se][id]['qas']['question'][i])\n",
        "                    answers.append(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans'])\n",
        "                    list_of_full_papers.append(paragraphs)\n",
        "                    relevant_paragraphs.append(ds[se][id]['qas']['answers'][i]['answer'][j]['evidence'])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'title': titles,\n",
        "        'question': questions,\n",
        "        'answer': answers,\n",
        "        'full_paper': list_of_full_papers,\n",
        "        'relevant_paragraphs': relevant_paragraphs\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage example:\n",
        "se = 'train'\n",
        "df = create_qa_dataframe(ds, se)"
      ],
      "metadata": {
        "id": "47uT5_BGfTZF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "-R7sCctcj3RT",
        "outputId": "19746f8c-d06d-4231-e997-8be87850ab9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  title  \\\n",
              "0     Minimally Supervised Learning of Affective Eve...   \n",
              "1     Minimally Supervised Learning of Affective Eve...   \n",
              "2     Minimally Supervised Learning of Affective Eve...   \n",
              "3     PO-EMO: Conceptualization, Annotation, and Mod...   \n",
              "4     PO-EMO: Conceptualization, Annotation, and Mod...   \n",
              "...                                                 ...   \n",
              "1358  Back to the Future -- Sequential Alignment of ...   \n",
              "1359  Analyzing Language Learned by an Active Questi...   \n",
              "1360  Deep Semi-Supervised Learning with Linguistica...   \n",
              "1361  Deep Semi-Supervised Learning with Linguistica...   \n",
              "1362  Open-Vocabulary Semantic Parsing with both Dis...   \n",
              "\n",
              "                                               question  \\\n",
              "0                             What is the seed lexicon?   \n",
              "1     What are labels available in dataset for super...   \n",
              "2            How large is raw corpus used for training?   \n",
              "3           How is the annotation experiment evaluated?   \n",
              "4           What are the aesthetic emotions formalized?   \n",
              "...                                                 ...   \n",
              "1358  What are three challenging tasks authors evalu...   \n",
              "1359  What is the difference in findings of Buck et ...   \n",
              "1360  What is the unsupervised task in the final layer?   \n",
              "1361                How many supervised tasks are used?   \n",
              "1362                   What knowledge base do they use?   \n",
              "\n",
              "                                                 answer  \\\n",
              "0     [seed lexicon consists of positive and negativ...   \n",
              "1                                  [negative, positive]   \n",
              "2                               [100 million sentences]   \n",
              "3     [confusion matrices of labels between annotators]   \n",
              "4     [feelings of suspense experienced in narrative...   \n",
              "...                                                 ...   \n",
              "1358  [paper acceptance prediction, Named Entity Rec...   \n",
              "1359  [AQA diverges from well structured language in...   \n",
              "1360                                [Language Modeling]   \n",
              "1361                                              [two]   \n",
              "1362                                         [Freebase]   \n",
              "\n",
              "                                             full_paper  \\\n",
              "0     [Affective events BIBREF0 are events that typi...   \n",
              "1     [Affective events BIBREF0 are events that typi...   \n",
              "2     [Affective events BIBREF0 are events that typi...   \n",
              "3     [1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...   \n",
              "4     [1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...   \n",
              "...                                                 ...   \n",
              "1358  [As time passes, language usage changes. For e...   \n",
              "1359  [ BIBREF0 propose a reinforcement learning fra...   \n",
              "1360  [It is natural to think of NLP tasks existing ...   \n",
              "1361  [It is natural to think of NLP tasks existing ...   \n",
              "1362  [Semantic parsing is the task of mapping a phr...   \n",
              "\n",
              "                                    relevant_paragraphs  \n",
              "0     [The seed lexicon consists of positive and neg...  \n",
              "1     [Affective events BIBREF0 are events that typi...  \n",
              "2     [As a raw corpus, we used a Japanese web corpu...  \n",
              "3     [We find that Cohen $\\kappa $ agreement ranges...  \n",
              "4     [To emotionally move readers is considered a p...  \n",
              "...                                                 ...  \n",
              "1358  [We consider three tasks representing a broad ...  \n",
              "1359  [Here we perform a qualitative analysis of thi...  \n",
              "1360  [In our model we represent linguistically moti...  \n",
              "1361  [FLOAT SELECTED: Figure 1: Our Hierarchical Ne...  \n",
              "1362  [Much recent work on semantic parsing has been...  \n",
              "\n",
              "[1363 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e48b22ec-5e40-4f1a-9748-b0161c582fd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>full_paper</th>\n",
              "      <th>relevant_paragraphs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>What is the seed lexicon?</td>\n",
              "      <td>[seed lexicon consists of positive and negativ...</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "      <td>[The seed lexicon consists of positive and neg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>What are labels available in dataset for super...</td>\n",
              "      <td>[negative, positive]</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Minimally Supervised Learning of Affective Eve...</td>\n",
              "      <td>How large is raw corpus used for training?</td>\n",
              "      <td>[100 million sentences]</td>\n",
              "      <td>[Affective events BIBREF0 are events that typi...</td>\n",
              "      <td>[As a raw corpus, we used a Japanese web corpu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PO-EMO: Conceptualization, Annotation, and Mod...</td>\n",
              "      <td>How is the annotation experiment evaluated?</td>\n",
              "      <td>[confusion matrices of labels between annotators]</td>\n",
              "      <td>[1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...</td>\n",
              "      <td>[We find that Cohen $\\kappa $ agreement ranges...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PO-EMO: Conceptualization, Annotation, and Mod...</td>\n",
              "      <td>What are the aesthetic emotions formalized?</td>\n",
              "      <td>[feelings of suspense experienced in narrative...</td>\n",
              "      <td>[1.1em, 1.1.1em, 1.1.1.1em, Thomas Haider$^{1,...</td>\n",
              "      <td>[To emotionally move readers is considered a p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>Back to the Future -- Sequential Alignment of ...</td>\n",
              "      <td>What are three challenging tasks authors evalu...</td>\n",
              "      <td>[paper acceptance prediction, Named Entity Rec...</td>\n",
              "      <td>[As time passes, language usage changes. For e...</td>\n",
              "      <td>[We consider three tasks representing a broad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>Analyzing Language Learned by an Active Questi...</td>\n",
              "      <td>What is the difference in findings of Buck et ...</td>\n",
              "      <td>[AQA diverges from well structured language in...</td>\n",
              "      <td>[ BIBREF0 propose a reinforcement learning fra...</td>\n",
              "      <td>[Here we perform a qualitative analysis of thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>Deep Semi-Supervised Learning with Linguistica...</td>\n",
              "      <td>What is the unsupervised task in the final layer?</td>\n",
              "      <td>[Language Modeling]</td>\n",
              "      <td>[It is natural to think of NLP tasks existing ...</td>\n",
              "      <td>[In our model we represent linguistically moti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>Deep Semi-Supervised Learning with Linguistica...</td>\n",
              "      <td>How many supervised tasks are used?</td>\n",
              "      <td>[two]</td>\n",
              "      <td>[It is natural to think of NLP tasks existing ...</td>\n",
              "      <td>[FLOAT SELECTED: Figure 1: Our Hierarchical Ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>Open-Vocabulary Semantic Parsing with both Dis...</td>\n",
              "      <td>What knowledge base do they use?</td>\n",
              "      <td>[Freebase]</td>\n",
              "      <td>[Semantic parsing is the task of mapping a phr...</td>\n",
              "      <td>[Much recent work on semantic parsing has been...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1363 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e48b22ec-5e40-4f1a-9748-b0161c582fd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e48b22ec-5e40-4f1a-9748-b0161c582fd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e48b22ec-5e40-4f1a-9748-b0161c582fd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eb014ca8-5608-4ec0-8455-4f204499d766\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb014ca8-5608-4ec0-8455-4f204499d766')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eb014ca8-5608-4ec0-8455-4f204499d766 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d1b3b6d7-54e6-48ef-b340-70bd4ecf0ddb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d1b3b6d7-54e6-48ef-b340-70bd4ecf0ddb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1363,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 659,\n        \"samples\": [\n          \"Deep Contextualized Acoustic Representations For Semi-Supervised Speech Recognition\",\n          \"\\\"What is Relevant in a Text Document?\\\": An Interpretable Machine Learning Approach\",\n          \"Mapping (Dis-)Information Flow about the MH17 Plane Crash\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1255,\n        \"samples\": [\n          \"What are multilingual models that were outperformed in performed experiment?\",\n          \"what pitfalls are mentioned in the paper?\",\n          \"How many comments were used?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_paper\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevant_paragraphs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw84IubVoCIh",
        "outputId": "e281e34a-edd4-4126-adbb-82d437cc669d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "# Load your training data\n",
        "# Assuming the training data is in a CSV file with a 'full_paper' column\n",
        "# Replace 'train_data.csv' with your actual file path if needed\n",
        "train_data = df\n",
        "\n",
        "# Prepare the corpus of paragraphs for TF-IDF\n",
        "# Flatten each 'full_paper' entry into individual paragraphs by splitting on double newlines\n",
        "corpus = []\n",
        "for paper in train_data['full_paper']:\n",
        "    for paragraph in paper:\n",
        "      if paragraph == '':\n",
        "        continue\n",
        "      else:\n",
        "        corpus.append(paragraph)\n",
        "    #paragraphs = paper.split('\\n\\n')  # Splitting each paper into paragraphs\n",
        "    #corpus.extend(paragraphs)  # Add each paragraph to the corpus list\n",
        "\n",
        "# Initialize and fit the TF-IDF vectorizer on the entire training corpus\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Save the vectorizer and matrix for later evaluation\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "with open('tfidf_matrix.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_matrix, f)\n",
        "\n",
        "print(\"TF-IDF model has been created and saved. You can now use it for retrieval on the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQQ702S8tAmK",
        "outputId": "e4805e8f-5bbf-4e96-ee03-9b3a7bd14d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF model has been created and saved. You can now use it for retrieval on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_qa_dataframe(ds, se):\n",
        "    # Lists to store the data\n",
        "    titles = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    list_of_full_papers = []\n",
        "    relevant_paragraphs = []\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    for id in range(len(ds[se])):\n",
        "        paragraphs = []\n",
        "        title = ds[se]['title'][id]\n",
        "        for section in ds[se][id]['full_text']['paragraphs']:\n",
        "            for paragraph in section:\n",
        "                paragraphs.append(paragraph)\n",
        "\n",
        "        # Iterate through questions for each document\n",
        "        for i in range(len(ds[se][id]['qas']['question'])):\n",
        "            # Iterate through answers for each question\n",
        "            for j in range(len(ds[se][id]['qas']['answers'][i]['answer'])):\n",
        "                # Check if there are extractive spans\n",
        "                if len(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans']) > 0:\n",
        "                    answer_span = ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans'][0]\n",
        "\n",
        "                    # Find the paragraph containing the answer\n",
        "                    #relevant_para = None\n",
        "                    #for para in paragraphs:\n",
        "                        #if answer_span in para:\n",
        "                            #relevant_para = para\n",
        "                            #break\n",
        "\n",
        "                    titles.append(title)\n",
        "                    questions.append(ds[se][id]['qas']['question'][i])\n",
        "                    answers.append(ds[se][id]['qas']['answers'][i]['answer'][j]['extractive_spans'])\n",
        "                    list_of_full_papers.append(paragraphs)\n",
        "                    relevant_paragraphs.append(ds[se][id]['qas']['answers'][i]['answer'][j]['evidence'])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'title': titles,\n",
        "        'question': questions,\n",
        "        'answer': answers,\n",
        "        'full_paper': list_of_full_papers,\n",
        "        'relevant_paragraphs': relevant_paragraphs\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage example:\n",
        "se = 'test'\n",
        "df_test = create_qa_dataframe(ds, se)"
      ],
      "metadata": {
        "id": "pF7taQU3cItC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Define a function to retrieve the most relevant paragraphs from a new paper\n",
        "def retrieve_from_paper(question, paper_text, tfidf_vectorizer):\n",
        "    # Split the paper into paragraphs\n",
        "    paragraphs = []\n",
        "    for paragraph in paper_text:\n",
        "      if paragraph == '':\n",
        "        continue\n",
        "      else:\n",
        "        paragraphs.append(paragraph)\n",
        "    #paragraphs = paper_text.split('\\n\\n')  # assuming paragraphs are separated by double newlines\n",
        "\n",
        "    # Transform each paragraph using the fitted TF-IDF vectorizer\n",
        "    paragraph_tfidf = tfidf_vectorizer.transform(paragraphs)\n",
        "\n",
        "    # Transform the question\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "\n",
        "    # Calculate cosine similarity between question and each paragraph\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "\n",
        "    # Rank paragraphs by similarity\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices]\n",
        "\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Load your test data (assuming the test data is a CSV file with the same format as the training set)\n",
        "# Replace 'test_data.csv' with the path to your actual test dataset\n",
        "test_data = df_test\n",
        "\n",
        "# Initialize counters for evaluation metrics\n",
        "total_questions = len(test_data)\n",
        "correct_retrievals = 0\n",
        "\n",
        "# Iterate over each row in the test set\n",
        "for index, row in test_data.iterrows():\n",
        "    question = row['question']\n",
        "    full_paper = row['full_paper']\n",
        "    relevant_paragraphs = row['relevant_paragraphs']  # Expected relevant paragraphs\n",
        "\n",
        "    # Retrieve the most relevant paragraphs\n",
        "    ranked_paragraphs = retrieve_from_paper(question, full_paper, tfidf_vectorizer)\n",
        "\n",
        "    # Check if the top-ranked paragraph is in the relevant paragraphs\n",
        "    top_paragraph = ranked_paragraphs[0][0]  # Get the text of the top-ranked paragraph\n",
        "    if top_paragraph in relevant_paragraphs:\n",
        "        correct_retrievals += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_retrievals / total_questions\n",
        "print(f\"Retrieval Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNpLsRRUaQSz",
        "outputId": "e55c7460-3419-4012-aaa5-094c0611ba9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Accuracy: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.tail(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "tdSzzUpbdorP",
        "outputId": "9e16b614-e2ad-4eaa-cb11-ae8a3dbdb428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  title  \\\n",
              "1807  Automatic Extraction of Personality from Text:...   \n",
              "1808  Automatic Extraction of Personality from Text:...   \n",
              "1809  Automatic Extraction of Personality from Text:...   \n",
              "1810  Automatic Extraction of Personality from Text:...   \n",
              "1811  How to evaluate sentiment classifiers for Twit...   \n",
              "1812  How to evaluate sentiment classifiers for Twit...   \n",
              "1813  How to evaluate sentiment classifiers for Twit...   \n",
              "1814  How to evaluate sentiment classifiers for Twit...   \n",
              "1815  How to evaluate sentiment classifiers for Twit...   \n",
              "1816  How to evaluate sentiment classifiers for Twit...   \n",
              "\n",
              "                                               question  \\\n",
              "1807                    What language model is trained?   \n",
              "1808                    What language model is trained?   \n",
              "1809       What machine learning models are considered?   \n",
              "1810       What machine learning models are considered?   \n",
              "1811  Which three variants of sequential validation ...   \n",
              "1812  Which three variants of sequential validation ...   \n",
              "1813  Which three variants of cross-validation are e...   \n",
              "1814  Which three variants of cross-validation are e...   \n",
              "1815         In what way are sentiment classes ordered?   \n",
              "1816         In what way are sentiment classes ordered?   \n",
              "\n",
              "                                                 answer  \\\n",
              "1807                                           [ULMFiT]   \n",
              "1808                                  [ULMFiT BIBREF21]   \n",
              "1809  [RandomForestRegressor, LinearSVR, KNeighborsR...   \n",
              "1810  [RandomForestRegressor, LinearSVR, KNeighborsR...   \n",
              "1811  [seq(9:1, 20, equi) - 9:1 training:test ratio,...   \n",
              "1812  [9:1 training:test ratio, 20 equidistant sampl...   \n",
              "1813  [10-fold, stratified, blocked;, 10-fold, not s...   \n",
              "1814  [xval(9:1, strat, block) - 10-fold, stratified...   \n",
              "1815                                     [time-ordered]   \n",
              "1816                      [negative, neutral, positive]   \n",
              "\n",
              "                                             full_paper  \\\n",
              "1807  [Since the introduction of the personality con...   \n",
              "1808  [Since the introduction of the personality con...   \n",
              "1809  [Since the introduction of the personality con...   \n",
              "1810  [Since the introduction of the personality con...   \n",
              "1811  [Social media are becoming an increasingly imp...   \n",
              "1812  [Social media are becoming an increasingly imp...   \n",
              "1813  [Social media are becoming an increasingly imp...   \n",
              "1814  [Social media are becoming an increasingly imp...   \n",
              "1815  [Social media are becoming an increasingly imp...   \n",
              "1816  [Social media are becoming an increasingly imp...   \n",
              "\n",
              "                                    relevant_paragraphs  \n",
              "1807  [As our language model we used ULMFiT BIBREF21...  \n",
              "1808  [As our language model we used ULMFiT BIBREF21...  \n",
              "1809  [Several regression models were tested from th...  \n",
              "1810  [Several regression models were tested from th...  \n",
              "1811  [Throughout our experiments we use only one tr...  \n",
              "1812  [Social media are becoming an increasingly imp...  \n",
              "1813  [First, we apply 10-fold cross-validation wher...  \n",
              "1814  [First, we apply 10-fold cross-validation wher...  \n",
              "1815  [The complexity of Twitter data raises some ch...  \n",
              "1816  [In the paper we address the task of sentiment...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7357d81a-a2ae-461c-a719-750e7c39e42e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>full_paper</th>\n",
              "      <th>relevant_paragraphs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>Automatic Extraction of Personality from Text:...</td>\n",
              "      <td>What language model is trained?</td>\n",
              "      <td>[ULMFiT]</td>\n",
              "      <td>[Since the introduction of the personality con...</td>\n",
              "      <td>[As our language model we used ULMFiT BIBREF21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1808</th>\n",
              "      <td>Automatic Extraction of Personality from Text:...</td>\n",
              "      <td>What language model is trained?</td>\n",
              "      <td>[ULMFiT BIBREF21]</td>\n",
              "      <td>[Since the introduction of the personality con...</td>\n",
              "      <td>[As our language model we used ULMFiT BIBREF21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1809</th>\n",
              "      <td>Automatic Extraction of Personality from Text:...</td>\n",
              "      <td>What machine learning models are considered?</td>\n",
              "      <td>[RandomForestRegressor, LinearSVR, KNeighborsR...</td>\n",
              "      <td>[Since the introduction of the personality con...</td>\n",
              "      <td>[Several regression models were tested from th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1810</th>\n",
              "      <td>Automatic Extraction of Personality from Text:...</td>\n",
              "      <td>What machine learning models are considered?</td>\n",
              "      <td>[RandomForestRegressor, LinearSVR, KNeighborsR...</td>\n",
              "      <td>[Since the introduction of the personality con...</td>\n",
              "      <td>[Several regression models were tested from th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>Which three variants of sequential validation ...</td>\n",
              "      <td>[seq(9:1, 20, equi) - 9:1 training:test ratio,...</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[Throughout our experiments we use only one tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1812</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>Which three variants of sequential validation ...</td>\n",
              "      <td>[9:1 training:test ratio, 20 equidistant sampl...</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1813</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>Which three variants of cross-validation are e...</td>\n",
              "      <td>[10-fold, stratified, blocked;, 10-fold, not s...</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[First, we apply 10-fold cross-validation wher...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1814</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>Which three variants of cross-validation are e...</td>\n",
              "      <td>[xval(9:1, strat, block) - 10-fold, stratified...</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[First, we apply 10-fold cross-validation wher...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1815</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>In what way are sentiment classes ordered?</td>\n",
              "      <td>[time-ordered]</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[The complexity of Twitter data raises some ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>How to evaluate sentiment classifiers for Twit...</td>\n",
              "      <td>In what way are sentiment classes ordered?</td>\n",
              "      <td>[negative, neutral, positive]</td>\n",
              "      <td>[Social media are becoming an increasingly imp...</td>\n",
              "      <td>[In the paper we address the task of sentiment...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7357d81a-a2ae-461c-a719-750e7c39e42e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7357d81a-a2ae-461c-a719-750e7c39e42e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7357d81a-a2ae-461c-a719-750e7c39e42e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e05f5874-411c-4837-be50-a100124ffffd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e05f5874-411c-4837-be50-a100124ffffd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e05f5874-411c-4837-be50-a100124ffffd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_test\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"How to evaluate sentiment classifiers for Twitter time-ordered data?\",\n          \"Automatic Extraction of Personality from Text: Challenges and Opportunities\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What machine learning models are considered?\",\n          \"In what way are sentiment classes ordered?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_paper\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevant_paragraphs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.iloc[1807]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "emx18W4Td-Uv",
        "outputId": "61cbcb2f-2dcb-4ef0-e69e-9748ad6ddafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title                  Automatic Extraction of Personality from Text:...\n",
              "question                                 What language model is trained?\n",
              "answer                                                          [ULMFiT]\n",
              "full_paper             [Since the introduction of the personality con...\n",
              "relevant_paragraphs    [As our language model we used ULMFiT BIBREF21...\n",
              "Name: 1807, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1807</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>Automatic Extraction of Personality from Text:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>What language model is trained?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <td>[ULMFiT]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>full_paper</th>\n",
              "      <td>[Since the introduction of the personality con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relevant_paragraphs</th>\n",
              "      <td>[As our language model we used ULMFiT BIBREF21...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Define a function to retrieve the most relevant paragraph from a new paper\n",
        "def retrieve_from_paper(question, paper_text, tfidf_vectorizer):\n",
        "    # Ensure the input paper text is split into paragraphs\n",
        "    paragraphs = [p for p in paper_text if p.strip()]  # Filter out empty paragraphs\n",
        "\n",
        "    # Transform each paragraph using the fitted TF-IDF vectorizer\n",
        "    paragraph_tfidf = tfidf_vectorizer.transform(paragraphs)\n",
        "\n",
        "    # Transform the question\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "\n",
        "    # Calculate cosine similarity between question and each paragraph\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "\n",
        "    # Rank paragraphs by similarity\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices]\n",
        "\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Example input for a single inference\n",
        "question = \"Is ULMFIT trained?\"\n",
        "# Assume full_paper is a list of paragraphs\n",
        "full_paper = []\n",
        "for paragraph in df_test.iloc[1807]['full_paper']:\n",
        "  if paragraph == '':\n",
        "    continue\n",
        "  else:\n",
        "    full_paper.append(paragraph)\n",
        "print(full_paper)\n",
        "\n",
        "# Retrieve the most relevant paragraphs\n",
        "ranked_paragraphs = retrieve_from_paper(question, full_paper, tfidf_vectorizer)\n",
        "\n",
        "# Display the top paragraph\n",
        "top_paragraph, top_similarity = ranked_paragraphs[0]\n",
        "print(\"Most relevant paragraph:\")\n",
        "print(f\"Paragraph: {top_paragraph}\\nSimilarity Score: {top_similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IsdOUogdmj5",
        "outputId": "dc4675c1-caf9-482b-c168-ef0734da926a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Since the introduction of the personality concept, psychologists have worked to formulate theories and create models describing human personality and reliable measure to accordingly. The filed has been successful to bring forth a number of robust models with corresponding measures. One of the most widely accepted and used is the Five Factor Model BIBREF0. The model describes human personality by five traits/factors, popularly referred to as the Big Five or OCEAN: Openness to experience, Conscientiousness, Extraversion, Agreeableness, and emotional stability (henceforth Stability). There is now an extensive body of research showing that these factors matter in a large number of domains of people’s life. Specifically, the Big Five factors have been found to predict life outcomes such as health, longevity, work performance, interpersonal relations, migration and social attitudes, just to mention some domains (e.g. BIBREF1, BIBREF2, BIBREF3, BIBREF4).To date, the most common assessment of personality is by self-report questionnaires BIBREF5.', 'In the past decade however, personality psychologist, together with computer scientist, have worked hard to solve the puzzle of extracting a personality profile (e.g., the Big Five factors) of an individual based on a combination of social media activities BIBREF6. However, in the aftermath of Cambridge Analytica scandal, where the privacy of millions of Facebook users was violated, this line of research has been met with skepticism and suspicion. More recent research focuses on text from a variety of sources, including twitter data (e.g. BIBREF7, BIBREF8). Recent development in text analysis, machine learning, and natural language models, have move the field into an era of optimism, like never before. Importantly, the basic idea in this research is that personality is reflected in the way people write and that written communication includes information about the author’s personality characteristics BIBREF9. Nevertheless, while a number of attempts has been made to extract personality from text (see below), the research is standing remarkably far from reality. There are, to our knowledge, very few attempts to test machine learning models “in the wild”. The present paper aims to deal with this concern. Specifically, we aim to (A) create a model which is able to extract Big Five personality from a text using machine learning techniques, (B) investigate whether a model trained on a large amount of solo-annotated data performs better than a model trained on a smaller amount of high quality data, and, (C) measure the performance of our models on data from another two domains that differ from the training data.', 'In BIBREF10 the authors trained a combination of logistic and linear regression models on data from 58,466 volunteers, including their demographic profiles, Facebook data and psychometric test results, such as their Big Five traits. This data, the myPersonality dataset BIBREF11, was available for academic research until 2018, although this access has since been closed down. A demonstration version of the trained system is available to the public in form of the ApplyMagicSauce web application of Cambridge University.', 'In 2018 media exposed the unrelated (and now defunct) company Cambridge Analytica to considerable public attention for having violated the privacy and data of millions of Facebook users and for having meddled in elections, with some of these operations misusing the aforementioned research results. This scandal demonstrates the commercial and political interest in this type of research, and it also emphasizes that the field has significant ethical aspects.', 'Several attempts have been made to automatically determining the Big Five personality traits using only text written by the test person. A common simplification in such approaches is to model each trait as binary (high or low) rather than on a more realistic granular spectrum.', 'The authors of BIBREF12 trained a Bayesian Multinomial Regression model on stylistic and content features of a collection of student-written stream-of-consciousness essays with associated Big Five questionnaire results of each respective student. The researchers focused on the classifier for stability. The original representation of the numerical factor was simplified to a dichotomy between positive and negative, denoting essay authors with values in the upper or lower third respectively, and discarding texts from authors in the more ambiguous middle third. The resulting classifier then achieved an accuracy of 65.7 percent. Similar performance for the other factors was claimed as well, but not published.', 'A number of regression models were trained and tested for Big Five analysis on texts in BIBREF13. To obtain training data the authors carried out a personality survey on a microblog site, which yielded the texts and the personality data from 444 users. This work is a rare example of the Big Five being represented an actual spectrum instead of a dichotomy, using an interval $[-1, 1]$. The performance of the systems was therefore measured as the deviation from the expected trait values. The best variant achieved an average Mean Absolute Percentage Error (i.e. MAPE over all five traits) of 14 percent.', 'In BIBREF14 the authors used neural networks to analyze the Big Five personality traits of Twitter users based on their tweets. The system had no fine-grained scoring, instead classifying each trait only as either yes (high) or no (low). The authors did not provide any details about their training data, and the rudimentary evaluation allows no conclusions regarding the actual performance of the system.', 'Deep convolutional neural networks were used in BIBREF8 as classifiers on the Pennebaker & King dataset of 2,469 Big Five annotated stream-of-consciousness essays BIBREF9. The authors filtered the essays, discarding all sentences that did not contain any words from a list of emotionally charged words. One classifier was then trained for each trait, with each trait classified only as either yes (high) or no (low). The trait classifiers achieved their respective best accuracies using different configurations. Averaging these best results yielded an overall best accuracy of 58.83 percent.', 'The authors of BIBREF15 trained and evaluated an assortment of Deep Learning networks on two datasets: a subset of the Big Five-annotated myPersonality dataset with 10,000 posts from 250 Facebook users, and another 150 Facebook users whose posts the authors collected manually and had annotated using the ApplyMagicSauce tool mentioned above. The traits were represented in their simplified binary form. Their best system achieved an average accuracy of 74.17 percent.', 'In BIBREF7 the accuracy of works on Big Five personality inference as a function of the size of the input text was studied. The authors showed that using Word Embedding with Gaussian Processes provided the best results when building a classifier for predicting the personality from tweets. The data consisted of self-reported personality ratings as well as tweets from a set of 1,323 participants.', 'In BIBREF16 a set of 694 blogs with corresponding self-reported personality ratings was collected. The Linguistic Inquiry and Word Count (LIWC) 2001 program was used to analyze the blogs. A total of 66 LIWC categories was used for each personality trait. The results revealed robust correlations between the Big Five traits and the frequency with which bloggers used different word categories.', 'We employed machine learning for our text-based analysis of the Big Five personality traits. Applying machine learning presupposes large sets of annotated training data, and our case is no exception. Since we are working with Swedish language, we could not fall back on any existing large datasets like the ones available for more widespread languages such as English. Instead our work presented here encompassed the full process from the initial gathering of data over data annotation and feature extraction to training and testing of the detection models. To get an overview of the process, the workflow is shown in Figure FIGREF4.', 'Data annotation is time intensive work. Nevertheless, we decided to assemble two datasets, one prioritizing quantity over quality and one vice versa. The two sets are:', '$D_\\\\textrm {\\\\textit {LR}}$: a large dataset with lower reliability (most text samples annotated by a single annotator),', '$D_\\\\textrm {\\\\textit {HR}}$: a smaller dataset with higher reliability (each text sample annotated by multiple annotators).', 'By evaluating both directions we hoped to gain insights into the best allocation of annotation resources for future work. Regarding the choice of machine learning methods we also decided to test two approaches:', 'support vector regression (SVR): a well-understood method for the prediction of continuous values,', 'pre-trained language model (LM) with transfer learning: an LM is first trained on large amounts of non-annotated text, learning the relations between words of a given language; it is then fine-tuned for classification with annotated samples, utilizing its language representation to learn better classification with less training data. LM methods currently dominate the state-of-the-art in NLP tasks BIBREF17.', 'Each method was used to train a model on each dataset, resulting in a total of four models: $\\\\textrm {\\\\textit {SVR}}(D_\\\\textrm {\\\\textit {LR}})$ and $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {LR}})$ denoting the SVR and the language model trained on the larger dataset, and $\\\\textrm {\\\\textit {SVR}}(D_\\\\textrm {\\\\textit {HR}})$ and $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ based on the smaller set with more reliable annotations.', 'Technically, each of these four models consists of five subvariants, one for each Big Five personality trait, though for the sake of simplicity we will keep referring to the four main models only. Furthermore, to enhance legibility we will omit the dataset denotation in the model name when it is clear from the context which version is meant (e.g. in result tables).', 'As we intended our models to predict the Big Five personality traits on a scale from -3 to 3, rather than binary classification, we required training data that contained samples representing the whole data range for each trait. Given that no such dataset was available for the Swedish language, we set up our own large-scale collection and annotation operation.', 'The data was retrieved from four different Swedish discussion forums and news sites. These sources were selected such as to increase the chances of finding texts from authors with a variety of different personalities. Specifically, the four sources are:', 'Avpixlat: a migration critical news site with an extensive comment section for each editorial article. The debate climate in the comment section commonly expresses disappointment towards the society, immigrants, minority groups and the government.', 'Familjeliv: a discussion forum with the main focus on family life, relationships, pregnancy, children etc.', 'Flashback: an online forum with the tagline “freedom of speech - for real”, and in 2018 the fourth most visited social media in SwedenBIBREF18. The discussions on Flashback cover virtually any topic, from computer and relationship problems to sex, drugs and ongoing crimes.', 'Nordfront: the Swedish news site of the Nordic Resistance Movement (NMR - Nordiska motståndsrörelsen). NMR is a nordic national socialist party. The site features editorial articles, each with a section for reader comments.', 'Web spiders were used to download the texts from these sources. In total this process yielded over 70 million texts, but due to time constraints only a small fraction could be annotated and thus form our training datasets $D_\\\\textrm {\\\\textit {LR}}$ and $D_\\\\textrm {\\\\textit {HR}}$. Table TABREF19 details the size of the datasets, and how many annotated texts from each source contributed to each dataset. $D_\\\\textrm {\\\\textit {HR}}$ also contains 59 additional student texts created by the annotators themselves, an option offered to them during the annotation process (described in the following section).', 'The texts were annotated by 18 psychology students, each of whom had studied at least 15 credits of personality psychology. The annotation was carried out using a web-based tool. A student working with this tool would be shown a text randomly picked from one of the sources, as well as instructions to annotate one of the Big Five traits by selecting a number from the discrete integer interval -3 to 3. Initially the students were allowed to choose which of the five traits to annotate, but at times they would be instructed to annotate a specific trait, to ensure a more even distribution of annotations. The tool kept the samples at a sufficiently meaningful yet comfortable size by picking only texts with at least two sentences, and truncating them if they exceeded five sentences or 160 words.', 'The large dataset $D_\\\\textrm {\\\\textit {LR}}$ was produced in this manner, with 39,370 annotated texts. Due to the random text selection for each annotator, the average sample received 1.02 annotations - i.e. almost every sample was annotated by only one student and for only one Big Five trait. The distribution of annotations for the different factors is shown in Figure FIGREF23. We considered the notable prevalence of -1 and 1 to be symptomatic of a potential problem: random short texts like in our experiment, often without context, are likely not to contain any definitive personality related hints at all, and thus we would have expected results closer to a normal distribution. The students preferring -1 and 1 over the neutral zero might have been influenced by their desire to glean some psychological interpretation even from unsuitable texts.', 'For $D_\\\\textrm {\\\\textit {HR}}$, the smaller set with higher annotation reliability, we therefore modified the process. Texts were now randomly selected from the subset of $D_\\\\textrm {\\\\textit {LR}}$ containing texts which had been annotated with -3 or 3. We reasoned that these annotations at the ends of the spectrum were indicative of texts where the authors had expressed their personalities more clearly. Thus this subset would be easier to annotate, and each text was potentially more suitable for the annotation of multiple factors.', 'Eventually this process resulted in 2,774 texts with on average 4.5 annotations each. The distribution for the different factors is shown in Table FIGREF24, where multiple annotations of the same factor for one text were compounded into a single average value.', \"The intra-annotator reliability of both datasets $D_\\\\textrm {\\\\textit {LR}}$ and $D_\\\\textrm {\\\\textit {HR}}$ is shown in Table TABREF21. The reliability was calculated using the Krippendorff's alpha coefficient. Krippendorff's alpha can handle missing values, which in this case was necessary since many of the texts were annotated by only a few annotators.\", 'Table TABREF22 shows how many texts were annotated for each factor, and Figure FIGREF25 shows how the different sources span over the factor values.', 'Avpixlat and Nordfront have a larger proportion of annotated texts with factors below zero, while Flashback and especially Familjeliv have a larger proportion in the positive interval. The annotators had no information about the source of the data while they were annotating.', 'To extract information from the annotated text data and make it manageable for the regression algorithm, we used Term Frequency-Inverse Document Frequency (TF-IDF) to construct features from our labeled data. TF-IDF is a measurement of the importance of continuous series of words or characters (so called n-grams) in a document, where n-grams appearing more often in documents are weighted as less important. TF-IDF is further explained in BIBREF19. In this paper, TF-IDF was used on both word and character level with bi-gram for words and quad-grams for characters.', 'Several regression models were tested from the scikit-learn framework BIBREF20, such as RandomForestRegressor, LinearSVR, and KNeighborsRegressor. The Support Vector Machine Regression yielded the lowest MAE and MSE while performing a cross validated grid search for all the models and a range of hyperparameters.', 'As our language model we used ULMFiT BIBREF21. ULMFiT is an NLP transfer learning algorithm that we picked due to its straightforward implementation in the fast.ai library, and its promising results on small datasets. As the basis of our ULMFiT model we built a Swedish language model on a large corpus of Swedish text retrieved from the Swedish Wikipedia and the aforementioned forums Flashback and Familjeliv. We then used our annotated samples to fine-tune the language model, resulting in a classifier for the Big Five factors.', 'The performance of the models was evaluated with cross validation measuring MAE, MSE and $\\\\textrm {R}^2$. We also introduced a dummy regressor. The dummy regressor is trained to always predict the mean value of the training data. In this way it was possible to see whether the trained models predicted better than just always guessing the mean value of the test data. To calculate the $\\\\text{R}^2$ score we use the following measurement:', 'where $y$ is the actual annotated score, $\\\\bar{y}$ is the sample mean, and $e$ is the residual.', 'The models were evaluated using 5-fold cross validation. The results for the cross validation is shown in table TABREF33 and TABREF34.', 'For both datasets $D_\\\\textrm {\\\\textit {LR}}$ and $D_\\\\textrm {\\\\textit {HR}}$, the trained models predict the Big Five traits better than the dummy regressor. This means that the trained models were able to catch signals of personality from the annotated data. Extraversion and agreeableness were easiest to estimate. The smallest differences in MAE between the trained models and the dummy regressor are for extraversion and conscientiousness, for models trained on the lower reliability dataset $D_\\\\textrm {\\\\textit {LR}}$. The explanation for this might be that both of the factors are quite complicated to detect in texts and therefore hard to annotate. For the models based on $D_\\\\textrm {\\\\textit {HR}}$, we can find a large difference between the MAE for both stability and agreeableness. Agreeableness measures for example how kind and sympathetic a person is, which appears much more naturally in text compared to extraversion and conscientiousness. Stability, in particular low stability, can be displayed in writing as expressions of emotions like anger or fear, and these are often easy to identify.', 'As set out in Section SECREF2, earlier attempts at automatic analysis of the Big Five traits have often avoided modelling the factors on a spectrum, instead opting to simplify the task to a binary classification of high or low. We consider our $[-3, 3]$ interval-based representation to be preferable, as it is sufficiently granular to express realistic nuances while remaining simple enough not to overtax annotators with too many choices. Nevertheless, to gain some understanding of how our approach would compare to the state of the art, we modified our methods to train binary classifiers on the large and small datasets. For the purposes of this training a factor value below zero was regarded as low and values above as high, and the classifiers learnt to distinguish only these two classes. The accuracy during cross validation was calculated and is presented in Table TABREF36. Note that a direct comparison with earlier systems is problematic due to the differences in datasets. This test merely serves to ensure that our approach is not out of line with the general performance in the field.', 'We conducted a head-to-head test (paired sample t-test) to compare the trained language model against the corresponding dummy regressor and found that the mean absolute error was significantly lower for the language model $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$, t(4) = 4.32, p = .02, as well as the $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {LR}})$, t(4) = 4.47, p = .02. Thus, the trained language models performed significantly better than a dummy regressor. In light of these differences and the slightly lower mean absolute error $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ compared to the $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {LR}})$ [t(4) = 2.73, p = .05] and considering that $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ is the best model in terms of $\\\\textrm {R}^2$ we take it for testing in the wild.', 'Textual domain differences may affect the performance of a trained model more than expected. In the literature systems are often only evaluated on texts from their training domain. However, in our experience this is insufficient to assess the fragility of a system towards the data, and thus its limitations with respect to an actual application and generalizability across different domains. It is critical to go beyond an evaluation of trained models on the initial training data domain, and to test the systems “in the wild”, on texts coming from other sources, possibly written with a different purpose. Most of the texts in our training data have a conversational nature, given their origin in online forums, or occasionally in opinionated editorial articles. Ideally a Big Five classifier should be able to measure personality traits in any human-authored text of a reasonable length. In practice though it seems likely that the subtleties involved in personality detection could be severely affected by superficial differences in language and form. To gain some understanding on how our method would perform outside the training domain, we selected our best model $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ and evaluated it on texts from two other domains.', 'The cover letters dataset was created during a master thesis project at Uppsala University. The aim of the thesis project was to investigate the relationship between self-reported personality and personality traits extracted from texts. In the course of the thesis, 200 study participants each wrote a cover letter and answered a personality form BIBREF22. 186 of the participants had complete answers and therefore the final dataset contained 186 texts and the associated Big Five personality scores.', 'We applied $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ to the cover letters to produce Big Five trait analyses, and we compared the results to the scores from the personality questionnaire. This comparison, measured in the form of the evaluation metrics MAE, MSE and $\\\\textrm {R}^2$, is shown in Table TABREF39. As it can be seen in the table, model performance is poor and $\\\\textrm {R}^2$ was not above zero for any of the factors.', 'The self-descriptions dataset is the result of an earlier study conducted at Uppsala University. The participants, 68 psychology students (on average 7.7 semester), were instructed to describe themselves in text, yielding 68 texts with an average of approximately 450 words. The descriptions were made on one (randomly chosen) of nine themes like politics and social issues, film and music, food and drinks, and family and children. Each student also responded to a Big Five personality questionnaires consisting of 120 items. The distribution of the Big Five traits for the dataset is shown in figure FIGREF42.', 'Given this data, we applied $\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$ to the self-description texts to compute the Big Five personality trait values. We then compared the results to the existing survey assessment using the evaluation metrics MAE, MSE and $\\\\textrm {R}^2$, as shown in Table TABREF40. As it can be seen in the table, model performance was poor and $\\\\textrm {R}^2$, like the results for the cover letters dataset, was not above zero for any of the Big Five factors.', 'In this paper, we aimed to create a model that is able to extract Big Five personality traits from a text using machine learning techniques. We also aimed to investigate whether a model trained on a large amount of solo-annotated data performs better than a model trained on a smaller amount of high-quality data. Finally, we aimed to measure model performance in the wild, on data from two domains that differ from the training data. The results of our experiments showed that we were able to create models with reasonable performance (compared to a dummy classifier). These models exhibit a mean absolute error and accuracy in line with state-or-the-art models presented in previous research, with the caveat that comparisons over different datasets are fraught with difficulties. We also found that using a smaller amount of high-quality training data with multi-annotator assessments resulted in models that outperformed models based on a large amount of solo-annotated data. Finally, testing our best model ($\\\\textrm {\\\\textit {LM}}(D_\\\\textrm {\\\\textit {HR}})$) in the wild and found that the model could not, reliably, extract people’s personality from their text. These findings reveal the importance of the quality of the data, but most importantly, the necessity of examining models in the wild. Taken together, our results show that extracting personality traits from a text remains a challenge and that no firm conclusions can be made on model performance before testing in the wild. We hope that the findings will be guiding for future research.']\n",
            "Most relevant paragraph:\n",
            "Paragraph: As our language model we used ULMFiT BIBREF21. ULMFiT is an NLP transfer learning algorithm that we picked due to its straightforward implementation in the fast.ai library, and its promising results on small datasets. As the basis of our ULMFiT model we built a Swedish language model on a large corpus of Swedish text retrieved from the Swedish Wikipedia and the aforementioned forums Flashback and Familjeliv. We then used our annotated samples to fine-tune the language model, resulting in a classifier for the Big Five factors.\n",
            "Similarity Score: 0.4191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where relevant_paragraphs is an empty list\n",
        "test_data = test_data[test_data['relevant_paragraphs'].apply(lambda x: len(x) != 0)]\n",
        "\n",
        "# Verify that rows with empty relevant_paragraphs have been removed\n",
        "print(\"Rows with non-empty relevant_paragraphs:\")\n",
        "print(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLlDIMWfoNK",
        "outputId": "d00e450a-118a-49a3-b679-408124fe47de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with non-empty relevant_paragraphs:\n",
            "                                                  title  \\\n",
            "0     End-to-End Trainable Non-Collaborative Dialog ...   \n",
            "1     End-to-End Trainable Non-Collaborative Dialog ...   \n",
            "2     End-to-End Trainable Non-Collaborative Dialog ...   \n",
            "3     End-to-End Trainable Non-Collaborative Dialog ...   \n",
            "4     End-to-End Trainable Non-Collaborative Dialog ...   \n",
            "...                                                 ...   \n",
            "1812  How to evaluate sentiment classifiers for Twit...   \n",
            "1813  How to evaluate sentiment classifiers for Twit...   \n",
            "1814  How to evaluate sentiment classifiers for Twit...   \n",
            "1815  How to evaluate sentiment classifiers for Twit...   \n",
            "1816  How to evaluate sentiment classifiers for Twit...   \n",
            "\n",
            "                                               question  \\\n",
            "0                     How big is the ANTISCAM dataset?    \n",
            "1                     How big is the ANTISCAM dataset?    \n",
            "2                     How big is the ANTISCAM dataset?    \n",
            "3                     How big is the ANTISCAM dataset?    \n",
            "4                     How big is the ANTISCAM dataset?    \n",
            "...                                                 ...   \n",
            "1812  Which three variants of sequential validation ...   \n",
            "1813  Which three variants of cross-validation are e...   \n",
            "1814  Which three variants of cross-validation are e...   \n",
            "1815         In what way are sentiment classes ordered?   \n",
            "1816         In what way are sentiment classes ordered?   \n",
            "\n",
            "                                                 answer  \\\n",
            "0                     [ 3,044 sentences in 100 dialogs]   \n",
            "1                             [220 human-human dialogs]   \n",
            "2     [220 human-human dialogs. , 3,044 sentences in...   \n",
            "3     [220 human-human dialogs. The average conversa...   \n",
            "4                             [220 human-human dialogs]   \n",
            "...                                                 ...   \n",
            "1812  [9:1 training:test ratio, 20 equidistant sampl...   \n",
            "1813  [10-fold, stratified, blocked;, 10-fold, not s...   \n",
            "1814  [xval(9:1, strat, block) - 10-fold, stratified...   \n",
            "1815                                     [time-ordered]   \n",
            "1816                      [negative, neutral, positive]   \n",
            "\n",
            "                                             full_paper  \\\n",
            "0     [Considerable progress has been made building ...   \n",
            "1     [Considerable progress has been made building ...   \n",
            "2     [Considerable progress has been made building ...   \n",
            "3     [Considerable progress has been made building ...   \n",
            "4     [Considerable progress has been made building ...   \n",
            "...                                                 ...   \n",
            "1812  [Social media are becoming an increasingly imp...   \n",
            "1813  [Social media are becoming an increasingly imp...   \n",
            "1814  [Social media are becoming an increasingly imp...   \n",
            "1815  [Social media are becoming an increasingly imp...   \n",
            "1816  [Social media are becoming an increasingly imp...   \n",
            "\n",
            "                                    relevant_paragraphs  \n",
            "0     [To enrich available non-collaborative task da...  \n",
            "1     [To enrich available non-collaborative task da...  \n",
            "2     [To enrich available non-collaborative task da...  \n",
            "3     [To enrich available non-collaborative task da...  \n",
            "4     [To enrich available non-collaborative task da...  \n",
            "...                                                 ...  \n",
            "1812  [Social media are becoming an increasingly imp...  \n",
            "1813  [First, we apply 10-fold cross-validation wher...  \n",
            "1814  [First, we apply 10-fold cross-validation wher...  \n",
            "1815  [The complexity of Twitter data raises some ch...  \n",
            "1816  [In the paper we address the task of sentiment...  \n",
            "\n",
            "[1815 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Define a function to retrieve the most relevant paragraphs from a new paper\n",
        "def retrieve_from_paper(question, paper_text, tfidf_vectorizer, k=5):\n",
        "    # Split the paper into paragraphs\n",
        "    paragraphs = [p for p in paper_text if p.strip()]  # Filter out empty paragraphs\n",
        "\n",
        "    # Transform each paragraph using the fitted TF-IDF vectorizer\n",
        "    paragraph_tfidf = tfidf_vectorizer.transform(paragraphs)\n",
        "\n",
        "    # Transform the question\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "\n",
        "    # Calculate cosine similarity between question and each paragraph\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "\n",
        "    # Rank paragraphs by similarity\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices[:k]]\n",
        "\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Evaluation metrics calculation\n",
        "def calculate_metrics(test_data, tfidf_vectorizer, k=5):\n",
        "    total_questions = len(test_data)\n",
        "    precision_at_k = 0\n",
        "    recall_at_k = 0\n",
        "    accuracy_at_k = 0\n",
        "\n",
        "    for index, row in test_data.iterrows():\n",
        "        question = row['question']\n",
        "        full_paper = row['full_paper']\n",
        "        relevant_paragraphs = row['relevant_paragraphs']  # Expected relevant paragraphs (list of strings)\n",
        "\n",
        "        # Retrieve the top-k most relevant paragraphs\n",
        "        ranked_paragraphs = retrieve_from_paper(question, full_paper, tfidf_vectorizer, k=k)\n",
        "        retrieved_paragraphs = [para[0] for para in ranked_paragraphs]  # Get only the text of the paragraphs\n",
        "\n",
        "        # Calculate Precision@k\n",
        "        relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "        precision_at_k += len(relevant_retrieved) / k\n",
        "\n",
        "        # Calculate Recall@k\n",
        "        print(relevant_paragraphs)\n",
        "        recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs)\n",
        "\n",
        "        # Calculate Accuracy@k (1 if at least one relevant paragraph is in top-k, else 0)\n",
        "        if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "            accuracy_at_k += 1\n",
        "\n",
        "    # Average the metrics over all questions\n",
        "    precision_at_k /= total_questions\n",
        "    recall_at_k /= total_questions\n",
        "    accuracy_at_k /= total_questions\n",
        "\n",
        "    print(f\"Precision@{k}: {precision_at_k:.2f}\")\n",
        "    print(f\"Recall@{k}: {recall_at_k:.2f}\")\n",
        "    print(f\"Accuracy@{k}: {accuracy_at_k:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "# Assuming `test_data` is your test DataFrame with columns 'question', 'full_paper', and 'relevant_paragraphs'\n",
        "calculate_metrics(test_data, tfidf_vectorizer, k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLNxpspKeIsa",
        "outputId": "3c3d6c53-756c-4513-ba6b-4603c8547f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "['To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.']\n",
            "[\"To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal.\"]\n",
            "[\"To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal.\", 'In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents. General intents are more closely related to the syntactic meaning of the sentence (open_question, yes_no_question, positive_answer, negative_answer, responsive_statement, and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold).', 'For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.']\n",
            "['Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21.', \"Previous studies use template-based methods to maintain sentence coherence. However, rigid templates lead to limited diversity, causing the user losing engagement. On the other hand, language generation models can generate diverse responses but are bad at being coherent. We propose Multiple Intents and Semantic Slots Annotation Neural Network (MISSA) to combine the advantages of both template and generation models and takes advantage from the hierarchical annotation at the same time. MISSA follows the TransferTransfo framework BIBREF0 with three modifications: (i) We first concurrently predict user's, system's intents and semantic slots; (ii) We then perform conditional generation to improve generated response's coherence. Specifically, we generate responses conditioned on the above intermediate representation (intents and slots); (iii) Finally, we generate multiple responses with the nucleus sampling strategy BIBREF5 and then apply a response filter, which contains a set of pre-defined constraints to select coherent responses. The constraints in the filter can be defined according to specific task requirements or general conversational rules.\"]\n",
            "['We compare MISSA mainly with two baseline models:', 'TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.', 'Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.']\n",
            "['We compare MISSA mainly with two baseline models:', 'TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.', 'Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.']\n",
            "['We compare MISSA mainly with two baseline models:', 'TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.', 'Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.']\n",
            "['We compare MISSA mainly with two baseline models:', 'TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.', 'Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.']\n",
            "['Experiments ::: Automatic Evaluation Metrics', 'Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.', \"Response-Intent Prediction (RIP) $\\\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\\\%$ accuracy and the semantic slot predictor achieves $77\\\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).\", 'Extended Response-Intent Prediction (ERIP) $\\\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.', \"Automatic metrics only validate the system’s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.\", \"Fluency Fluency is used to explore different models' language generation quality.\", \"Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\", 'Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.', \"Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\", \"Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.\"]\n",
            "['Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.', \"Response-Intent Prediction (RIP) $\\\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\\\%$ accuracy and the semantic slot predictor achieves $77\\\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).\", 'Extended Response-Intent Prediction (ERIP) $\\\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.', \"Fluency Fluency is used to explore different models' language generation quality.\", \"Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\", 'Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.', \"Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\", \"Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.\"]\n",
            "[\"Fluency Fluency is used to explore different models' language generation quality.\", \"Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\", 'Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.', \"Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\", \"Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.\"]\n",
            "['To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours. Liu is claimed to perform in real-time while, requiring a cluster of 5 repeated messages to judge them for rumours. Note that although these algorithm are state-of-the-art for detecting rumours as quickly as possible, they still require a certain delay to reach their full potential.']\n",
            "['To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours. Liu is claimed to perform in real-time while, requiring a cluster of 5 repeated messages to judge them for rumours. Note that although these algorithm are state-of-the-art for detecting rumours as quickly as possible, they still require a certain delay to reach their full potential.']\n",
            "['To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours. Liu is claimed to perform in real-time while, requiring a cluster of 5 repeated messages to judge them for rumours. Note that although these algorithm are state-of-the-art for detecting rumours as quickly as possible, they still require a certain delay to reach their full potential.']\n",
            "['We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.']\n",
            "['We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.']\n",
            "['We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.']\n",
            "['trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would find later on. We also only consider news articles published before the timestamps of the social media messages.', \"For our social media stream, we chose Sina Weibo, a Chinese social media service with more than 200 million active users. Micro-blogs from Sina Weibo are denoted as 'weibos'.\"]\n",
            "['trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would find later on. We also only consider news articles published before the timestamps of the social media messages.']\n",
            "['To increase instantaneous detection performance, we compensate for the absence of future information by consulting additional data sources. In particular, we make use of news wire articles, which are considered to be of high credibility. This is reasonable as according to Petrovic et. al (2013), in the majority of cases, news wires lead social media for reporting news. When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour. Note that this closely resembles the definition of what a rumour is.']\n",
            "['Rumour detection is a challenging task, as it requires determining the truth of information (Zhao et. al, 2015). The Cambridge dictionary, defines a rumour as information of doubtful or unconfirmed truth. We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. Numerous features have been proposed for rumour detection on social media, many of which originate from an original study on information credibility by Castillo et. al (2011). Unfortunately, the currently most successful features rely on information based on graph propagation and clustering, which can only be computed retrospectively. This renders them close to useless when detecting rumours early on. We introduce two new classes of features, one based on novelty, the other on pseudo feedback. Both feature categories improve detection accuracy early on, when information is limited.']\n",
            "['The following baselines were used in our experiments:', 'LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.', 'Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.', 'HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.', 'GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own.']\n",
            "['We propose Sentence Level Recurrent Topic Model (SLRTM) to tackle the limitations of the aforementioned works. In the new model, we assume the words in the same sentence to share the same topic in order to guarantee topic coherence, and we assume the generation of a word to rely on the whole history in the same sentence in order to fully characterize the sequential dependency. Specifically, for a particular word INLINEFORM0 within a sentence INLINEFORM1 , we assume its generation depends on two factors: the first is the whole set of its historical words in the sentence and the second is the sentence topic, which we regard as a pseudo word and has its own distributed representations. We use Recurrent Neural Network (RNN) BIBREF16 , such as Long Short Term Memory (LSTM) BIBREF17 or Gated Recurrent Unit (GRU) network BIBREF18 , to model such a long term dependency.', 'The following baselines were used in our experiments:', 'LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.', 'Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.', 'HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.', 'GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own.']\n",
            "['The following baselines were used in our experiments:', 'LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.', 'Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.', 'HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.', 'GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own.']\n",
            "['The following baselines were used in our experiments:', 'LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.', 'Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.', 'HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.', 'GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own.']\n",
            "[\"We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations.\"]\n",
            "['We propose Sentence Level Recurrent Topic Model (SLRTM) to tackle the limitations of the aforementioned works. In the new model, we assume the words in the same sentence to share the same topic in order to guarantee topic coherence, and we assume the generation of a word to rely on the whole history in the same sentence in order to fully characterize the sequential dependency. Specifically, for a particular word INLINEFORM0 within a sentence INLINEFORM1 , we assume its generation depends on two factors: the first is the whole set of its historical words in the sentence and the second is the sentence topic, which we regard as a pseudo word and has its own distributed representations. We use Recurrent Neural Network (RNN) BIBREF16 , such as Long Short Term Memory (LSTM) BIBREF17 or Gated Recurrent Unit (GRU) network BIBREF18 , to model such a long term dependency.', \"We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations.\"]\n",
            "[\"We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations.\"]\n",
            "['We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation task and a document classification task, on two datasets; (2) qualitative inspection, including the examination of the sentences generated under each topic, in order to test whether SLRTM performs well in the topic2sentence task.']\n",
            "['We evaluate our model in two English NER datasets and four Chinese NER datasets.', '(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.', '(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.', '(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.', '(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.', '(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.', '(6) Resume NER was annotated by BIBREF33.']\n",
            "['We evaluate our model in two English NER datasets and four Chinese NER datasets.', '(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.', '(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.', '(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.', '(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.', '(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.', '(6) Resume NER was annotated by BIBREF33.']\n",
            "['FLOAT SELECTED: Table 1: Details of Datasets.', 'In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.', 'We evaluate our model in two English NER datasets and four Chinese NER datasets.', '(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.', '(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.', '(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.', '(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.', '(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.', '(6) Resume NER was annotated by BIBREF33.']\n",
            "['We evaluate our model in two English NER datasets and four Chinese NER datasets.', '(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.', '(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.', '(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.', '(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.', '(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.', '(6) Resume NER was annotated by BIBREF33.']\n",
            "['In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.']\n",
            "['This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%—much higher than the other systems.', 'For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy—much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .']\n",
            "['For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy—much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .']\n",
            "['This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%—much higher than the other systems.', 'This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.']\n",
            "['This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%—much higher than the other systems.']\n",
            "['In this thesis proposal, I will present KALM BIBREF5 , BIBREF6 , a system for knowledge authoring and question answering. KALM is superior to the current CNL systems in that KALM has a complex frame-semantic parser which can standardize the semantics of the sentences that express the same meaning via different linguistic structures. The frame-semantic parser is built based on FrameNet BIBREF7 and BabelNet BIBREF8 where FrameNet is used to capture the meaning of the sentence and BabelNet BIBREF8 is used to disambiguate the meaning of the extracted entities from the sentence. Experiment results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems.', 'This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%—much higher than the other systems.']\n",
            "['This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%—much higher than the other systems.', 'For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy—much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .']\n",
            "['For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy—much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .']\n",
            "['For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy—much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .']\n",
            "['Since the EVENTI campaign, there has been a lack of further research, especially in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier. The pre-trained models and scripts running the system (or re-train it) are publicly available. .']\n",
            "['Since the EVENTI campaign, there has been a lack of further research, especially in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier. The pre-trained models and scripts running the system (or re-train it) are publicly available. .']\n",
            "['Results for the experiments are illustrated in Table 2 . We also report the results of the best system that participated at EVENTI Subtask B, FBK-HLT BIBREF23 . FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Figure 1 plots charts comparing F1 scores of the network initialized with each of the five embeddings against the FBK-HLT system for the event detection and classification tasks, respectively.']\n",
            "['Results for the experiments are illustrated in Table 2 . We also report the results of the best system that participated at EVENTI Subtask B, FBK-HLT BIBREF23 . FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Figure 1 plots charts comparing F1 scores of the network initialized with each of the five embeddings against the FBK-HLT system for the event detection and classification tasks, respectively.']\n",
            "[\"To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:\", 'Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.', 'Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.', 'Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.', 'In steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.']\n",
            "[\"To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:\", 'Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.', 'Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.', 'Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.', 'In steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.']\n",
            "[\"To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:\"]\n",
            "['With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.', 'First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\\\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.', \"To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:\", 'Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.']\n",
            "['First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\\\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.']\n",
            "['Our contributions include:', 'Two datasets for studying language and gender, each consisting of over 300K sentences.']\n",
            "['To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.']\n",
            "['To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.']\n",
            "[\"Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces “polite refusal” (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to “play along” (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the “deflection” strategy, such as “Why do you ask?”. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the “clean” seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users.\"]\n",
            "[\"Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces “polite refusal” (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to “play along” (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the “deflection” strategy, such as “Why do you ask?”. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the “clean” seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users.\"]\n",
            "['We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11% BIBREF2 and 30% BIBREF6. Since we are not allowed to directly quote from our corpus in order to protect customer rights, we summarise the data to a total of 109 “prototypical\" utterances - substantially extending the previous dataset of 35 utterances from Amanda:EthicsNLP2018 - and categorise these utterances based on the Linguistic Society\\'s definition of sexual harassment BIBREF7:']\n",
            "['In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as “acceptable behaviour in a work environment” and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker is to label following the methodology described in BIBREF21, where each utterance is rated relatively to a reference on a user-defined scale. Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women. This is in-line with our target population: 57% of users of smart speakers are male and the majority are under 44 BIBREF22.']\n",
            "['In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as “acceptable behaviour in a work environment” and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker is to label following the methodology described in BIBREF21, where each utterance is rated relatively to a reference on a user-defined scale. Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women. This is in-line with our target population: 57% of users of smart speakers are male and the majority are under 44 BIBREF22.']\n",
            "['In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as “acceptable behaviour in a work environment” and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker is to label following the methodology described in BIBREF21, where each utterance is rated relatively to a reference on a user-defined scale. Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women. This is in-line with our target population: 57% of users of smart speakers are male and the majority are under 44 BIBREF22.']\n",
            "['This paper presents the first user study on perceived appropriateness of system responses after verbal abuse. We put strategies used by state-of-the-art systems to the test in a large-scale, crowd-sourced evaluation. The full annotated corpus contains 2441 system replies, categorised into 14 response types, which were evaluated by 472 raters - resulting in 7.7 ratings per reply.']\n",
            "['The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.']\n",
            "['We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as:']\n",
            "['The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.']\n",
            "['We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\\\rho $ functions.', 'Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.']\n",
            "['Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.']\n",
            "['Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.']\n",
            "['Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.', 'We use the datasets distributed by LDC for Catalan (CAT), Spanish (SPA), German (DEU), Czech (CZE) and English (ENG) BIBREF17 , BIBREF18 ; and datasets made available by BIBREF19 , BIBREF20 for Finnish (FIN) and Turkish (TUR) respectively . Datasets are provided with syntactic dependency annotations and semantic roles of verbal predicates. In addition, English supplies nominal predicates annotated with semantic roles and does not provide any morphological feature.']\n",
            "['Results and Discussion ::: Datasets', 'We have used the following datasets for training and evaluation:', 'The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\", 'The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.', 'Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.', 'Flickr30K and COCO, as image-sentence matching benchmarks.']\n",
            "['We have used the following datasets for training and evaluation:', 'The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\", 'The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.', 'Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.', 'Flickr30K and COCO, as image-sentence matching benchmarks.']\n",
            "['We have used the following datasets for training and evaluation:', 'The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\", 'The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.', 'Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.', 'Flickr30K and COCO, as image-sentence matching benchmarks.']\n",
            "['We have used the following datasets for training and evaluation:', 'The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\", 'The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.', 'Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.', 'Flickr30K and COCO, as image-sentence matching benchmarks.']\n",
            "[\"We use HolE BIBREF19 and Vecsigrafo BIBREF16 to learn semantic embeddings. The latter extends the Swivel algorithm BIBREF20 to jointly learn word, lemma and concept embeddings on a corpus disambiguated against the KG, outperforming the previous state of the art in word and word-sense embeddings by co-training word, lemma and concept embeddings as opposed to training each individually. In contrast to Vecsigrafo, which requires both a text corpus and a KG, HolE follows a graph-based approach where embeddings are learnt exclusively from the KG. As section SECREF14 will show, this gives Vecsigrafo a certain advantage in the FCC task. Following up with the work presented in BIBREF16, our experiments focus on Sensigrafo, the KG underlying Expert System's Cogito NLP proprietary platform. Similar to WordNet, on which Vecsigrafo has also been successfully trained, Sensigrafo is a general-purpose KG with lexical and semantic information that contains over 300K concepts, 400K lemmas and 80 types of relations rendering 3M links. We use Cogito to disambiguate the text corpora prior to training Vecsigrafo. All the semantic (lemma and concept) embeddings produced with HolE or Vecsigrafo are 100-D.\", 'Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.']\n",
            "['Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora it is trained on, Wikipedia or SemScholar, raising lemma coverage to 42% and 47%, respectively.']\n",
            "['We put our FCC task in the context of the more general problem of image-sentence matching through a bidirectional retrieval task where images are sought given a text query and vice versa. While table TABREF20 focuses on natural images datasets (Flickr30K and COCO), table TABREF21 shows results on scientific datasets (SciGraph and SemScholar) rich in scientific figures and diagrams. The selected baselines (Embedding network, 2WayNet, VSE++ and DSVE-loc) report results obtained on the Flickr30K and COCO datasets, which we also include in table TABREF20. Performance is measured in recall at k ($Rk$), with k={1,5,10}. From the baselines, we successfully reproduced DSVE-loc, using the code made available by the authors, and trained it on SciGraph and SemScholar.']\n",
            "['The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method.']\n",
            "['We evaluate our method in the task it was trained to solve: determining whether a figure and a caption correspond. We also compare the performance of the FCC task against two supervised baselines, training them on a classification task against the SciGraph taxonomy. For such baselines we first train the vision and language networks independently and then combine them. The feature extraction parts of both networks are the same as described in section SECREF6. On top of them, we attach a fully connected layer with 128 neurons and ReLU activation and a softmax layer, with as many neurons as target classes.', 'The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method.']\n",
            "['The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method.']\n",
            "['The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method.']\n",
            "['The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\"]\n",
            "['The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.', \"Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\", 'The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.']\n",
            "['In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions.']\n",
            "['A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar.']\n",
            "['In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.']\n",
            "['In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.']\n",
            "['In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.']\n",
            "['In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.']\n",
            "['For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100. However on ly the first of these values was significant, with a p-value of INLINEFORM0 . Regarding the hidden size of the RNN, we could not find statistical difference across the tested sizes. Dropout also had inconsistent effects, but was generally useful.', 'Anger Dataset', 'Joy Dataset', 'In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25. Regarding the hidden size of the RNN, we observed that 100 hidden units offered better performance in our experiments, with an average absolute gain of 0.052 ( INLINEFORM2 ) over 50 hidden units. Compared to the models with 200 hidden units, the performance difference was statistically not significant.', 'Fear Dataset', 'On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200.', 'Sadness Dataset', 'Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25. Results were statistically equivalent for size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.']\n",
            "['To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 .']\n",
            "['To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 .']\n",
            "['We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Given the subjectivity and difficulty of the task, we reminded the annotators not to let their personal opinions about the topics being discussed in the tweets influence their annotation decisions.']\n",
            "['Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech.']\n",
            "['Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is the first work that examines how annotators react to hate speech comments.']\n",
            "['Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is the first work that examines how annotators react to hate speech comments.']\n",
            "[\"The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets. Each of the annotated aspects represents a classification task of its own, that could either be evaluated independently, or, as intended in this paper, tested on how it impacts other tasks. The different labels are designed to facilitate the study of the correlations between the explicitness of the tweet, the type of hostility it conveys, its target attribute, the group it dehumanizes, how different people react to it, and the performance of multitask learning on the five tasks. We assigned each tweet to five annotators, then applied majority voting to each of the labeling tasks. Given the numbers of annotators and labels in each annotation sub-task, we allowed multilabel annotations in the most subjective classification tasks, namely the hostility type and the annotator's sentiment labels, in order to keep the right human-like approximations. If there are two annotators agreeing on two labels respectively, we add both labels to the annotation.\"]\n",
            "['We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task. Since in natural language processing, there is a peculiar interest in multitask learning, where different tasks can be used to help each other BIBREF7, BIBREF8, BIBREF9, we use a unified model to handle the annotated data in all three languages and five tasks. We adopt BIBREF8 as a learning algorithm adapted to loosely related tasks such as our five annotated aspects and, use the Babylon cross-lingual embeddings BIBREF10 to align the three languages. We compare the multilingual multitask learning settings with monolingual multitask, multilingual single-task, and monolingual single-task learning settings respectively. Then, we report the performance results of the different settings and discuss how each task affects the remaining ones. We release our dataset and code to the community to extend research work on multilingual hate speech detection and classification.']\n",
            "['Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech.']\n",
            "['Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech.']\n",
            "['Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech.']\n",
            "['We present the labelset the annotators refer to, and statistics of our annotated data in the following.', 'Dataset ::: Final Dataset ::: Directness label', 'Annotators determine the explicitness of the tweet by labeling it as direct or indirect speech. This should be based on whether the target is explicitly named, or less easily discernible, especially if the tweet contains humor, metaphor, or figurative speech. Table TABREF20 shows that even when partly using equivalent keywords to search for candidate tweets, there are still significant differences in the resulting data.', 'Dataset ::: Final Dataset ::: Hostility type', 'To identify the hostility type of the tweet, we stick to the following conventions: (1) if the tweet sounds dangerous, it should be labeled as abusive; (2) according to the degree to which it spreads hate and the tone its author uses, it can be hateful, offensive or disrespectful; (3) if the tweet expresses or spreads fear out of ignorance against a group of individuals, it should be labeled as fearful; (4) otherwise it should be annotated as normal. We define this task to be multilabel. Table TABREF20 shows that hostility types are relatively consistent across different languages and offensive is the most frequent label.', 'Dataset ::: Final Dataset ::: Target attribute', 'After annotating the pilot dataset, we noticed common misconceptions regarding race, ethnicity, and nationality, therefore we merged these attributes into one label origin. Then, we asked the annotators to determine whether the tweet insults or discriminates against people based on their (1) origin, (2) religious affiliation, (3) gender, (4) sexual orientation, (5) special needs or (6) other. Table TABREF20 shows there are fewer tweets targeting disability in Arabic compared to English and French and no tweets insulting people based on their sexual orientation which may be due to the fact that the labels of gender, gender identity, and sexual orientation use almost the same wording. On the other hand, French contains a small number of tweets targeting people based on their gender in comparison to English and Arabic. We have observed significant differences in terms of target attributes in the three languages. More data may help us examine the problems affecting targets of different linguistic backgrounds.', 'Dataset ::: Final Dataset ::: Target group', 'We determined 16 common target groups tagged by the annotators after the first annotation step. The annotators had to decide on whether the tweet is aimed at women, people of African descent, Hispanics, gay people, Asians, Arabs, immigrants in general, refugees; people of different religious affiliations such as Hindu, Christian, Jewish people, and Muslims; or from political ideologies socialists, and others. We also provided the annotators with a category to cover hate directed towards one individual, which cannot be generalized. In case the tweet targets more than one group of people, the annotators should choose the group which would be the most affected by it according to them. Table TABREF10 shows the counts of the five categories out of 16 that commonly occur in the three languages. In fact, most of the tweets target individuals or fall into the “other” category. In the latter case, they may target people with different political views such as liberals or conservatives in English and French, or specific ethnic groups such as Kurdish people in Arabic. English tweets tend to have more tweets targeting people with special needs, due to common language-specific demeaning terms used in conversations where people insult one another. Arabic tweets contain more hateful comments towards women for the same reason. On the other hand, the French corpus contains more tweets that are offensive towards African people, due to hateful comments generated by debates about immigrants.', 'Dataset ::: Final Dataset ::: Sentiment of the annotator', \"We claim that the choice of a suitable emotion representation model is key to this sub-task, given the subjective nature and social ground of the annotator's sentiment analysis. After collecting the annotation results of the pilot dataset regarding how people feel about the tweets, and observing the added categories, we adopted a range of sentiments that are in the negative and neutral scales of the hourglass of emotions introduced by BIBREF29. This model includes sentiments that are connected to objectively assessed natural language opinions, and excludes what is known as self-conscious or moral emotions such as shame and guilt. Our labels include shock, sadness, disgust, anger, fear, confusion in case of ambivalence, and indifference. This is the second multilabel task of our model.\"]\n",
            "[\"The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets. Each of the annotated aspects represents a classification task of its own, that could either be evaluated independently, or, as intended in this paper, tested on how it impacts other tasks. The different labels are designed to facilitate the study of the correlations between the explicitness of the tweet, the type of hostility it conveys, its target attribute, the group it dehumanizes, how different people react to it, and the performance of multitask learning on the five tasks. We assigned each tweet to five annotators, then applied majority voting to each of the labeling tasks. Given the numbers of annotators and labels in each annotation sub-task, we allowed multilabel annotations in the most subjective classification tasks, namely the hostility type and the annotator's sentiment labels, in order to keep the right human-like approximations. If there are two annotators agreeing on two labels respectively, we add both labels to the annotation.\"]\n",
            "[\"The final dataset is composed of a pilot corpus of 100 tweets per language, and comparable corpora of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets. Each of the annotated aspects represents a classification task of its own, that could either be evaluated independently, or, as intended in this paper, tested on how it impacts other tasks. The different labels are designed to facilitate the study of the correlations between the explicitness of the tweet, the type of hostility it conveys, its target attribute, the group it dehumanizes, how different people react to it, and the performance of multitask learning on the five tasks. We assigned each tweet to five annotators, then applied majority voting to each of the labeling tasks. Given the numbers of annotators and labels in each annotation sub-task, we allowed multilabel annotations in the most subjective classification tasks, namely the hostility type and the annotator's sentiment labels, in order to keep the right human-like approximations. If there are two annotators agreeing on two labels respectively, we add both labels to the annotation.\"]\n",
            "['SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation.', 'Named Entities. Most NERD approaches link recognized entities with database entries or websites. This method helps to categorize and summarize text, but also contributes to the disambiguation of words in texts. The primary issue in MT systems is caused by common words from a source language that are used as proper nouns in a target language. For instance, the word “Kiwi\" is a family name in New Zealand which comes from the Māori culture, but it also can be a fruit, a bird, or a computer program. Named Entities are a common and difficult problem in both MT (see Koehn BIBREF0 ) and SW fields. The SW achieved important advances in NERD using structured data and semantic annotations, e.g., by adding an rdf:type statement which identifies whether a certain kiwi is a fruit BIBREF14 . In MT systems, however, this problem is directly related to the ambiguity problem and therefore has to be resolved in that wider context.', \"Non-standard speech. The non-standard language problem is a rather important one in the MT field. Many people use the colloquial form to speak and write to each other on social networks. Thus, when MT systems are applied on this context, the input text frequently contains slang, MWE, and unreasonable abbreviations such as “Idr = I don't remember.” and “cya = see you”. Additionally, idioms contribute to this problem, decreasing the translation quality. Idioms often have an entirely different meaning than their separated word meanings. Consequently, most translation outputs of such expressions contain errors. For a good translation, the MT system needs to recognize such slang and try to map it to the target language. Some SMT systems like Google or Bing have recognition patterns over non-standard speech from old translations through the Web using SMT approaches. In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open. Moreover, each person has their own speaking form.\", 'Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs. Once the KBs are translated, we suggest including them in the language models for improving the translation of entities.']\n",
            "['SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation.', 'Named Entities. Most NERD approaches link recognized entities with database entries or websites. This method helps to categorize and summarize text, but also contributes to the disambiguation of words in texts. The primary issue in MT systems is caused by common words from a source language that are used as proper nouns in a target language. For instance, the word “Kiwi\" is a family name in New Zealand which comes from the Māori culture, but it also can be a fruit, a bird, or a computer program. Named Entities are a common and difficult problem in both MT (see Koehn BIBREF0 ) and SW fields. The SW achieved important advances in NERD using structured data and semantic annotations, e.g., by adding an rdf:type statement which identifies whether a certain kiwi is a fruit BIBREF14 . In MT systems, however, this problem is directly related to the ambiguity problem and therefore has to be resolved in that wider context.', 'Therefore, we suggest that user characteristics can be applied as context for solving the non-standard language problem. These characteristics can be extracted from social media or user logs and stored as user properties using SWT, e.g., FOAF vocabulary. These ontologies have properties which would help identify the birth place or the interests of a given user. For instance, the properties foaf:interest and sioc:topic can be used to describe a given person\\'s topics of interest. If the person is a computer scientist and the model contains topics such as “Information Technology\" and “Sports\", the SPARQL queries would search for terms inserted in this context which are ambiguous. Furthermore, the property foaf:based_near may support the problem of idioms. Assuming that a user is located in a certain part of Russia and he is reading an English web page which contains some idioms, this property may be used to gather appropriate translations of idioms from English to Russian using a given RDF KB. Therefore, an MT system can be adapted to a user by using specific data about him in RDF along with given KBs. Recently, Moussallem et al BIBREF16 have released a multilingual linked idioms dataset as a first part of supporting the investigation of this suggestion. The dataset contains idioms in 5 languages and are represented by knowledge graphs which facilitates the retrieval and inference of translations among the idioms.', 'Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs. Once the KBs are translated, we suggest including them in the language models for improving the translation of entities.']\n",
            "['SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation.', 'Based on the surveyed works on our research BIBREF6 , SWT have mostly been applied at the semantic analysis step, rather than at the other stages of the translation process, due to their ability to deal with concepts behind the words and provide knowledge about them. As SWT have developed, they have increasingly been able to resolve some of the open challenges of MT. They may be applied in different ways according to each MT approach.', 'Named Entities. Most NERD approaches link recognized entities with database entries or websites. This method helps to categorize and summarize text, but also contributes to the disambiguation of words in texts. The primary issue in MT systems is caused by common words from a source language that are used as proper nouns in a target language. For instance, the word “Kiwi\" is a family name in New Zealand which comes from the Māori culture, but it also can be a fruit, a bird, or a computer program. Named Entities are a common and difficult problem in both MT (see Koehn BIBREF0 ) and SW fields. The SW achieved important advances in NERD using structured data and semantic annotations, e.g., by adding an rdf:type statement which identifies whether a certain kiwi is a fruit BIBREF14 . In MT systems, however, this problem is directly related to the ambiguity problem and therefore has to be resolved in that wider context.', 'Therefore, we suggest that user characteristics can be applied as context for solving the non-standard language problem. These characteristics can be extracted from social media or user logs and stored as user properties using SWT, e.g., FOAF vocabulary. These ontologies have properties which would help identify the birth place or the interests of a given user. For instance, the properties foaf:interest and sioc:topic can be used to describe a given person\\'s topics of interest. If the person is a computer scientist and the model contains topics such as “Information Technology\" and “Sports\", the SPARQL queries would search for terms inserted in this context which are ambiguous. Furthermore, the property foaf:based_near may support the problem of idioms. Assuming that a user is located in a certain part of Russia and he is reading an English web page which contains some idioms, this property may be used to gather appropriate translations of idioms from English to Russian using a given RDF KB. Therefore, an MT system can be adapted to a user by using specific data about him in RDF along with given KBs. Recently, Moussallem et al BIBREF16 have released a multilingual linked idioms dataset as a first part of supporting the investigation of this suggestion. The dataset contains idioms in 5 languages and are represented by knowledge graphs which facilitates the retrieval and inference of translations among the idioms.', 'Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs. Once the KBs are translated, we suggest including them in the language models for improving the translation of entities.']\n",
            "['On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions. For instance, the English language contains irregular verbs like “set” or “put”. Depending on the structure of a sentence, it is not possible to recognize their verbal tense, e.g., present or past tense. Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue. For instance, the sentence “Anna usually put her notebook on the table for studying\" may be annotated using a certain vocabulary and represented by triples. Thus, the verb “put\", which is represented by a predicate that groups essential information about the verbal tense, may support the generation step of a given MT system. This sentence usually fails when translated to rich morphological languages, such as Brazilian-Portuguese and Arabic, for which the verb influences the translation of “usually\" to the past tense. In this case, a reasoning technique may support the problem of finding a certain rule behind relationships between source and target texts in the alignment phase (training phase). However, a well-known problem of reasoners is the poor run-time performance. Therefore, this run-time deficiency needs to be addressed or minimized before implementing reasoners successfully into MT systems.', 'Named Entities. Most NERD approaches link recognized entities with database entries or websites. This method helps to categorize and summarize text, but also contributes to the disambiguation of words in texts. The primary issue in MT systems is caused by common words from a source language that are used as proper nouns in a target language. For instance, the word “Kiwi\" is a family name in New Zealand which comes from the Māori culture, but it also can be a fruit, a bird, or a computer program. Named Entities are a common and difficult problem in both MT (see Koehn BIBREF0 ) and SW fields. The SW achieved important advances in NERD using structured data and semantic annotations, e.g., by adding an rdf:type statement which identifies whether a certain kiwi is a fruit BIBREF14 . In MT systems, however, this problem is directly related to the ambiguity problem and therefore has to be resolved in that wider context.', \"Non-standard speech. The non-standard language problem is a rather important one in the MT field. Many people use the colloquial form to speak and write to each other on social networks. Thus, when MT systems are applied on this context, the input text frequently contains slang, MWE, and unreasonable abbreviations such as “Idr = I don't remember.” and “cya = see you”. Additionally, idioms contribute to this problem, decreasing the translation quality. Idioms often have an entirely different meaning than their separated word meanings. Consequently, most translation outputs of such expressions contain errors. For a good translation, the MT system needs to recognize such slang and try to map it to the target language. Some SMT systems like Google or Bing have recognition patterns over non-standard speech from old translations through the Web using SMT approaches. In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open. Moreover, each person has their own speaking form.\"]\n",
            "['Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popović BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.']\n",
            "['(1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech.']\n",
            "['Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popović BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.']\n",
            "['Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popović BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.']\n",
            "[\"EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\", 'We used spectral entropy because it captures the spectral ( frequency domain) and signal complexity information of EEG. It is also a widely used feature in EEG signal analysis BIBREF18 . Similarly zero crossing rate was chosen as it is a commonly used feature both for speech recognition and bio signal analysis. Remaining features were chosen to capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the performance of the ASR system significantly went up by 20 % when we added the remaining additional features.', 'The recorded speech signal was sampled at 16KHz frequency. We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. The MFCC features were also sampled at 100Hz same as the sampling frequency of EEG features to avoid seq2seq problem.']\n",
            "[\"EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\", 'After extracting EEG and acoustic features as explained in the previous section, we used non linear methods to do feature dimension reduction in order to obtain set of EEG features which are better representation of acoustic features. We reduced the 155 EEG features to a dimension of 30 by applying Kernel Principle Component Analysis (KPCA) BIBREF19 .We plotted cumulative explained variance versus number of components to identify the right feature dimension as shown in Figure 2. We used KPCA with polynomial kernel of degree 3 BIBREF0 . We further computed delta, delta and delta of those 30 EEG features, thus the final feature dimension of EEG was 90 (30 times 3) for both the data sets.']\n",
            "[\"EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\"]\n",
            "[\"EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\"]\n",
            "['We built two types of simultaneous speech EEG recording databases for this work. For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment. Except two subjects, rest all were native English speakers for both the databases. All subjects were UT Austin undergraduate,graduate students in their early twenties.', 'For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in presence of background noise of 40 dB (noise generated by room air conditioner fan). We then asked each subject to repeat the same experiment two more times, thus we had 30 speech EEG recording examples for each sentence.', 'For data set B, the 8 subjects were asked to repeat the same previous experiment but this time we used background music played from our lab computer to generate a background noise of 65 dB. Here we had 24 speech EEG recording examples for each sentence.', 'We used Brain Vision EEG recording hardware. Our EEG cap had 32 wet EEG electrodes including one electrode as ground as shown in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes.']\n",
            "['We built two types of simultaneous speech EEG recording databases for this work. For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment. Except two subjects, rest all were native English speakers for both the databases. All subjects were UT Austin undergraduate,graduate students in their early twenties.']\n",
            "['We compare LadaBERT with state-of-the-art model compression approaches on five public datasets of different tasks of natural language understanding, including sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI) and pairwise semantic equivalence (QQP). The statistics of these datasets are described in Table TABREF27.', 'The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation.', 'FLOAT SELECTED: Table 3: Performance comparison on various model sizes']\n",
            "['We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.', 'One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.']\n",
            "['We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.']\n",
            "['One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.']\n",
            "['We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.']\n",
            "['Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization corpus) and thus used mostly for testing BIBREF7 . To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.', 'In a nutshell, we retrieved hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example). The highlights (created by news editors) are genuinely abstractive summaries and therefore not readily suited to supervised training. To create the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence BIBREF7 . Specifically, we designed a rule-based system that determines whether a document sentence matches a highlight and should be labeled with 1 (must be in the summary), and 0 otherwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We adjusted the weights of the rules on 9,000 documents with manual sentence labels created by woodsend2010automatic. The method obtained an accuracy of 85% when evaluated on a held-out set of 216 documents coming from the same dataset and was subsequently used to label 200K documents. Approximately 30% of the sentences in each document were deemed summary-worthy.', 'We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.']\n",
            "['We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.']\n",
            "['One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.', 'We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.']\n",
            "['In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology prediction experiments.', 'WordNet is a knowledge base (KB) of single words and relations between them such as hypernymy and meronymy. For our task, we use the hypernym relations only. ConceptNet is a KB of triples consisting of a left term $t_1$ , a relation $R$ , and a right term $t_2$ . The relations come from a fixed set of size 34. But unlike WordNet, terms in ConceptNet can be phrases. We focus on the Is-A relation in this work. MCG also consists of hierarchical relations between multi-word phrases, ranging from extremely general to specific. Examples from each dataset are shown in Table 1 .']\n",
            "['A core problem in artificial intelligence is to capture, in machine-usable form, the collection of information that an ordinary person would have, known as commonsense knowledge. For example, a machine should know that a room may have a door, and that when a person enters a room, it is generally through a door. This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines.']\n",
            "['Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.']\n",
            "['We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF).', 'Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between words. This can help with vocabulary generalization as models can adapt to words not previously seen in training data. In our feature set we include a 300-dimensional word2vec word representation trained on a large news corpus BIBREF36 . We obtain a representation for each segment by averaging the embedding of each word in the segment. We also experimented with the use of GloVe BIBREF37 , and Sent2Vec BIBREF38 , an extension of word2vec for sentences.', 'We use two sources of sentiment features: manually constructed lexica, and pre-trained sentiment embeddings. When available, manually constructed lexica are a useful resource for identifying expressions of sentiment BIBREF21 . We obtained word percentages across 192 lexical categories using Empath BIBREF39 , which extends popular tools such as the Linguistic Inquiry and Word Count (LIWC) BIBREF22 and General Inquirer (GI) BIBREF40 by adding a wider range of lexical categories. These categories include emotion classes such as surprise or disgust.']\n",
            "['Training data provided for the task included documents were collected from social media, SMS, news articles, and news wires. This consisted of 76 documents in English and 47 in Spanish. The data are relevant to the HADR domain but are not grounded in a common HADR incident. Each document is annotated for situation frames and associated sentiment by 2 trained annotators from the Linguistic Data Consortium (LDC). Sentiment annotations were done at a segment (sentence) level, and included Situation Frame, Polarity (positive / negative), Sentiment Score, Emotion, Source and Target. Sentiment labels were annotated between the values of -3 (very negative) and +3 (very positive) with 0.5 increments excluding 0. Additionally, the presence or absence of three specific emotions: fear, anger, and joy/happiness was marked. If a segment contains sentiment toward more than one target, each will be annotated separately. Summary of the training data is given in Table 2 .']\n",
            "['From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation – layout – which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing.']\n",
            "['AI2D and AI2D-RST share most node types that represent different diagram elements, namely text, graphics, arrows and the image constant, which is a node that stands for the entire diagram. In AI2D, generic diagram elements such as titles describing the entire diagram are typically connected to the image constant. In AI2D-RST, the image constant acts as the root node of the tree in the grouping graph. In addition to text, graphics, arrows and the image constant, AI2D-RST features two additional node types for groups and discourse relations, whereas AI2D includes an additional node for arrowheads. To summarise, AI2D contains five distinct node types, whereas AI2D-RST has six. Note, however, that only grouping and connectivity graphs used in this study, which limits the number to five for AI2D-RST.', 'The same features are used for both AI2D and AI2D-RST for nodes with layout information, namely text, graphics, arrows and arrowheads (in AI2D only). The position, size and shape of each diagram element are described using the following features: (1) the centre point of the bounding box or polygon, divided by the height and width of the diagram image, (2) area, or the number of pixels within the polygon, divided by the total number of pixels in the image, and (3) the solidity of the polygon, or the polygon area divided by the area of its convex hull. This yields a 4-dimensional feature vector describing the position and size of each diagram element in the layout. Each dimension is set to zero for grouping nodes in AI2D-RST and image constant nodes in AI2D and AI2D-RST.', 'AI2D-RST models discourse relations using nodes, which have a 25-dimensional, one-hot encoded feature vector to represent the type of discourse relation, which are drawn from Rhetorical Structure Theory BIBREF21. In AI2D, the discourse relations derived from engelhardt2002 are represented using a 10-dimensional one-hot encoded vector, which is associated with edges connecting diagram elements participating in the relation. Because the two resources draw on different theories and represent discourse relations differently, I use the grouping and connectivity graph for AI2D-RST representations and ignore the edge features in AI2D, as these descriptions attempt to describe roughly the same multimodal structures. A comparison of discourse relations is left for a follow-up study focusing on representing the discourse structure of diagrams.', 'Whereas AI2D encodes information about semantic relations using edges, in AI2D-RST the information carried by edges depends on the graph in question. The edges of the grouping graph do not have features, whereas the edges of the connectivity graph have a 3-dimensional, one-hot encoded vector that represents the type of connection. The edges of the discourse structure graph have a 2-dimensional, one-hot encoded feature vector to represent nuclearity, that is, whether the nodes that participate in a discourse relations act as nuclei or satellites.']\n",
            "['Table TABREF22 shows the results for node classification using various graph neural network architectures. Because the results are not entirely comparable due to different node types present in the two resources, it is more reasonable to compare architectures. SAGE, GCN and GAT clearly outperform SGC in classifying nodes from both resources, as does the random forest classifier. AI2D nodes are classified with particularly high accuracy, which may result from having to learn representations for only one node type, that is, the image constant ($N = 1000$). AI2D-RST, in turn, must learn representations from scratch for both image constants ($N = 1000$) and grouping nodes ($N = 7300$).']\n",
            "['The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.']\n",
            "['The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.']\n",
            "['The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.']\n",
            "['This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer.']\n",
            "['The majority of NLP-task related neural architectures rely on word embeddings, popularized by Mikolov et al BIBREF9 to represent texts. In essence these embeddings are latent-vector representations that aim to capture the underlying meaning of words. Distances between such latent-vectors are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g. pretrained word embeddings) and create higher level representations of input (e.g. convolutional feature maps or hidden-states). These properties suggest that neural approaches are better able to generalize to unseen examples that poorly match the training set. We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question. RQ: Can we increase robustness of controversy detection using neural methods?']\n",
            "['We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).']\n",
            "['We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).']\n",
            "['We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).']\n",
            "[\"We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .\"]\n",
            "['We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).', \"We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .\"]\n",
            "[\"We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .\"]\n",
            "[\"We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .\"]\n",
            "[\"Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection. To this end, we focus only on fulltext features and suggest to leverage the semantic representations of word embeddings to reduce the vocabulary-gap for unseen topics and exact lexical representations.\"]\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table .']\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.']\n",
            "['For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism.']\n",
            "['For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism.']\n",
            "['BIBREF1 first proposed anchors in the context of topic modeling: words that are high precision indicators of underlying topics. In contrast to our approach, anchors are typically selected automatically, constrained to appear in only one topic, and used primarily to aid optimization BIBREF17 . In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the interpretability of traditional topic models are often mixed BIBREF18 , but our experiments suggest that our approach yields more coherent topics.']\n",
            "['To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics.']\n",
            "['To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics.']\n",
            "['To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics.']\n",
            "['To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics.']\n",
            "['Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.']\n",
            "['Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.']\n",
            "['Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.']\n",
            "['We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.', 'FLOAT SELECTED: Table 1: Details of Weibo NER corpus.']\n",
            "['We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.']\n",
            "['We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.']\n",
            "['We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.']\n",
            "['We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.']\n",
            "['As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER. We detail all of the experiments in this section.']\n",
            "['As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER. We detail all of the experiments in this section.']\n",
            "['As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER. We detail all of the experiments in this section.']\n",
            "['As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER. We detail all of the experiments in this section.']\n",
            "[\"While zero-shot transfer is a good measure of a model's natural cross-lingual effectiveness, the more practical setting is the few-shot transfer scenario as we almost always have access to, or can cheaply acquire, a small amount of data in the target language. We report the few-shot transfer results of mBERT and MMTE on the POS tagging dataset in TABREF33. To simulate the few-shot setting, in addition to using English data, we use 10 examples from each language (upsampled to 1000). MMTE outperforms mBERT in few-shot setting by 0.6 points averaged over 48 languages. Once again, we see that the gains are more pronounced in low resource languages.\"]\n",
            "['We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.', 'We use universal dependencies POS tagging data from the Universal Dependency v2.3 BIBREF6, BIBREF20. Gold segmentation is used for training, tuning and testing. The POS tagging task has 17 labels for all languages. We consider 48 different languages. These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model. The task-specific network consists of a one layer feed-forward neural network with 784 units. Since MMTE operates on the subword-level, we only consider the representation of the first subword token of each word. The optimizer used is Adafactor with learning rate schedule (0.1,40k). The evaluation metric used is F1-score, which is same as accuracy in our case since we use gold-segmented data. Results of both in-language and zero-shot setting are reported in Table TABREF27.']\n",
            "['Given the wide distribution of data across language pairs, we used a temperature based data balancing strategy. For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\\\frac{D_l}{\\\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.']\n",
            "['We use universal dependencies POS tagging data from the Universal Dependency v2.3 BIBREF6, BIBREF20. Gold segmentation is used for training, tuning and testing. The POS tagging task has 17 labels for all languages. We consider 48 different languages. These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model. The task-specific network consists of a one layer feed-forward neural network with 784 units. Since MMTE operates on the subword-level, we only consider the representation of the first subword token of each word. The optimizer used is Adafactor with learning rate schedule (0.1,40k). The evaluation metric used is F1-score, which is same as accuracy in our case since we use gold-segmented data. Results of both in-language and zero-shot setting are reported in Table TABREF27.']\n",
            "['Experiments ::: Evaluation Metrics', 'Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset.', 'Experiments ::: Evaluation Metrics ::: Automatic', 'For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models.', 'Following ACUTE-EVAL, the annotator is provided with two full dialogues made by self-chat or human-dialogue. The annotator is asked to choose which of the two dialogues is better in terms of engagingness, interestingness, and humanness. For each comparison, we sample 60–100 conversations from both models. In Appendix C, we report the exact questions and instructions given to the annotators, and the user interface used in the evaluation. We hired native speakers annotators for all six considered languages. The annotators were different from the dataset collection annotators to avoid any possible bias.']\n",
            "['For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models.', 'Asking humans to evaluate the quality of a dialogue model is challenging, especially when multiple models have to be compared. The likert score (a.k.a. 1 to 5 scoring) has been widely used to evaluate the interactive experience with conversational models BIBREF70, BIBREF65, BIBREF0, BIBREF1. In such evaluation, a human interacts with the systems for several turns, and then they assign a score from 1 to 5 based on three questions BIBREF0 about fluency, engagingness, and consistency. This evaluation is both expensive to conduct and requires many samples to achieve statistically significant results BIBREF6. To cope with these issues, BIBREF6 proposed ACUTE-EVAL, an A/B test evaluation for dialogue systems. The authors proposed two modes: human-model chats and self-chat BIBREF71, BIBREF72. In this work, we opt for the latter since it is cheaper to conduct and achieves similar results BIBREF6 to the former. Another advantage of using this method is the ability to evaluate multi-turn conversations instead of single-turn responses.']\n",
            "['Table TABREF20 compares monolingual, multilingual, and cross-lingual models in terms of BLEU and perplexity in the human-translated test set. On both evaluation matrices, the causal decoder models outperform the encoder-decoder models. We observe that the encoder-decoder model tends to overlook dialogue context and generate digressive responses. (Generated samples are available in Appendix D) We hypothesize that this is because the one-to-many problem BIBREF76 in open-domain conversation weakens the relation between encoder and decoder; thus the well pre-trained decoder (Bert) easily converges to a locally-optimal, and learns to ignore the dialogue context from the encoder and generate the response in an unconditional language model way. We leave the investigation of this problem to future work. On the other hand, M-CausalBert achieves a comparable or slightly better performance compared to CausalBert, which suggests that M-CausalBert leverages the data from other languages. As expected, we observe a significant gap between the cross-lingual model and other models, which indicates that cross-lingual zero-shot conversation modeling is very challenging.', 'FLOAT SELECTED: Table 3: Results of automatic evaluation score on test set in seven languages. We compute the BLEU score and perplexity (ppl.) for monolingual, multilingual, and cross-lingual models.']\n",
            "['Experiments ::: Implementation Details ::: Multilingual Models', 'We use the \"BERT-Base, Multilingual Cased\" checkpoint, and we denote the multilingual encoder-decoder model as M-Bert2Bert ($\\\\sim $220M parameters) and causal decoder model as M-CausalBert ($\\\\sim $110M parameters). We fine-tune both models in the combined training set (English in Persona-chat BIBREF0, six languages in Xpersona) for five epochs with AdamW optimizer and a learning rate of $6.25e$-5.', 'Experiments ::: Implementation Details ::: Monolingual Models', 'To verify whether the multilingual agent will under-perform the monolingual agent in the monolingual conversational task, we build a monolingual encoder-decoder model and causal decoder model for each language. For a fair comparison, we initialize the monolingual models with a pre-trained monolingual BERT BIBREF5, BIBREF73, BIBREF74. We denote the monolingual encoder-decoder model as Bert2Bert ($\\\\sim $220M parameters) and causal decoder model as CausalBert ($\\\\sim $110M parameters). Then we fine-tune each model in each language independently for the same number of epoch and optimizer as the multilingual model.', 'Experiments ::: Implementation Details ::: Translation-based Models', 'Another strong baseline we compare with is Poly-encoder BIBREF75, a large-scale pre-trained retrieval model that has shown state-of-the-art performance in the English Persona-chat dataset BIBREF6. We adapt this model to the other languages by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language. Thus, the response generation flow is: target query $\\\\rightarrow $ English query $\\\\rightarrow $ English response $\\\\rightarrow $ target response. We denote this model as Poly.', 'Experiments ::: Implementation Details ::: Cross-lingual Models.', 'In the first pre-training stage, we use the pre-trained weights from XLMR-base BIBREF60. Then, we follow the second pre-training stage of XNLG BIBREF4 for pre-training Italian, Japanese, Korean, Indonesia cross-lingual transferable models. For Chinese and French, we directly apply the pre-trained XNLG BIBREF4 weights. Then, the pre-trained models are fine-tune on English PersonaChat training set and early stop based on the perplexity on target language validation set.']\n",
            "['Another strong baseline we compare with is Poly-encoder BIBREF75, a large-scale pre-trained retrieval model that has shown state-of-the-art performance in the English Persona-chat dataset BIBREF6. We adapt this model to the other languages by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language. Thus, the response generation flow is: target query $\\\\rightarrow $ English query $\\\\rightarrow $ English response $\\\\rightarrow $ target response. We denote this model as Poly.']\n",
            "['To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages.']\n",
            "['Personalized dialogue agents have been shown efficient in conducting human-like conversation. This progress has been catalyzed thanks to existing conversational dataset such as Persona-chat BIBREF0, BIBREF1. However, the training data are provided in a single language (e.g., English), and thus the resulting systems can perform conversations only in the training language. For wide, commercial dialogue systems are required to handle a large number of languages since the smart home devices market is increasingly international BIBREF2. Therefore, creating multilingual conversational benchmarks is essential, yet challenging since it is costly to perform human annotation of data in all languages.', 'To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages.']\n",
            "['The proposed XPersona dataset is an extension of the persona-chat dataset BIBREF0, BIBREF1. Specifically, we extend the ConvAI2 BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. Since the test set of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native speaker annotators with a fluent level of English and asked them to revise the machine-translated dialogues and persona sentences in the validation set and test set according to original English dialogues. The main goal of human annotation is to ensure the resulting conversations are coherent and fluent despite the cultural differences in target languages. Therefore, annotators are not restricted to only translate the English dialogues, and they are allowed to modify the original dialogues to improve the dialogue coherence in the corresponding language while retaining the persona information. The full annotation instructions are reported in Appendix A.']\n",
            "['To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages.']\n",
            "['We consider the following recurrent baselines:', 'baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .', 's-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.', 's-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.', 's-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .', 'baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .', 'concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.', 'concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .']\n",
            "['baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'All remaining models are based on the Transformer architecture BIBREF2 . A Transformer avoids recurrence completely: it follows an encoder-decoder architecture using stacked self-attention and fully connected layers for both the encoder and decoder.', 'baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .']\n",
            "['We consider the following recurrent baselines:', 'baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .', 's-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.', 's-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.', 's-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .', 'baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .', 'concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.', 'concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .', 'BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work.']\n",
            "['We consider the following recurrent baselines:', 'baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .', 's-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.', 's-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.', 's-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .', 'baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .', 'concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.', 'concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .', 'BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work.']\n",
            "['Section SECREF2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section SECREF3 describes our test suite. The context-aware models we use in our experiments are detailed in Section SECREF4 . We discuss our experiments in Section SECREF5 and the results in Section SECREF6 .', 'baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .', 's-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.', 's-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.', 's-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .', 'baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .', 'concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.', 'concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .', 'BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work.']\n",
            "['This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.', 'baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.', 'concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .', 's-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.', 's-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.', 's-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .', 'concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.', 'concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .', 'BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work.']\n",
            "['We train all models on the data from the WMT 2017 English INLINEFORM0 German news translation shared task ( INLINEFORM1 5.8 million sentence pairs). These corpora do not have document boundaries, therefore a small fraction of sentences will be paired with wrong context, but we expect the model to be robust against occasional random context (see also BIBREF8 ). Experimental setups for the RNN and Transformer models are different, and we describe them separately.']\n",
            "['We automatically create a test set from the OpenSubtitles corpus BIBREF22 . The goal is to provide a large number of difficult test cases where an English pronoun has to be translated to a German pronoun.']\n",
            "['We train all models on the data from the WMT 2017 English INLINEFORM0 German news translation shared task ( INLINEFORM1 5.8 million sentence pairs). These corpora do not have document boundaries, therefore a small fraction of sentences will be paired with wrong context, but we expect the model to be robust against occasional random context (see also BIBREF8 ). Experimental setups for the RNN and Transformer models are different, and we describe them separately.']\n",
            "['In this paper, we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string. In our FET approach, the commonness score is also used as the confidence on the linking result (i.e., the $\\\\mathbf {g}$ used in the prediction part of Subsection SECREF5). Within a same document, we also use the same heuristic used in BIBREF19 to find coreferences of generic mentions of persons (e.g., “Matt”) to more specific mentions (e.g., “Matt Damon”).']\n",
            "['Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention.', 'FLOAT SELECTED: Figure 1: Our approach. The example sentence is “Earlier on Tuesday, Donald Trump pledged to help hard-hit U.S. farmers caught in the middle of the escalating trade war.” Here, the correct label for the mention Donald Trump should be /person, /person/politician. “[Mention]” is a special token that we use to represent the mention.', 'To obtain the context representation, we first use a special token $w_m$ to represent the mention (the token “[Mention]” in Figure FIGREF4). Then, the word sequence of the sentence becomes $w_1,...,w_{p_l-1},w_m,w_{p_l+1},...,w_n$. Their corresponding word embeddings are fed into two layers of BiLSTMs. Let $\\\\mathbf {h}_m^1$ and $\\\\mathbf {h}_m^2$ be the output of the first and the second layer of BiLSTMs for $w_m$, respectively. We use $\\\\mathbf {f}_c=\\\\mathbf {h}_m^1+\\\\mathbf {h}_m^2$ as the context representation vector.', 'Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\\\mathbf {g}$. Then, we get $\\\\mathbf {f}=\\\\mathbf {f}_c\\\\oplus \\\\mathbf {f}_s\\\\oplus \\\\mathbf {f}_e\\\\oplus \\\\mathbf {g}$, where $\\\\oplus $ means concatenation. $\\\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\\\mathbf {t}_1,...,\\\\mathbf {t}_k$. Then the score of the mention $m$ having the type $t_i\\\\in T$ is calculated as the dot product of $\\\\mathbf {u}_m$ and $\\\\mathbf {t}_i$:']\n",
            "['To obtain the context representation, we first use a special token $w_m$ to represent the mention (the token “[Mention]” in Figure FIGREF4). Then, the word sequence of the sentence becomes $w_1,...,w_{p_l-1},w_m,w_{p_l+1},...,w_n$. Their corresponding word embeddings are fed into two layers of BiLSTMs. Let $\\\\mathbf {h}_m^1$ and $\\\\mathbf {h}_m^2$ be the output of the first and the second layer of BiLSTMs for $w_m$, respectively. We use $\\\\mathbf {f}_c=\\\\mathbf {h}_m^1+\\\\mathbf {h}_m^2$ as the context representation vector.']\n",
            "['We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on.']\n",
            "['We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on.']\n",
            "['We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on.']\n",
            "[\"In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.\"]\n",
            "[\"In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.\"]\n",
            "[\"In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.\"]\n",
            "[\"In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.\"]\n",
            "['CONCEPT RECOGNITION ::: BIO Labelling Scheme', 'abb: represents abbreviations such as TRL representing Technology Readiness Level.', 'grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.', 'syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.', 'opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.', 'seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.', 'event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.', \"org: represents an organization such as `NASA', `aerospace industry', etc.\", \"art: represents names of artifacts or instruments such as `AS1300'\", \"cardinal: represents numerical values such as `1', `100', 'one' etc.\", 'loc: represents location-like entities such as component facilities or centralized facility.', 'mea: represents measures, features, or behaviors such as cost, risk, or feasibility.']\n",
            "['Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.']\n",
            "['Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.']\n",
            "['Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.']\n",
            "['Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1.']\n",
            "['Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1.']\n",
            "['Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1.']\n",
            "['To evaluate the classification performance, precision, recall and F-measure were computed.']\n",
            "['To evaluate the classification performance, precision, recall and F-measure were computed.']\n",
            "['Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach?']\n",
            "[\"One of the crucial tasks for researchers to carry out scientific investigations is to detect existing ideas that are related to their research topics. Research ideas are usually documented in scientific publications. Normally, there is one main idea stated in the abstract, explicitly presenting the aim of the paper. There are also other sub-ideas distributed across the entire paper. As the growth rate of scientific publication has been rising dramatically, researchers are overwhelmed by the explosive information. It is almost impossible to digest the ideas contained in the documents emerged everyday. Therefore, computer assisted technologies such as document summarization are expected to play a role in condensing information and providing readers with more relevant short texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual.\"]\n",
            "[\"The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 .\"]\n",
            "[\"The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 .\", 'To compare the three models effectiveness on the AZ task, the three models on a same ACL dataset (introduced int he dataset section) were trained. The word2vec were also trained using different parameters, such as different dimension of features. To evaluate the impact from different domains, the first model was trained on different corpus.']\n",
            "['The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :', 'INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 (4)', 'where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 .', \"The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 .\"]\n",
            "['In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors.']\n",
            "['The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :', 'The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 .', \"The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 .\"]\n",
            "[\"One of the crucial tasks for researchers to carry out scientific investigations is to detect existing ideas that are related to their research topics. Research ideas are usually documented in scientific publications. Normally, there is one main idea stated in the abstract, explicitly presenting the aim of the paper. There are also other sub-ideas distributed across the entire paper. As the growth rate of scientific publication has been rising dramatically, researchers are overwhelmed by the explosive information. It is almost impossible to digest the ideas contained in the documents emerged everyday. Therefore, computer assisted technologies such as document summarization are expected to play a role in condensing information and providing readers with more relevant short texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual.\"]\n",
            "[\"One of the crucial tasks for researchers to carry out scientific investigations is to detect existing ideas that are related to their research topics. Research ideas are usually documented in scientific publications. Normally, there is one main idea stated in the abstract, explicitly presenting the aim of the paper. There are also other sub-ideas distributed across the entire paper. As the growth rate of scientific publication has been rising dramatically, researchers are overwhelmed by the explosive information. It is almost impossible to digest the ideas contained in the documents emerged everyday. Therefore, computer assisted technologies such as document summarization are expected to play a role in condensing information and providing readers with more relevant short texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual.\"]\n",
            "[\"One of the crucial tasks for researchers to carry out scientific investigations is to detect existing ideas that are related to their research topics. Research ideas are usually documented in scientific publications. Normally, there is one main idea stated in the abstract, explicitly presenting the aim of the paper. There are also other sub-ideas distributed across the entire paper. As the growth rate of scientific publication has been rising dramatically, researchers are overwhelmed by the explosive information. It is almost impossible to digest the ideas contained in the documents emerged everyday. Therefore, computer assisted technologies such as document summarization are expected to play a role in condensing information and providing readers with more relevant short texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual.\"]\n",
            "[\"We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.\"]\n",
            "[\"We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.\"]\n",
            "['Spam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.', 'Spam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.']\n",
            "[\"We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.\"]\n",
            "[\"We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.\"]\n",
            "[\"We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.\"]\n",
            "['We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs.']\n",
            "['We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs.']\n",
            "['Related Work ::: Monolingual Pre-Training', 'We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs.']\n",
            "['We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs.']\n",
            "['We use the denoising auto-encoding (DAE) objective BIBREF24 to pretrain the encoder-decoder attention mechanism. Given sentence $x$ from the monolingual corpus, we use three types of noise to obtain the randomly perturbed text $\\\\hat{x}$. First, the word order is locally shuffled. Second, we randomly drop tokens of the sentence with a probability of $0.1$. Third, we substitute tokens with the special padding token P with a probability of $0.1$. The pre-training objective is to recover the original sentence $x$ by conditioning on $\\\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\\\cdots ,x_{i-1}$.']\n",
            "['We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs.']\n",
            "['We use the denoising auto-encoding (DAE) objective BIBREF24 to pretrain the encoder-decoder attention mechanism. Given sentence $x$ from the monolingual corpus, we use three types of noise to obtain the randomly perturbed text $\\\\hat{x}$. First, the word order is locally shuffled. Second, we randomly drop tokens of the sentence with a probability of $0.1$. Third, we substitute tokens with the special padding token P with a probability of $0.1$. The pre-training objective is to recover the original sentence $x$ by conditioning on $\\\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\\\cdots ,x_{i-1}$.']\n",
            "['We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:', 'CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.', 'Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.', 'Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.', 'We conduct experiments on the zero-shot Chinese-Chinese QG task to evaluate the cross-lingual transfer ability. In this task, models are trained with English QG data but evaluated with Chinese QG examples. We include the following models as our baselines:', 'Xlm Fine-tuning XLM with the English QG data.', 'Pipeline (Xlm) The pipeline of translating input Chinese sentences into English first, then performing En-En-QG with the XLM model, and finally translating back to the Chinese. We use the Transformer as the translator, which is also trained on the MultiUN dataset.', 'Pipeline (Xlm) with Google Translator Same to Pipeline (Xlm) but using Google Translator to translate the texts.']\n",
            "['We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:', 'CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.', 'Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.', 'Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.', 'We conduct experiments on the zero-shot Chinese-Chinese QG task to evaluate the cross-lingual transfer ability. In this task, models are trained with English QG data but evaluated with Chinese QG examples. We include the following models as our baselines:', 'Xlm Fine-tuning XLM with the English QG data.', 'Pipeline (Xlm) The pipeline of translating input Chinese sentences into English first, then performing En-En-QG with the XLM model, and finally translating back to the Chinese. We use the Transformer as the translator, which is also trained on the MultiUN dataset.', 'Pipeline (Xlm) with Google Translator Same to Pipeline (Xlm) but using Google Translator to translate the texts.']\n",
            "['We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:', 'CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.', 'Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.', 'Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.']\n",
            "['For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.', 'Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.']\n",
            "[\"For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts—9 randomly sampled prompts and 1 true corresponding prompt—and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model.\"]\n",
            "[\"For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts—9 randomly sampled prompts and 1 true corresponding prompt—and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model.\"]\n",
            "[\"For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts—9 randomly sampled prompts and 1 true corresponding prompt—and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model.\"]\n",
            "['We evaluate a number of baselines:', '(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism.', '(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.', '(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.', '(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories.']\n",
            "['We evaluate a number of baselines:', '(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism.', '(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.', '(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.', '(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories.']\n",
            "['We evaluate a number of baselines:', '(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism.', '(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.', '(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.', '(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories.']\n",
            "['High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.']\n",
            "['High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.']\n",
            "['High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.']\n",
            "['To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation.']\n",
            "[\"We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example.\"]\n",
            "[\"We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example.\"]\n",
            "['We study five pre-trained word embeddings for our model:', 'word2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.', 'fastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.', 'GloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).', 'Baroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.', 'SL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .']\n",
            "['We study five pre-trained word embeddings for our model:', 'word2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.', 'GloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).', 'Baroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.', 'fastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.', 'SL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .']\n",
            "['We study five pre-trained word embeddings for our model:', 'word2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.', 'fastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.', 'GloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).', 'Baroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.', 'SL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .']\n",
            "['We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.', 'In STSB and MRPC, as employing the five pre-trained embeddings, the INLINEFORM0 is increased. This factor limits the number of random values when initializing word embedding representations because a word out of a pre-trained word embedding is assigned a random word embedding representation. In other words, a word out of a pre-trained word embedding is assigned a random semantic meaning. Therefore, the increase of the INLINEFORM1 improves the performance of measuring textual similarity. In STSB and MRPC, our multiple pre-trained word embedding achieves a significant improvement in performance compared against using one word embedding. In SICK-R and SICK-E, although the INLINEFORM2 is not increased when employing five pre-trained embeddings, the performance of our model is improved. This fact shows that our model learned an efficient word embedding via these pre-trained word embeddings.']\n",
            "['In STSB and MRPC, as employing the five pre-trained embeddings, the INLINEFORM0 is increased. This factor limits the number of random values when initializing word embedding representations because a word out of a pre-trained word embedding is assigned a random word embedding representation. In other words, a word out of a pre-trained word embedding is assigned a random semantic meaning. Therefore, the increase of the INLINEFORM1 improves the performance of measuring textual similarity. In STSB and MRPC, our multiple pre-trained word embedding achieves a significant improvement in performance compared against using one word embedding. In SICK-R and SICK-E, although the INLINEFORM2 is not increased when employing five pre-trained embeddings, the performance of our model is improved. This fact shows that our model learned an efficient word embedding via these pre-trained word embeddings.', 'In this section, we evaluate the efficiency of using multiple pre-trained word embeddings. We compare our multiple pre-trained word embeddings model against models using only one pre-trained word embedding. The same objective function and Multi-level comparison are applied for these models. In case of using one pre-trained word embedding, the dimension of LSTM and the number of convolutional filters are set to the length of the corresponding word embedding. Table TABREF57 shows the experimental results of this comparison. Because the approach using five word embeddings outperforms the approaches using two, three, or four word embeddings, we only report the performance of using five word embeddings. We also report INLINEFORM0 which is the proportion of vocabulary available in a pre-trained word embedding. SICK dataset ignores idiomatic multi-word expressions, and named entities, consequently the INLINEFORM1 of SICK is quite high.']\n",
            "['We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.', 'We evaluate our M-MaxLSTM-CNN model on three tasks: STS, textual entailment recognition, paraphrase identification. The advantages of M-MaxLSTM-CNN are: i) simple but efficient for combining various pre-trained word embeddings with different dimensions; ii) using Multi-level comparison shows better performances compared to using only sentence-sentence comparison; iii) does not require hand-crafted features (e.g., alignment features, Ngram overlaps, syntactic features, dependency features) compared to the state-of-the-art ECNU BIBREF6 on STS Benchmark dataset.']\n",
            "['We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.']\n",
            "['We evaluate our M-MaxLSTM-CNN model on three tasks: STS, textual entailment recognition, paraphrase identification. The advantages of M-MaxLSTM-CNN are: i) simple but efficient for combining various pre-trained word embeddings with different dimensions; ii) using Multi-level comparison shows better performances compared to using only sentence-sentence comparison; iii) does not require hand-crafted features (e.g., alignment features, Ngram overlaps, syntactic features, dependency features) compared to the state-of-the-art ECNU BIBREF6 on STS Benchmark dataset.', 'We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.']\n",
            "[\"We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.\"]\n",
            "[\"We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.\"]\n",
            "[\"We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.\"]\n",
            "[\"We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.\"]\n",
            "[\"We presented each tweet in the dataset to three separate annotators as a Human Intelligence Task (HIT) on Amazon's Mechanical Turk (MTurk) platform. By the time of recruitment, 6,897 of the 9,803 aggressive tweets were accessible from the Twitter web page. The remainder of the tweets had been removed, or the Twitter account had been locked or suspended.\"]\n",
            "[\"Network features have been shown to improve text-based models BIBREF6, BIBREF25, and they can help classifiers distinguish between bullies and victims BIBREF32. These features may also capture some of the more social aspects of cyberbullying, such as power imbalance and visibility among peers. However, many centrality measures and clustering algorithms require detailed network representations. These features may not be scalable for real-world applications. We propose a set of low-complexity measurements that can be used to encode important higher-order relations at scale. Specifically, we measure the relative positions of the author and target accounts in the directed following network by computing modified versions of Jaccard's similarity index as we now explain.\", 'Feature Engineering ::: Social Network Features ::: Neighborhood Overlap', 'Feature Engineering ::: Social Network Features ::: User-based features', 'We also use basic user account metrics drawn from the author and target profiles. Specifically, we count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines, as in BIBREF11, BIBREF4, and BIBREF8.']\n",
            "[\"We asked our annotators to consider the full message thread for each tweet as displayed on Twitter's web interface. We also gave them a list of up to 15 recent mentions by the author of the tweet, directed towards any of the other accounts mentioned in the original thread. Then we asked annotators to interpret each tweet in light of this social context, and had them provide us with labels for five key cyberbullying criteria. We defined these criteria in terms of the author account (“who posted the given tweet?”) and the target (“who was the tweet about?” – not necessarily the first mention). We also stated that “if the target is not on Twitter or their handle cannot be identified” the annotator should “please write OTHER.” With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\", \"Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\", 'Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).', 'Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.', \"Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\", 'Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.']\n",
            "[\"We asked our annotators to consider the full message thread for each tweet as displayed on Twitter's web interface. We also gave them a list of up to 15 recent mentions by the author of the tweet, directed towards any of the other accounts mentioned in the original thread. Then we asked annotators to interpret each tweet in light of this social context, and had them provide us with labels for five key cyberbullying criteria. We defined these criteria in terms of the author account (“who posted the given tweet?”) and the target (“who was the tweet about?” – not necessarily the first mention). We also stated that “if the target is not on Twitter or their handle cannot be identified” the annotator should “please write OTHER.” With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\", \"Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\", 'Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).', 'Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.', \"Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\", 'Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.', 'Each of these criteria was represented as a binary label, except for power imbalance, which was ternary. We asked “Is there strong evidence that the author is more powerful than the target? Is the target more powerful? Or if there is not any good evidence, just mark equal.” We recognized that an imbalance of power might arise in a number of different circumstances. Therefore, we did not restrict our definition to just one form of power, such as follower count or popularity.']\n",
            "[\"We asked our annotators to consider the full message thread for each tweet as displayed on Twitter's web interface. We also gave them a list of up to 15 recent mentions by the author of the tweet, directed towards any of the other accounts mentioned in the original thread. Then we asked annotators to interpret each tweet in light of this social context, and had them provide us with labels for five key cyberbullying criteria. We defined these criteria in terms of the author account (“who posted the given tweet?”) and the target (“who was the tweet about?” – not necessarily the first mention). We also stated that “if the target is not on Twitter or their handle cannot be identified” the annotator should “please write OTHER.” With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\", \"Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\", 'Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).', 'Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.', \"Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\", 'Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.']\n",
            "['We have established that cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression. Standard Bag of Words (BoW) features based on single sentences, such as $n$-grams and word embeddings, may thus lead machine learning algorithms to incorrectly classify friendly or joking behavior as cyberbullying BIBREF12, BIBREF10, BIBREF9. To more reliably capture the nuances of repetition, harmful intent, visibility among peers, and power imbalance, we designed a new set of features from the social and linguistic traces of Twitter users. These measures allow our classifiers to encode the dynamic relationship between the message author and target, using network and timeline similarities, expectations from language models, and other signals taken from the message thread.']\n",
            "['We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively. We also checked the syntactic correctness of the generated sentences.']\n",
            "['We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively. We also checked the syntactic correctness of the generated sentences.', 'For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria. The Phrase Overlap criterion introduced by BIBREF9 and extended by BIBREF10 is chosen because it gives more weight to multi token phrase overlaps as its rarer. DISPLAYFORM0 DISPLAYFORM1', 'To evaluate the syntactic correctness of the generated sentences we have used the Link Grammar Parser for English language developed by BIBREF11 which uses the idea of linkages. A linkage is the relationship between different words in a sentence from a syntactic point of view. We use the Link Grammar Parser to find number of valid linkages of the sentence after post processing. We break the generated jokes, quotes, tweets into individual sentences. Since the parsing is dependent on capitalization and our training data has been normalized to all small letters, we add necessary capitalization before feeding sentences to the parser. Here are examples of linkage diagram produced by syntactic parsing of a generated sentence. The lines represents the links between the words and each link is labeled with the type of link. Details about the link labels can be found in the documentation of the parser.', 'To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment as there is no proven system to measure the quality of content objectively.']\n",
            "['For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria. The Phrase Overlap criterion introduced by BIBREF9 and extended by BIBREF10 is chosen because it gives more weight to multi token phrase overlaps as its rarer. DISPLAYFORM0 DISPLAYFORM1']\n",
            "[\"Our training data consists of jokes, quotes, and tweets from different sources. We combined multiple sources and de-duplicated them to arrive at a large corpus for training. The two sources for jokes are CrowdTruth and Subreddits. After cleaning, we ended up with 96910 jokes and a vocabulary size of 8922 words. The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences. The final datasets can be found on our github repository. When we train the controlled LSTM with the combined data, we use weighted sample strategy so that the three categories contribute equally to loss even though their numbers are different.\"]\n",
            "[\"Our training data consists of jokes, quotes, and tweets from different sources. We combined multiple sources and de-duplicated them to arrive at a large corpus for training. The two sources for jokes are CrowdTruth and Subreddits. After cleaning, we ended up with 96910 jokes and a vocabulary size of 8922 words. The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences. The final datasets can be found on our github repository. When we train the controlled LSTM with the combined data, we use weighted sample strategy so that the three categories contribute equally to loss even though their numbers are different.\"]\n",
            "[\"Our training data consists of jokes, quotes, and tweets from different sources. We combined multiple sources and de-duplicated them to arrive at a large corpus for training. The two sources for jokes are CrowdTruth and Subreddits. After cleaning, we ended up with 96910 jokes and a vocabulary size of 8922 words. The two sources for quotes are Quotables and the TheWebMiner. After cleaning, we ended up with 43383 quotes and a vocabulary size of 8916 words. We downloaded the scraped tweets from kaggle and ended up with 130250 tweets with a vocabulary size of 10805 words after cleaning. We constrained the vocabulary to about 10000 words in each case. Finally, we combined the jokes, quotes, and tweets along with their class labels (joke is 0, quote is 1, tweet is 2) into a single unified dataset. The combined dataset consists of 270543 sentences and a vocabulary size of 12614 words. Each sentence starts with a 'sos' tag and ends with a 'eos' tag to denote the start and end of sentences. The final datasets can be found on our github repository. When we train the controlled LSTM with the combined data, we use weighted sample strategy so that the three categories contribute equally to loss even though their numbers are different.\"]\n",
            "['A controlled LSTM can be used to train a network in a supervised way on multiple categorical data like jokes, quotes, and tweets by augmenting the category tag to the input word at every time-step. This way the neural net can learn the difference in the semantics of a joke and quote and generate more creative content using the mix of knowledge gained by training in a supervised manner on multiple categorical data. We show how our model is able to generate a joke vs quote depending on the category input for the same prefix of words. We also found that a network trained on the combined dataset generated fewer offensive jokes compared to the one trained on just the jokes (as the jokes scraped from the internet were offensive with high probability). This is the first time anyone has used controlled LSTM architecture to generate texts with different sentiments. We show how the network learns to introduce incongruities in the generated text (making it funny) when asked to generate a joke as opposed to a quote (which is inspirational).With the current resurgence of deep-neural networks and its astounding success in natural language generation, our paper tries to achieve the above goal.']\n",
            "['Table TABREF10 compares the experimental results of our CapsE with previous state-of-the-art published results, using the same evaluation protocol. Our CapsE performs better than its closely related CNN-based model ConvKB on both experimental datasets (except Hits@10 on WN18RR and MR on FB15k-237), especially on FB15k-237 where our CapsE gains significant improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), and INLINEFORM1 % absolute improvement in Hits@10. Table TABREF10 also shows that our CapsE obtains the best MR score on WN18RR and the highest MRR and Hits@10 scores on FB15k-237.']\n",
            "['In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.', 'In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .', \"Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again.\"]\n",
            "['In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.', 'In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .', 'We use “Quora-Question-Pair” dataset 1. This is a commonly used dataset containing 400k question pairs, annotated manually to be semantically equivalent or not. Due to its high quality, it is a standard dataset to test the success of various semantic similarity tasks. Various models which are tested on this data set are proposed, including but not limited to BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 .']\n",
            "['In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.', 'In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .', 'We use “Quora-Question-Pair” dataset 1. This is a commonly used dataset containing 400k question pairs, annotated manually to be semantically equivalent or not. Due to its high quality, it is a standard dataset to test the success of various semantic similarity tasks. Various models which are tested on this data set are proposed, including but not limited to BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 .']\n",
            "['In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.', 'In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .', 'We use “Quora-Question-Pair” dataset 1. This is a commonly used dataset containing 400k question pairs, annotated manually to be semantically equivalent or not. Due to its high quality, it is a standard dataset to test the success of various semantic similarity tasks. Various models which are tested on this data set are proposed, including but not limited to BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 .']\n",
            "['The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.', \"tab:iaa-results shows raw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons. Agreement levels on scene role, function, and full construal are high for both phases, attesting to the validity of the annotation framework in Chinese. However, there is a slight decrease from Phase 1 to Phase 2, possibly due to the seven newly attested adpositions in Phase 2 and the 1-year interval between the two annotation phases.\"]\n",
            "['The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.']\n",
            "['Corpus Annotation ::: Preprocessing ::: Tokenization', 'After automatic tokenization using Jieba, we conducted manual corrections to ensure that all potential adpositions occur as separate tokens, closely following the Chinese Penn Treebank segmentation guidelines BIBREF39. The final corpus includes all 27 chapters of The Little Prince, with a total of 20k tokens.', 'Corpus Annotation ::: Preprocessing ::: Adposition Targets', 'All annotators jointly identified adposition targets according to the criteria discussed in subsec:adpositioncriteria. Manual identification of adpositions was necessary as an automatic POS tagger was found unsuitable for our criteria (sec:adpositionidentification).', 'Corpus Annotation ::: Preprocessing ::: Data Format', 'Though parsing is not essential to this annotation project, we ran the StanfordNLP BIBREF40 dependency parser to obtain POS tags and dependency trees. These are stored alongside supersense annotations in the CoNLL-U-Lex format BIBREF41, BIBREF0. CoNLL-U-Lex extends the CoNLL-U format used by the Universal Dependencies BIBREF42 project to add additional columns for lexical semantic annotations.', 'Corpus Annotation ::: Reliability of Annotation', 'The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.']\n",
            "['The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.']\n",
            "['Our corpus contains 933 manually identified adpositions. Of these, 70 distinct adpositions, 28 distinct scene roles, 26 distinct functions, and 41 distinct full construals are attested in annotation. Full statistics of token and type frequencies are shown in tab:stats. This section presents the most frequent adpositions in Mandarin Chinese, as well as quantitative and qualitative comparisons of scene roles, functions, and construals between Chinese and English annotations.']\n",
            "['Our corpus contains 933 manually identified adpositions. Of these, 70 distinct adpositions, 28 distinct scene roles, 26 distinct functions, and 41 distinct full construals are attested in annotation. Full statistics of token and type frequencies are shown in tab:stats. This section presents the most frequent adpositions in Mandarin Chinese, as well as quantitative and qualitative comparisons of scene roles, functions, and construals between Chinese and English annotations.']\n",
            "['Github Repo', 'https://github.com/Sairamvinay/Fake-News-Dataset']\n",
            "['These previous approaches lack a clear contextual analysis used in NLP. We considered the semantic meaning of each word and we feel that the presence of particular words influence the meaning. We reckoned this important since we felt the contextual meaning of the text needs to be preserved and analyzed for better classification. Other studies emphasize the user and features related to them. In [4], “45 features…[were used] for predicting accuracy...across four types: structural, user, content, and temporal,” so features included characteristics beyond the text. Article [6] \"learn[s] the representations of news articles, creators and subjects simultaneously.\" In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest.']\n",
            "['Once the representations of text are pre-trained from previous unsupervised learning, the representations are then fed into 5 different models to perform supervised learning on the downstream task. In this case, the downstream task is a binary classification of the fake news as either real or fake. A k-fold prediction error is obtained from each of the 5 models, and since we have 3 different pre-training models, we have a total of 15 models to compare.', 'Methods ::: Fine-tuning ::: Artificial Neural Network (ANN)', 'We trained simple Artificial Neural Networks which contains an input layer, particular number of output layers (specified by a hyperparameter) in which each hidden layer contains the same number of neurons and the same activation function, and an output layer with just one node for the classification (real or fake) which uses sigmoid as an activation function. We chose sigmoid as the output layer activation and the binary_crossentropy as the loss since it is a binary classification problem and the use of softmax normalizes the results which is not needed for this problem and since we use only one output node to return the activation, we applied sigmoid for the output layer activation. We performed Grid Search strategy to find the best hyper-parameters such as activations, optimizers, number of hidden layers and number of hidden neurons. We had used Keras Sequential model and we used Dense Layers which contains connections to every hidden node in the next layer.', 'Due to the limitation of computing resource, the grid search for Neural Networks is divided into three sequential steps. Instead of performing grid search on all the hyperparameters all at once, we chose to do grid search for the activations for the hidden layers, optimizers and the number of hidden layers and hidden neurons (done together). We coupled the number of hidden layers and the number of neurons since we believed that each of these hyperparameters interact with each other in improving the model training. We also did a K-fold Split for 3 splits at each step and picked the best hyperparameters which renders the highest accuracy.', 'Methods ::: Fine-tuning ::: Long Short Term Memory networks (LSTMs)', 'Long Short Term Memory networks (LSTMs) is a special recurrent neural network (RNN) introduced by Hochreiter & Schmidhuber (1997)$^{8}$.', '(Christopher Olah. “Understanding LSTM Networks.”)', 'The chain-like nature of an RNN allows information to be passed from the beginning all the way to the end. The prediction at time step $t$ depends on all previous predictions at time step $t’ < t$. However, when a typical RNN is used in a larger context (i.e. a relatively large time steps), the RNN suffers from the issue of vanishing gradient descent $^{9}$. LSTMs, a special kind of RNN, can solve this long-term dependency problem.', 'Each cell in a typical LSTMs network contains 3 gates (i.e., forget gate, input gate, and output gate) to decide whether or not information should be maintained in the cell state $C_t$.', 'For CountVectorizer and TfidfVectorizer, each sample of text is converted into a 1-d feature vector of size 10000. As a result, the number of time steps (i.e. the maximum amount of word vectors for each sample) for these two can only be set to 1, as the pre-trained representations are done at the sample’s level. By contrast, the number of time steps for Word2Vec can either be 1, if we simply take an average of the word embeddings, or the length of the sentence, where each word has an embedding and thus the pre-trained representations are done at the word’s level. We choose the approach with 1 timestep in our model because it requires less computation power. Meanwhile, we also do the length of the sentence, and 200 time steps are chosen as 200 is close to the mean amount of words in each sample and it is a fairly common choice in practice. However, since we do not have enough computation power to fine-tune (grid search) our model, we leave it out for our model and include it only in the final section.', 'In the LSTM layer, a dropout rate of 0.2, a common choice in practice$^{10}$ , is used to prevent overfitting. Grid search is performed in order to pick decent values of hyperparameters, including the number of hidden units in the LSTM layer, the number of hidden layers, the activation functions and the number of nodes in the hidden layer, and the optimizer. Relatively small numbers of hidden layers (i.e., {0, 1, 2}) and nodes (i.e., {200, 400, 600}) are selected as the basis for grid search, because this is a simple binary classification task and too many of them would cause overfitting.', 'Due to the limitation of computing resource, the grid search for LSTMs is divided into four sequential steps. Instead of performing grid search on all the hyperparameters all at once, the grid search is first done on the number of hidden layers and all other hyperparameters are randomly selected from the subset. Then, the grid search is done on the number of nodes in the hidden layer(s), using the best number of hidden layer found in step 1. The grid search completes when all four steps are finished. In each step we used K-fold cross validation with $K = 3$.', 'Methods ::: Fine-tuning ::: Random Forest', 'A random forest is an ensemble classifier that estimates based on the combination of different decision trees. So random forest will fit a number of decision tree classifiers on various subsamples of the dataset. A random best subsets are built by each tree in the forest. In the end, it gives the best subset of features among all the random subsets of features.', 'In our project, 3 random forest algorithms have been applied with models count vectorizer, tfidf and word-to-vector. Random forest algorithm requires 4 hyperparameters to tune, such as the number of trees in the forest (i.e., {200, 400, 800}); the maximum depth of the tree (i.e., {1,5,9}); the minimum number of samples required to be at a lead node (i.e., {2, 4}); The minimum number of samples at each leaf node has the effect of smoothing the model, especially during regression; the minimum number of samples required to be at a leaf node (i.e., {5, 10}). All parameters are applied to grid search and in the end, the best set of parameters can be determined as we used K-fold cross validation with $K = 3$.', 'Methods ::: Fine-tuning ::: Logistic Regression', 'Logistic regression is a statistical machine learning algorithm that classifies the data by considering outcome variables on extreme ends and this algorithm is providing a discriminatory line between classes. Compared to another simple model, linear regression, which requires hard threshold in classification, logistic regression can overcome threshold values for a large dataset. Logistic regression produces a logistic curve, which is limited to values between 0 to 1, by adding sigmoid function in the end.', 'In regards to our project, three logistic regressions have been applied with models CountVectorizer, TF-IDF and Word2Vec. We did grid search on the solvers, including newton-cg, sag, lbfgs and liblinear. Grid search is also performed on the inverse of regularization parameter with values being {0, 4, 10}. Best parameter sets can be determined as we used K-fold cross validation with $K = 3$.', 'Methods ::: Fine-tuning ::: Support Vector Machine (SVM)', 'SVM is a supervised machine learning algorithm in which a hyperplane is created in order to separate and categorize features. The optimal hyperplane is usually calculated by creating support vectors on both sides of the hyperplane in which each vector must maximize the distance between each other. In other words, the larger the distance between each vector around the hyperplane, the more accurate the decision boundary will be between the categories of features.', 'In regards to our project, we fit 3 support vector machines on CountVectorizer, TfidfVectorizer, and WordToVectorizer. An SVM requires specific parameters such as a kernel type, $C$, maximum iterations, etc. In our case, we needed to determine the optimal $C$ as well as the optimal kernel for each fit. We used K-fold cross validation with $K = 3$. A grid search of kernel types and $C$ was performed in order to give us the most accurate svm model. The parameters we used for each kernel were linear and rbf while the values we used for $C$ were 0.25 ,0.5, and 0.75. Once the grid search was completed for these hyperparameters, the model was evaluated with the most optimal hyperparameters using cross validation of 3 splits.']\n",
            "['These previous approaches lack a clear contextual analysis used in NLP. We considered the semantic meaning of each word and we feel that the presence of particular words influence the meaning. We reckoned this important since we felt the contextual meaning of the text needs to be preserved and analyzed for better classification. Other studies emphasize the user and features related to them. In [4], “45 features…[were used] for predicting accuracy...across four types: structural, user, content, and temporal,” so features included characteristics beyond the text. Article [6] \"learn[s] the representations of news articles, creators and subjects simultaneously.\" In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest.', 'We had devised this problem into 3 different phases: pre-processing, text-to-numeric representation conversion using pre-trained algorithms, and then evaluate the models using state-of-the-art machine learning algorithms. We had analysed the data set and in particular the text part of the data explaining how it is distributed and then we converted each text into numeric representation using pre-training models such as TFIDF, CV and W2V for vector representation. Finally, we evaluated our numeric conversion data using significant machine learning algorithms such as neural networks, classification algorithms etc to perform the classification.']\n",
            "['While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.']\n",
            "['As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent.']\n",
            "['As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent.']\n",
            "['As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent.']\n",
            "['While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.']\n",
            "['While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.']\n",
            "['The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:', 'Wrong alignment', 'Partial alignment with slightly compositional translational equivalence', 'Partial alignment with compositional translation and additional or missing information', 'Correct alignment with compositional translation and few additional or missing information', 'Correct alignment and fully compositional translation', 'The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:', 'Partial alignment, some words or sentences may be missing', 'Correct alignment, allowing non-spoken syllables at start or end.', 'The evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.']\n",
            "['The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:', 'Wrong alignment', 'Partial alignment with slightly compositional translational equivalence', 'Partial alignment with compositional translation and additional or missing information', 'Correct alignment with compositional translation and few additional or missing information', 'Correct alignment and fully compositional translation']\n",
            "['The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:', 'Wrong alignment', 'Partial alignment, some words or sentences may be missing', 'Correct alignment, allowing non-spoken syllables at start or end.']\n",
            "['The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:', 'Wrong alignment', 'Partial alignment, some words or sentences may be missing', 'Correct alignment, allowing non-spoken syllables at start or end.', 'The evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.']\n",
            "['We first compare our proposed SeG with aforementioned approaches in Table TABREF19 for top-N precision (i.e., P@N). As shown in the top panel of the table, our proposed model SeG can consistently and significantly outperform baseline (i.e., PCNN+ATT) and all recently-promoted works in terms of all P@N metric. Compared to PCNN with selective attention (i.e., PCNN+ATT), our proposed SeG can significantly improve the performance by 23.6% in terms of P@N mean for all sentences; even if a soft label technique is applied (i.e., PCNN+ATT+SL) to alleviate wrongly labeled problem, our performance improvement is also very significant, i.e., 7.8%.', 'Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3% , even if they propose sophisticated techniques to handle the noisy training data. These verify the effectiveness of our approach over previous works when solving the wrongly labeled problem that frequently appears in distantly supervised relation extraction.']\n",
            "['We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.']\n",
            "['We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.']\n",
            "['We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.']\n",
            "['This paper describes the systems developed by the department of electronic engineering, institute of microelectronics of Tsinghua university and TsingMicro Co. Ltd. (THUEE) for the NIST 2019 speaker recognition evaluation (SRE) CTS challenge BIBREF0. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet, multitask and c-vector are developed in this evaluation. All the subsystems consists of a deep neural network followed by dimension deduction, score normalization and calibration. For each system, we begin with a summary of the data usage, followed by a description of the system setup along with their hyperparameters. Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets.', 'FLOAT SELECTED: Table 1. Datasets Notations']\n",
            "['Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.']\n",
            "['Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.']\n",
            "['At the second series of the experiments, we applied EuroVoc information retrieval thesaurus to two European Union collections: Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words. The results are shown in Table 3. It can be seen that inclusion of EuroVoc synsets improves the topic coherence and increases kernel uniqueness (in contrast to results with WordNet). Adding ngrams further improves the topic coherence and kernel uniqueness.']\n",
            "['To estimate the quality of topic models in a real task, we chose Islam informational portal \"Golos Islama\" (Islam Voice) (in Russian). This portal contains both news articles related to Islam and articles discussing Islam basics. We supposed that the thematic analysis of this specialized site can be significantly improved with domain-specific knowledge described in the thesaurus form. We extracted the site contents using Open Web Spider and obtained 26,839 pages.']\n",
            "['At the second series of the experiments, we applied EuroVoc information retrieval thesaurus to two European Union collections: Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words. The results are shown in Table 3. It can be seen that inclusion of EuroVoc synsets improves the topic coherence and increases kernel uniqueness (in contrast to results with WordNet). Adding ngrams further improves the topic coherence and kernel uniqueness.']\n",
            "['For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .']\n",
            "['For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .']\n",
            "['For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .']\n",
            "['Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a qualitatively different manner. For example some African Americans often use the term n*gga in everyday language online BIBREF2 , people use terms like h*e and b*tch when quoting rap lyrics, and teenagers use homophobic slurs like f*g as they play video games. Such language is prevalent on social media BIBREF3 , making this boundary condition crucial for any usable hate speech detection system .']\n",
            "['Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a qualitatively different manner. For example some African Americans often use the term n*gga in everyday language online BIBREF2 , people use terms like h*e and b*tch when quoting rap lyrics, and teenagers use homophobic slurs like f*g as they play video games. Such language is prevalent on social media BIBREF3 , making this boundary condition crucial for any usable hate speech detection system .']\n",
            "['Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a qualitatively different manner. For example some African Americans often use the term n*gga in everyday language online BIBREF2 , people use terms like h*e and b*tch when quoting rap lyrics, and teenagers use homophobic slurs like f*g as they play video games. Such language is prevalent on social media BIBREF3 , making this boundary condition crucial for any usable hate speech detection system .']\n",
            "['We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .']\n",
            "['We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .']\n",
            "['We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .']\n",
            "['We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.']\n",
            "['We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.']\n",
            "['We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.']\n",
            "['Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12–16 layers. This finding suggests that these models may be overparameterized for SST-2.']\n",
            "['Our research contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to fine-tune only one fourth of the final layers to achieve within 10% parity with the full model. Surprisingly, on SST-2, a sentiment classification dataset, we find that not fine-tuning all of the layers leads to improved quality.']\n",
            "[\"When the annotation artifacts of the training set cannot be generalized to the testing set, which should be more common in the real-world, predicting by artifacts may hurt models' performance. Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.\", \"Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability.\"]\n",
            "['We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing.']\n",
            "['We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing.']\n",
            "['We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing.']\n",
            "['In active learning, the learning algorithm is set to proactively select a subset of available examples to be manually labeled next from a pool of yet unlabeled instances. The fundamental idea behind the concept is that a machine learning algorithm could potentially achieve a better accuracy quicker and using fewer training data if it were allowed to choose the most informative data it wants to learn from. In our experiment, we found that the entropy algorithm is the best way to build machine learning models fast and efficiently. Vote entropy and KL divergence, the query-by-committee active learning methods are helpful for the training of machine learning ensemble classifiers. However, all the active learning strategies we tested do not work well with deep learning model (i.e., CNN) or deep learning-based ensemble classifier.']\n",
            "['We implemented a pool-based active learning pipeline to test which classifier and active learning strategy is most efficient to build up an event classification classifier of Twitter data. We queried the top 300 most “informative” tweets from the rest of the pool (i.e., excluding the tweets used for training the classifiers) at each iteration. Table 3 shows the active learning and classifier combinations that we evaluated. The performance of the classifiers was measured by F-score. Fig 3 shows the results of the different active learning strategies combined with LR (i.e., the baseline), RF (i.e., the best performed machine learning model), and CNN (i.e., the best performed deep learning model). For both machine learning models (i.e., LR and RF), using the entropy strategy can reach the optimal performance the quickest (i.e., the least amount of tweets). While, the least confident algorithm does not have any clear advantages compared with random selection. For deep learning model (i.e., CNN), none of the active learning strategies tested are useful to improve the CNN classifier’s performance. Fig 4 shows the results of query-by-committee algorithms (i.e., vote entropy and KL divergence) combined with machine learning and deep learning ensemble classifiers. Query-by-committee algorithms are slightly better than random selection when it applied to machine learning ensemble classifier. However, query-by-committee algorithms are not useful for the deep learning ensemble classifier.']\n",
            "['Our data came from two different sources as shown in Table 1. First, we collected 2,803,164 tweets using the Twitter search API BIBREF27 from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68). After filtering out duplicates and non-English tweets, 1,952,079 tweets were left. Second, we used the same list of keywords to identify relevant tweets from a database of historical random public tweets we collected from January 1, 2013 to December 30, 2017. We found 1,733,905 relevant tweets from this database. Due to the different mechanisms behind the two Twitter APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets related to the provided keywords within 10 to 14 days before the time of data collection; while the Twitter streaming API returns a random sample (i.e., roughly 1% to 20% varying across the years) of all public tweets at the time and covers a wide range of topics. After integrating the tweets from the two data sources, there were 3,685,984 unique tweets.']\n",
            "['Our data came from two different sources as shown in Table 1. First, we collected 2,803,164 tweets using the Twitter search API BIBREF27 from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68). After filtering out duplicates and non-English tweets, 1,952,079 tweets were left. Second, we used the same list of keywords to identify relevant tweets from a database of historical random public tweets we collected from January 1, 2013 to December 30, 2017. We found 1,733,905 relevant tweets from this database. Due to the different mechanisms behind the two Twitter APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets related to the provided keywords within 10 to 14 days before the time of data collection; while the Twitter streaming API returns a random sample (i.e., roughly 1% to 20% varying across the years) of all public tweets at the time and covers a wide range of topics. After integrating the tweets from the two data sources, there were 3,685,984 unique tweets.']\n",
            "['Our data came from two different sources as shown in Table 1. First, we collected 2,803,164 tweets using the Twitter search API BIBREF27 from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68). After filtering out duplicates and non-English tweets, 1,952,079 tweets were left. Second, we used the same list of keywords to identify relevant tweets from a database of historical random public tweets we collected from January 1, 2013 to December 30, 2017. We found 1,733,905 relevant tweets from this database. Due to the different mechanisms behind the two Twitter APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets related to the provided keywords within 10 to 14 days before the time of data collection; while the Twitter streaming API returns a random sample (i.e., roughly 1% to 20% varying across the years) of all public tweets at the time and covers a wide range of topics. After integrating the tweets from the two data sources, there were 3,685,984 unique tweets.']\n",
            "['Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%.']\n",
            "['Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%.']\n",
            "['We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work.']\n",
            "['Our neural network-based POS tagger can be divided into 3 steps: embedding, encoding, and prediction. First, the tagger embeds the words and optionally additional features of such words (e.g., affixes). From this embedding process, we get vector representations of the words and the features. Next, the tagger learns contextual information in the encoding step via either a feedforward network with context window or a bidirectional LSTM BIBREF16 . Finally, in prediction step, the tagger predicts the POS tags from the output of the encoding step using either a softmax or a CRF layer.', 'Encoding. In the encoding step, the tagger learns contextual information by using either a feedforward network with context window or a bidirectional LSTM (biLSTM). The feedforward network accepts as input the concatenation of the embedding of the current word and INLINEFORM0 preceding and succeeding words for some context window size INLINEFORM1 . Formally, given a sequence of word embedding INLINEFORM2 , the input of the feedforward network at timestep INLINEFORM3 is DISPLAYFORM0']\n",
            "['Our neural network-based POS tagger can be divided into 3 steps: embedding, encoding, and prediction. First, the tagger embeds the words and optionally additional features of such words (e.g., affixes). From this embedding process, we get vector representations of the words and the features. Next, the tagger learns contextual information in the encoding step via either a feedforward network with context window or a bidirectional LSTM BIBREF16 . Finally, in prediction step, the tagger predicts the POS tags from the output of the encoding step using either a softmax or a CRF layer.']\n",
            "['We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online.']\n",
            "['We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online.']\n",
            "['We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online.']\n",
            "['In this work, we explored different neural network architectures for Indonesian POS tagging. We evaluated our experiments on the IDN Tagged Corpus BIBREF11 . Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result for Indonesian POS tagging on the dataset. We release the dataset split that we used to serve as a benchmark for future work.']\n",
            "['In this work, we explored different neural network architectures for Indonesian POS tagging. We evaluated our experiments on the IDN Tagged Corpus BIBREF11 . Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result for Indonesian POS tagging on the dataset. We release the dataset split that we used to serve as a benchmark for future work.']\n",
            "['We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work.']\n",
            "['We download parts of Chinese Wikipedia articles from Large-Scale Chinese Datasets for NLP. For word segmentation and filtering the stopwords, we apply the jieba toolkit based on the stopwords table. Finally, we get 11,529,432 segmented words. In accordance with their work BIBREF14 , all items whose Unicode falls into the range between 0x4E00 and 0x9FA5 are Chinese characters. We crawl the stroke information of all 20,402 characters from an online dictionary and render each character glyph to a 28 INLINEFORM0 28 1-bit grayscale bitmap by using Pillow.']\n",
            "['We download parts of Chinese Wikipedia articles from Large-Scale Chinese Datasets for NLP. For word segmentation and filtering the stopwords, we apply the jieba toolkit based on the stopwords table. Finally, we get 11,529,432 segmented words. In accordance with their work BIBREF14 , all items whose Unicode falls into the range between 0x4E00 and 0x9FA5 are Chinese characters. We crawl the stroke information of all 20,402 characters from an online dictionary and render each character glyph to a 28 INLINEFORM0 28 1-bit grayscale bitmap by using Pillow.']\n",
            "['The values of the metrics in this case were almost as good and comparable to the CC ones. However, the model trained with a larger vocabulary had higher results. Also, the model with the dataset vectors did not have the flexibility to classify unknown words.', 'As a next step, the test set of the dataset was altered by replacing words with syntactical mistakes to test the tolerance of the model in OOV words. Suffixes of verbs were altered and vowels were replaced with others, affecting 20% of the tokens of the dataset. Using again the more complex tagset for training, the results can be found in Table 3.', 'What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class.', 'One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.', 'It was noticed that the model performed better when using the vectors from different FastText models. It was expected that the second experiment would have performed better, as the tokens were inserted into the same FastText model and the vectors exported from both sources should match.']\n",
            "['One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.']\n",
            "['The values of the metrics in this case were almost as good and comparable to the CC ones. However, the model trained with a larger vocabulary had higher results. Also, the model with the dataset vectors did not have the flexibility to classify unknown words.', 'One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.']\n",
            "['Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2.', 'In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC). A percentage of Greek Wikipedia was parsed and used for training in spaCy. The results from the training are presented in SECREF13.']\n",
            "['Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2.', 'The greek Part of Speech Tagging and Named Entity Recognition models presented in this paper were developed using the spaCy library BIBREF3. SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models for the implementation of Natural Language Processing components that achieve state-of-the-art results in many of these tasks.']\n",
            "['Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7.']\n",
            "['Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token. The various values of POS tags were retrieved and incorporated into a tag map. The labels and morphology they describe are explained below.', 'Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7.']\n",
            "['Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7.']\n",
            "['To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\\\kappa $ = $0.93$ (Cohen’s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user\\'s status to or from \"unavailable\" in the time between the first and second annotations. Upon resolving the disagreements, 413 $(4\\\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\\\%)$ as \"unavailable\".']\n",
            "['We used the 8262 \"bot\" and \"non-bot\" users in experiments to train and evaluate three classification systems. We split the users into $80\\\\%$ (training) and $20\\\\%$ (test) sets, stratified based on the distribution of \"bot\" and \"non-bot\" users. The training set includes $61,160,686$ tweets posted by 6610 users, and the held-out test set includes $15,703,735$ tweets posted by 1652 users. First, we evaluated Botometer on our held-out test set. Botometer is a publicly available bot detection system designed for political dot detection. It outputs a score between 0 and 1 for a user, representing the likelihood that a user is a bot. Second, we used the Botometer score for each user as a feature in training a gradient boosting classifier which is a decision tree-based ensemble machine learning algorithm with gradient boosting BIBREF23 and can be used to address class imbalance. To adapt the Botometer scores to our binary classification task, we set the threshold to $0.47$, based on performing 5-fold cross validation over the training set. To further address the class imbalance, we used the Synthetic Minority Over-sampling Technique (SMOTE)BIBREF24 to create artificial instances of \"bot\" users in the training set. We also performed 5-fold cross validation over the training set to optimize parameters for the classifier; we used exponential as the loss function, set the number of estimators to 200, and set the learning rate to $0.1$. Third, we used the classifier with an extended set of features that are not used by Botometer. Based on our manual annotation, we consider the following features to be potentially informative for distinguishing \"bot\" and \"non-bot\" users in health-related data:', 'Tweet Diversity. Considering that \"bot\" users may re-post the same tweets, we used the ratio of a user\\'s unique tweets to the total number of tweets posted by the user, where 0 indicates that the user has posted only the same tweet multiple times, and 1 indicates that each tweet is unique and has been posted only once. As Figure 1 illustrates, a subset of \"bot\" users (in the training set) have posted more of the same tweets than \"non-bot\" users.', 'URL score. During manual annotation, we found that \"bot\" users\\' tweets frequently contain URLs (e.g., advertisements for health-related products, such as medications), so we use the ratio of the number of a user\\'s tweets containing a URL to the total number of tweets posted by the user.', 'Mean Daily Posts. Considering that \"bot\" users may post tweets more frequently than \"non-bot\" users, we measured the average and standard deviation of the number of tweets posted daily by a user. As Figure 1 illustrates, a subset of \"bot\" users post, on average, more tweets daily than \"non-bot\" users.', 'Topics. Considering that \"bot\" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user\\'s tweets. We used Latent Dirichlet Allocation (LDA)BIBREF25 to extract the top five topics from all of the users\\' 1000 most recent tweets (or all the tweets if a user has posted less than 1000 tweets), and used the mean of the weights of each topic across all of a user\\'s tweets.', 'Mean Post Length. Considering that the length of tweets may be different between \"bot\" and \"non-bot\" users, we used the mean word length and standard deviation of a user\\'s tweets.', 'Profile Picture. In addition to tweet-related features, we used features based on information in users\\' profiles. Considering that a \"non-bot\" user\\'s profile picture may be more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the majority of \"non-bot\" users (in the training set), whereas at least one face was detected in the profile picture of the majority of \"bot\" users.', 'User Name. Finally, we used a publicly available lexicon to detect the presence or absence of a person\\'s name in a user name. As Figure 2 illustrates, the name of a person is present (1) in approximately half of \"non-bot\" user names, whereas the name of a person is absent (0) in the majority of \"bot\" user names.']\n",
            "['To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\\\kappa $ = $0.93$ (Cohen’s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user\\'s status to or from \"unavailable\" in the time between the first and second annotations. Upon resolving the disagreements, 413 $(4\\\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\\\%)$ as \"unavailable\".']\n",
            "['As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work.']\n",
            "['As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work.']\n",
            "['In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.']\n",
            "['In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.']\n",
            "['In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.']\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "['Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric.']\n",
            "['Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric.']\n",
            "['Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric.']\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "[\"For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\"]\n",
            "['We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM.']\n",
            "['We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM.']\n",
            "['We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM.']\n",
            "['Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.']\n",
            "['Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.']\n",
            "['In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 .']\n",
            "['Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.']\n",
            "['Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label.']\n",
            "['We sourced our cloze data from version 2.4 of the Universal Dependencies treebanks BIBREF28. The UD treebanks use a consistent schema across all languages to annotate naturally occurring sentences at the word level with rich grammatical information. We used the part-of-speech and dependency information to identify potential agreement relations. Specifically, we identified all instances of subject-verb, noun-determiner, noun-attributive adjective and subject-predicate adjective word pairs. We then used the morphosyntactic annotations for number, gender, case and person to filter out word pairs that disagree due to errors in the underlying data source (e.g. one is annotated as plural while the other is singular) or that are not annotated for any of the four features.']\n",
            "['With its comparatively limited inflectional morphology, English only exhibits subject-verb and determiner agreement (in demonstratives, “this” vs. “these”) and even then only agrees for number. Languages with richer inflectional morphology tend to display more agreement types and involve more features. French, for example, employs all four types of agreement relations. Examples are given in (UNKREF3)-(UNKREF6). The subject and verb in (UNKREF3) agree for number, while the noun and determiner in (UNKREF4), the noun and attributive adjective in (UNKREF5) and the subject and predicated adjective in (UNKREF6) agree for both number and gender.', \"`The keys to the door are on the table.'\", \"`I can see the keys.'\", \"`I no longer want the completely broken keys.'\", \"`The keys to the door are broken.'\"]\n",
            "['Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.']\n",
            "['Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.']\n",
            "['Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.']\n",
            "['As a baseline, we consider TextFooler BIBREF11 which performs synonym replacement using a fixed word embedding space BIBREF22. We only consider the top $K{=}50$ synonyms from the MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based embeddings of the adversarial and input text.']\n",
            "['We use 3 popular text classification models: word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier. For each dataset we train the model on the training data and perform the adversarial attack on the test data. For complete model details refer to Appendix.']\n",
            "['We use 3 popular text classification models: word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier. For each dataset we train the model on the training data and perform the adversarial attack on the test data. For complete model details refer to Appendix.']\n",
            "['The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features.', 'FLOAT SELECTED: Table 5: Test Results — Full Analysis', 'Research on automatic sarcasm detection in other domains has been limited, but recently a publicly-available corpus of sarcastic and non-sarcastic Amazon product reviews was released by Filatova FILATOVA12.661 to facilitate research. BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 test many feature combinations on this dataset, including those based on metadata (e.g., Amazon star rating), sentiment, grammar, the presence of interjections (e.g., “wow”) or laughter (e.g., through onomatopoeia or acronyms such as “lol”), the presence of emoticons, and bag-of-words features. Their highest F1 (0.744) is achieved using all of these with a logistic regression classifier; however, using only the star rating, they still achieve an F1 of 0.717. This highlights the need for high-performing, general features for sarcasm detection; metadata features are highly domain-specific, and even bag-of-words trends may be unique to certain domains (“trump” was one of the most common unigrams in our own Twitter training set, but only occurred once across all Amazon product reviews).']\n",
            "['The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features.']\n",
            "['Research on automatic sarcasm detection in other domains has been limited, but recently a publicly-available corpus of sarcastic and non-sarcastic Amazon product reviews was released by Filatova FILATOVA12.661 to facilitate research. BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 test many feature combinations on this dataset, including those based on metadata (e.g., Amazon star rating), sentiment, grammar, the presence of interjections (e.g., “wow”) or laughter (e.g., through onomatopoeia or acronyms such as “lol”), the presence of emoticons, and bag-of-words features. Their highest F1 (0.744) is achieved using all of these with a logistic regression classifier; however, using only the star rating, they still achieve an F1 of 0.717. This highlights the need for high-performing, general features for sarcasm detection; metadata features are highly domain-specific, and even bag-of-words trends may be unique to certain domains (“trump” was one of the most common unigrams in our own Twitter training set, but only occurred once across all Amazon product reviews).']\n",
            "['The features used for each train/test scenario are shown in the first column of Table TABREF18 . Twitter Features refers to all features listed in Table TABREF11 preceded by the parenthetical (T), and Amazon Features to all features preceded by (A). General: Other Polarity includes the positive and negative percentages, average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective positive words, and their negative counterparts. We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic.']\n",
            "['The features used for each train/test scenario are shown in the first column of Table TABREF18 . Twitter Features refers to all features listed in Table TABREF11 preceded by the parenthetical (T), and Amazon Features to all features preceded by (A). General: Other Polarity includes the positive and negative percentages, average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective positive words, and their negative counterparts. We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic.']\n",
            "['The features used for each train/test scenario are shown in the first column of Table TABREF18 . Twitter Features refers to all features listed in Table TABREF11 preceded by the parenthetical (T), and Amazon Features to all features preceded by (A). General: Other Polarity includes the positive and negative percentages, average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective positive words, and their negative counterparts. We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic.']\n",
            "[\"Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags “#sarcasm,” “#happiness,” “#sadness,” “#anger,” “#surprise,” “#fear,” and “#disgust” were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\"]\n",
            "[\"Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags “#sarcasm,” “#happiness,” “#sadness,” “#anger,” “#surprise,” “#fear,” and “#disgust” were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\"]\n",
            "[\"Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags “#sarcasm,” “#happiness,” “#sadness,” “#anger,” “#surprise,” “#fear,” and “#disgust” were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\"]\n",
            "[\"Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags “#sarcasm,” “#happiness,” “#sadness,” “#anger,” “#surprise,” “#fear,” and “#disgust” were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\"]\n",
            "[\"Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags “#sarcasm,” “#happiness,” “#sadness,” “#anger,” “#surprise,” “#fear,” and “#disgust” were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\", 'The downloaded tweets were filtered to remove retweets, “@replies,” and tweets containing links. Retweets were removed to avoid having duplicate copies of identical tweets in the dataset, @replies were removed in case the hashtag referred to content in the tweet to which it replied rather than content in the tweet itself, and tweets with links were likewise removed in case the hashtag referred to content in the link rather than in the tweet itself. Requiring that the specified hashtag trailed the rest of the tweet (it could only be followed by other hashtags) was done based on the observation that when sarcastic or emotional hashtags occur in the main tweet body, the tweet generally discusses sarcasm or the specified emotion, rather than actually expressing sarcasm or the specified emotion. Finally, requiring that only one of the specified hashtags trailed the tweet eliminated cases of ambiguity between sarcastic and non-sarcastic tweets. All trailing “#sarcasm” or emotion hashtags were removed from the data before training and testing, and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 .']\n",
            "['In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score.']\n",
            "['In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score.']\n",
            "['Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features.', 'Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .']\n",
            "['Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features.', 'Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .']\n",
            "['The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD.']\n",
            "['In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score.']\n",
            "['In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score.']\n",
            "[\"Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5.\"]\n",
            "[\"Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5.\"]\n",
            "['Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features.', 'Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .']\n",
            "['Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features.', 'Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .']\n",
            "['Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features.', 'Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .']\n",
            "['The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343.']\n",
            "['The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.']\n",
            "['To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence “Bill plays a game” would not fully understand the sentence “Bill plays a popular board game” without knowing the meaning of “popular” and “board game” but could infer that the latter sentence indicates that its subject plays a type of game.', 'The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.']\n",
            "['To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence “Bill plays a game” would not fully understand the sentence “Bill plays a popular board game” without knowing the meaning of “popular” and “board game” but could infer that the latter sentence indicates that its subject plays a type of game.', 'The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.', 'Method ::: Sentence Structure Recognition', 'The sentence structure recognition process involves 2 modules: natural language processing (NLP) module and logical reasoning on result from NLP module. In this paper, we make use of the Stanford Parser tools described in BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14', 'The NLP module tokenizes the input free text to produce a dependency-based parse tree and part-of-speech tag (POS tag). The dependency-based parse tree and the POS tag are then transform into an answer set program (ASP) BIBREF15 which contains only facts. Table TABREF13 shows the transformation of the result of NLP module into an ASP program for the sentence “Bill plays a game”. In this table, nsubj, det, dobj and punct denote relations in the dependency-based parse tree, and mean nominal subject, determiner, direct object and punctuation respectively. Full description of all relations in a dependency-based parse tree can be found in the Universal Dependency website. The second set of notations are the POS tag PRP, VBP, DT and NN corresponding to pronoun, verb, determiner and noun. Readers can find the full list of POS tag in Penn Treebank Project.', 'From the collection of the dependency atoms from the dependency-based parse tree, we determine the structure of a sentence using an ASP program, called $\\\\Pi _1$ (Listing ).', 'Each of the rule above can be read as if the right-hand side is true then the left-hand side must be true. These rules define five possible structures of a sentence represented by the atom structure(x,y). $x$ and $y$ in the atom structure(x,y) denote the type of the structure and the number of dependency relations applied to activate the rule generating this atom, respectively. We refer to $y$ as the $i$-value of the structure. For example, $structure(1,1)$ will be recognized if the nsubj relation is in the dependency-based parse tree; $structure(3,3)$ needs 3 dependency relations to be actived: nsubj, xcomp and dobj. We often use structure #$x$ to indicate a structure of type $x$.', 'Together with the collection of the atoms encoding the relations in the dependency-based parse tree, $\\\\Pi _1$ generates several atoms of the form $structure(x,y)$ for a sentence. Among all these atoms, an atom with the highest $i$-value represents the structure constructed using the highest number of dependency relations. And hence, that structure is the most informative structure that is recoginized for the sentence. Observe that $structure(1,1)$ is the most simplified structure of any sentence.', 'Method ::: Sentence Components Recognition', 'The goal of this step is to identify the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the dependency-based parse tree. For example, the sentence “Bill plays a game” is encoded by a structure #2 and we expect that Bill, plays, and game correspond to the subject, verb, and object, respectively.', 'We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\\\Pi _2$ (Listing ). The first four rules of $\\\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in “Cathy is gorgeous,” the part after tobe is an adjective, but in “Cathy is a beautiful girl,” the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\\\Pi _2$.', 'The result of program $\\\\Pi _2$ is an one-to-one mapping of some of the words in the sentence into the importaint components of a sentence, called main components, i.e. subject, object and verb. The mapping is constructed by using the core arguments in Universal Dependency Relations . Since not every word in the sentence is in a core argument relation, there are some words in the sentence that are not in the domain of the mapping that $\\\\Pi _2$ produces. We denote these words are complement components. To identify these words, we encode the Non-core dependents and Nominal dependents from Universal Dependency Relations into the set of rules in program $\\\\Pi _3$.', 'Program $\\\\Pi _3$ (Listing ), together with the atoms extracted from the dependency-based parse tree such as $compound(P,N)$ ($N$ is compound noun at the position $P$ in the sentence), $amod(P,J)$ ($J$ is an adjective modifier), etc., is used to identify the complement components of the main components computed by $\\\\Pi _2$ while maintaining the structure of the sentence created by $\\\\Pi _1$. For example, a complement of a noun could be another noun (as “board” in “board game”), or an adjective (as “popular” in “popular board game”), or a preposition (as “for adults” in “board game for adults”).', 'The input of Program $\\\\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\\\\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps.', 'Method ::: GF Grammar Encoder', 'The goal of the encoder is to identify appropriate GF rules for the construction of a GF grammar of a sentence given its structure and its components identified in the previous two modules. This is necessary since a sentence can be encoded in GF by more than one set of rules; for example, the sentence “Bill wants to play a game” can be encoded by the rules', 'In GF, NP, VV, V2, VP, and Cl stand for noun phrase, verb-phrase-complement verb, two-place verb, verb phrase and clause, respectively. Note that although the set of GF grammatical rules can be used to construct a constituency-based parse tree , the reverse direction is not always true. To the best of our knowledge, there exists no algorithm for converting a constituency-based parse tree to a set GF grammar rules. We therefore need to identify the GF rules for each sentence structure.', 'In our system, a GF rule is assigned to a structure initially (Table TABREF19). Each rule in Table TABREF19 represents the first level of the constituency-based parse tree. It acts as the coordinator for all other succeeding rules.', 'Given the seed components identified in Section SECREF15 and the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence “Bill plays a popular board game with his close friends.”, a GF grammar for structure #2 can be constructed, which can only generate the sentence “Bill plays game.” because it does not contain any complement components identified in Section SECREF15. Therefore, we assgin a set of GF rules for the construction of each parameter in the GF rules in Table TABREF19. The set of GF rules has to follow two conventions. The first one is after applying the set of rules to some components of the sentence, the type of the production is one of the type in Table TABREF19, e.g. $NP$, $VP$, $Cl$, $V2$, .... The second convention is that the GF encoder will select the rules as the order from top to bottom in Table TABREF20. Note that the encoder always has information of what type of input and output for the rule it is looking for.', 'For instance, we have “game” is the object (main components), and we know that we have to construct “game” in the result GF grammar to be a NP (noun phrase). Program $\\\\Pi _2$ identifies that there are two complement components for the word “game”, which are “board” and “popular”, a noun and an adjective respectively. The GF encoder then select the set of rules: N $\\\\rightarrow $ N $\\\\rightarrow $ CN and A $\\\\rightarrow $ AP to create the common noun “board game” and the adjective phrase first. The next rule is AP $\\\\rightarrow $ CN $\\\\rightarrow $ CN. The last rule to be applied is CN $\\\\rightarrow $ NP. The selection is easily decided since the input and the output of the rules are pre-determined, and there is no ambiguity in the selection process.', 'The encoder uses the GF rules and the components identified by the previous subsections to produce different constructors for different components of a sentence. A part of the output of the GF encoder for the object “game” is', 'The encoder will also create the operators that will be included in the oper section of the GF grammar for supporting the new constructor. For example, the following operators will be generated for serving the Game constructor above:', 'Method ::: GF Grammar Exporter', 'The GF Grammar Exporter has the simplest job among all modules in the system. It creates a GF program for a paragraph using the GF grammars created for the sentences of the paragraph. By taking the union of all respective elements of each grammar for each sentence, i.e., categories, functions, linearizations and operators, the Grammar Exporter will group them into the set of categories (respectively, categories, functions, linearizations, operators) of the final grammar.']\n",
            "['We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class.', \"The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference BIBREF7. We create an intermediate representation that can be used to translate the original article in English to another language. In this use case, we translate the intermediate representation back to English and measure how the translated version stacks up again the original one. We assess the generation quality automatically with BLEU-3 and ROUGE-L (F measure). BLEU BIBREF16 and ROUGE BIBREF17 algorithms are chosen to evaluate our generator since the central idea of both metrixes is “the closer a machine translation is to a professional human translation, the better it is”, thus, they are well-aligned with our use cases' purpose. In short, the higher BLUE and ROUGE score are, the more similar the hypothesis text and the reference text is. In our use case, the hypothesis for BLEU and ROUGE is the generated English content from the intermediate representation, and the reference text is the original text from Wikipedia.\"]\n",
            "['We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class.']\n",
            "['Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations. The only modification made to accommodate the fact that we train the model with text in nearly a thousand languages, rather than one, is that language embedding vectors are concatenated to the inputs of the LSTMs at each time step and the hidden layer before the softmax. We used three separate embeddings for these levels, in an attempt to capture different types of information about languages. The model structure is summarized in fig:model.', 'In contrast to related work, we focus on massively multilingual data sets to cover for the first time a substantial amount of the linguistic diversity in the world in a project related to data-driven language modeling. We do not presuppose any prior knowledge about language similarities and evolution and let the model discover relations on its own purely by looking at the data. The only supervision that is giving during training is a language identifier as a one-hot encoding. From that and the actual training examples, the system learns dense vector representations for each language included in our data set along with the character-level RNN parameters of the language model itself.']\n",
            "['Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations. The only modification made to accommodate the fact that we train the model with text in nearly a thousand languages, rather than one, is that language embedding vectors are concatenated to the inputs of the LSTMs at each time step and the hidden layer before the softmax. We used three separate embeddings for these levels, in an attempt to capture different types of information about languages. The model structure is summarized in fig:model.']\n",
            "['Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations. The only modification made to accommodate the fact that we train the model with text in nearly a thousand languages, rather than one, is that language embedding vectors are concatenated to the inputs of the LSTMs at each time step and the hidden layer before the softmax. We used three separate embeddings for these levels, in an attempt to capture different types of information about languages. The model structure is summarized in fig:model.']\n",
            "['We now take a look at the language vectors found during training with the full model of 990 languages. fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family. While our experiments indicate that finding more remote relationships (say, connecting the Germanic languages to the Celtic) is difficult for the model, it is clear that the language vectors preserves similarity properties between languages.']\n",
            "['MagiCoder: overview', 'The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms.', 'We can distinguish five phases in the procedure that will be discussed in detail in Sections UID18 , UID19 , UID20 , UID23 , UID28 , respectively.', 'Preprocessing of the original text: tokenization (i.e., segmentation of the text into syntactical units), stemming (i.e., reduction of words to a particular root form), elimination of computationally irrelevant words.', 'Word-by-word linear scan of the description and “voting task”: a word “votes” LLTs it belongs to. For each term voted by one or more words, we store some information about the retrieved syntactical matching.', 'Weights calculation: recognized terms are weighted depending on information about syntactical matching.', 'Sorting of voted terms and winning terms release: the set of voted term is pruned, terms are sorted and finally a solution (a set of winning terms) is released.']\n",
            "['Figure SECREF34 depicts the pseudocode of MagiCoder. We represent dictionaries either as sets of words or as sets of functions. We describe the main procedures and functions used in the pseudocode.', 'Procedure INLINEFORM0 takes the narrative description, performs tokenization and stop-word removal and puts it into an array of words.', 'Procedures INLINEFORM0 and INLINEFORM1 get LLTs and create a dictionary of words and of their stemmed versions, respectively, which belong to LLTs, retaining the information about the set of terms containing each word.', 'By the functional notation INLINEFORM0 (resp., INLINEFORM1 ), we refer to the set of LLTs containing the word INLINEFORM2 (resp., the stem of INLINEFORM3 ).', 'Function INLINEFORM0 returns the stemmed version of word INLINEFORM1 .', 'Function INLINEFORM0 returns the position of word INLINEFORM1 in term INLINEFORM2 .', 'INLINEFORM0 is a flag, initially set to 0, which holds 1 if at least a stemmed matching with the MedDRA term INLINEFORM1 is found.', 'INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are arrays and INLINEFORM3 appends INLINEFORM4 to array INLINEFORM5 , where INLINEFORM6 may be an element or a sequence of elements.', 'INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 .', 'Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of the properties INLINEFORM2 of its elements.', 'Procedure INLINEFORM0 , where INLINEFORM1 is a set of terms and INLINEFORM2 is a term, tests whether INLINEFORM3 (considered as a string) is prefix of a term in INLINEFORM4 . Dually, procedure INLINEFORM5 tests if in INLINEFORM6 there are one or more prefixes of INLINEFORM7 , and eventually remove them from INLINEFORM8 .', 'Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise. We assume that before starting the final phase of building the solution (i.e., the returned set of LLTs), INLINEFORM5 for any word INLINEFORM6 belonging to the description.', 'Procedures INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is a set of terms, implement ordered-phrases and maximal-set-of-voters criteria (defined in Section UID28 ), respectively.', 'Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values.', '[!t] MagiCoder( INLINEFORM0 text, INLINEFORM1 dictionary, INLINEFORM2 integer)', 'INLINEFORM0 : the narrative description;', 'INLINEFORM0 : a data structure containing the MedDRA INLINEFORM1 s;', 'INLINEFORM0 : the maximum number of winning terms that have to be released by the procedure an ordered set of LLTs INLINEFORM1 = CreateMetaDict( INLINEFORM2 ) INLINEFORM3 = CreateStemMetaDict( INLINEFORM4 ) adr_clear = Preprocessing( INLINEFORM5 ) adr_length = adr_clear.length INLINEFORM6 = INLINEFORM7 for each non-stop-word in the description (i INLINEFORM8 test whether the current word belongs to MedDRA adr_clear[i] INLINEFORM9 for each term containing the word t INLINEFORM10 (adr_clear[i]) keep track of the index of the voting word INLINEFORM11 [ INLINEFORM12 ,i] keep track of the index of the recognized word in INLINEFORM13 INLINEFORM14 [ INLINEFORM15 , INLINEFORM16 (adr_clear[i])]', 'INLINEFORM0 = INLINEFORM1 test if the current (stemmed) word belongs the stemmed MedDRA stem(adr_clear[i]) INLINEFORM2 t INLINEFORM3 (stem(adr_clear[i])) test if the current term has not been exactly voted by the same word i INLINEFORM4 INLINEFORM5 [ INLINEFORM6 , i] INLINEFORM7 [ INLINEFORM8 , INLINEFORM9 (adr_clear[i])] keep track that INLINEFORM10 has been covered by a stemmed word INLINEFORM11 = true INLINEFORM12 = INLINEFORM13 for each voted term, calculate the four weights of the corresponding criteria t INLINEFORM14 INLINEFORM15 [ INLINEFORM16 ] filtering of the voted terms by the first heuristic criterium INLINEFORM17 multiple value sorting of the voted terms INLINEFORM18 = sortby( INLINEFORM19 ) t INLINEFORM20 index INLINEFORM21 select a term INLINEFORM22 if it has been completely covered, its i-th voting word has not been covered or if its i-th voting word has been perfectly recognized in INLINEFORM23 and if INLINEFORM24 is not prefix of another already selected terms INLINEFORM25 AND (( INLINEFORM26 = false OR (mark(adr_clear(index))=0)) AND t INLINEFORM27 AND prefix( INLINEFORM28 ,t)=false) mark(adr_clear(index))=1 remove from the selected term set all terms which are prefix of INLINEFORM29 INLINEFORM30 = remove_prefix( INLINEFORM31 ,t) INLINEFORM32 = INLINEFORM33 filtering of the finally selected terms by the second heuristic criterium INLINEFORM34 INLINEFORM35 INLINEFORM36 Pseudocode of MagiCoder']\n",
            "['We can distinguish five phases in the procedure that will be discussed in detail in Sections UID18 , UID19 , UID20 , UID23 , UID28 , respectively.', 'Definition of ad hoc data structures: the design of data structures is central to perform an efficient computation; our main data structures are hash tables, in order to guarantee an efficient access both to MedDRA terms and to words belonging to MedDRA terms.', 'Preprocessing of the original text: tokenization (i.e., segmentation of the text into syntactical units), stemming (i.e., reduction of words to a particular root form), elimination of computationally irrelevant words.', 'Word-by-word linear scan of the description and “voting task”: a word “votes” LLTs it belongs to. For each term voted by one or more words, we store some information about the retrieved syntactical matching.', 'Weights calculation: recognized terms are weighted depending on information about syntactical matching.', 'Sorting of voted terms and winning terms release: the set of voted term is pruned, terms are sorted and finally a solution (a set of winning terms) is released.']\n",
            "['The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms.']\n",
            "['With respect to the first version BIBREF7 , we extended our proposal following several directions. First of all, we refined the procedure: MagiCoder has been equipped with some heuristic criteria and we started to address the problem of including auxiliary dictionaries (e.g., in order to deal with synonyms). MagiCoder computational complexity has been carefully studied and we will show that it is linear in the size of the dictionary (in this case, the number of LLTs in MedDRA) and the text description. We performed an accurate test of MagiCoder performances: by means of well-known statistical measures, we collected a significant set of quantitative information about the effective behavior of the procedure. We largely discuss some crucial key-points we met in the development of this version of MagiCoder, proposing short-time solutions we are addressing as work in progress, such as changes in stemming algorithm, considering synonyms, term filtering heuristics.', 'The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms.', 'INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a “voting task”: at the INLINEFORM1 -th step, it marks (i.e., “votes”) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description. Moreover, it keeps track of the position where the INLINEFORM5 -th word occurs in INLINEFORM6 .']\n",
            "['A specific challenge will be to narrow down the data to parts relevant for personal recovery, since there is no control over the discussed topics compared to structured interviews. To investigate how individuals discuss personal recovery online and what (potentially unrecorded) aspects they associate with it, without a priori narrowing down the search-space to specific known keywords seems like a chicken-and-egg problem. We propose to address this challenge by an iterative approach similar to the one taken in a corpus linguistic study of cancer metaphors BIBREF63 . Drawing on results from previous qualitative research BIBREF24 , BIBREF23 , we will compile an initial dictionary of recovery-related terms. Next, we will examine a small portion of the dataset manually, which will be partly randomly sampled and partly selected to contain recovery-related terms. Based on this, we will be able to expand the dictionary and additionally automatically annotate semantic concepts of the identified relevant text passages using a semantic tagging approach such as the UCREL Semantic Analysis System (USAS) BIBREF64 . Crucially for the multilingual aspect of the project, USAS can tag semantic categories in eight languages BIBREF8 . Then, semantic tagging will be applied to the full corpus to retrieve all text passages mentioning relevant concepts. Furthermore, distributional semantics methods BIBREF65 , BIBREF66 can be used to find terms that frequently co-occur with words from our keyword dictionary. Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects.']\n",
            "['The importance of the recovery concept in the design of mental health services has recently been prominently reinforced, suggesting ‘recovery-oriented social enterprises as key component of the integrated service’ BIBREF20 . We think that a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals as diverse as those it is supposed to serve. Therefore, we expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population. The datasets collected in this project can serve as useful resources for future research. More generally, our social-media data-driven approach could be applied to investigate other areas of mental health if it proves successful in leading to relevant new insights.']\n",
            "[\"Since language and culture are important factors in our research questions, we need information on the language of the texts and the country of residence of their authors, which is not provided in a structured format in the three data sources. For language identification, Twitter employs an automatic tool BIBREF48 , which can be used to filter tweets according to 60 language codes, and there are free, fairly accurate tools such as the Google Compact Language Detector, which can be applied to Reddit and blog posts. The location of Twitter users can be automatically inferred from their tweets BIBREF49 or the (albeit noisy) location field in their user profiles BIBREF50 . Only one attempt to classify the location of Reddit users has been published so far BIBREF51 showing meagre results, indicating that the development of robust location classification approaches on this platform would constitute a valuable contribution. Some companies collect mental health-related online data and make them available to researchers subject to approval of their internal review boards, e.g., OurDataHelps by Qntfy or the peer-support forum provider 7 Cups. Unlike `raw' social media data, these datasets have richer user-provided metadata and explicit consent for research usage. On the other hand, less data is available, the process to obtain access might be tedious within the short timeline of a PhD project and it might be impossible to share the used portions of the data with other researchers. Therefore, we will follow up the possibilities of obtaining access to these datasets, but in parallel also collect our own datasets to avoid dependence on external data providers.\"]\n",
            "[\"Concurrently, it has been argued for mental health research that it would constitute a `valuable critical step' BIBREF10 to analyse first-hand accounts by individuals with lived experience of severe mental health issues in blog posts, tweets, and discussion forums. Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 .\"]\n",
            "[\"We plan to collect data relevant for BD in general as well as for personal recovery in BD from three sources varying in their available amount versus depth of the accounts we expect to find: 1) Twitter, 2) Reddit (focusing on mental health-related content unlike previous work), 3) blogs authored by affected individuals. Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent for using their texts.\"]\n",
            "['Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.', 'We implement the GPT-2 model based on the transformers library BIBREF8. The model configuration is 8 attention heads per layer, 8 layers, 512 embedding dimensions, and 1024 feed-forward layer dimensions. We employ the OpenAIAdam optimizer and train the model with 400,000 steps in total on 4 NVIDIA 1080Ti GPUs. The characters with frequency less than 3 in CCPC1.0 are treated as UNK and a vocabulary with 11259 tokens (characters) is finally built up.']\n",
            "['liao2019gpt applied GPT to Chinese classical poetry generation. They pre-trained the model on a Chinese news corpus with 235M sentences and then fine-tuning the model on Chinese poem corpus with 250,000 Jueju and Lvshi, 20,000 CIs, 700,000 pairs of couplets. A key point is they defined a unified format to formulate different types of training samples, as [form, identifier 1, theme, identifier 2, body], where “body” accommodates the full content of an SHI, CI, or couplet in corresponding “form” with “theme” as its title. Experiments demonstrated GPT-based poem generation gained promising performance, meanwhile still faced some limitations, for instance, only 70% of the generated CIs for the Cipai Shuidiaogetou, a sort of CI with quite long body, are correct in form.', 'Regarding this, we think the work of liao2019gpt could be improved in the following three respects. First, there is a large improving room for better fitting the form requirement of CI in the process of generation, especially for those with relatively long body length. Second, their formulation format for training samples can be supplemented, for example, the stanza structure of CI is missing. Third, using contemporary Chinese news corpus to pre-train the model may not be necessary, owing to distinctive differences in both meaning and form between contemporary Chinese and Chinese classical poetry language.']\n",
            "['Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.', 'With this naive GPT-2 model, we see from the experimental results that the generated poems appear pretty good in both meaning and sound(including rhyme), though if being observed carefully, there still exist some in-depth problems in sentence fluency and thematic coherence of the whole poem which are uneasy to solve. As for form, the model can perform well in generating Jueju and Lvshi of SHI whereas rather poorly in generating various Cipai of CI, with quite high form errors. Figure 4(a) is an example of a generated CI by this model, under Cipai of Busuanzi, where two characters are mistakenly missing which obviously violates the form requirement.']\n",
            "['Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.', 'In this paper, we propose a uniformed computational framework that tries to generate major types of Chinese classical poems with two major forms of SHI, Jueju, and Lvshi, as well as 121 major forms (Cipai) of CI using a single model. Preliminary experimental results validate the effectiveness of the proposed framework. The implemented model has been incorporated into Jiuge BIBREF0, the most influential Chinese classical poetry generation system developed by Tsinghua University (refer to http://jiuge.thunlp.cn/).']\n",
            "['Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.']\n",
            "['Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.', \"Besides, we compared our system with a fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github.\"]\n",
            "['Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.']\n",
            "['Experiments ::: Task 1: Word Similarity Task ::: Task Description', 'The task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\\\rho _p$ as the metric, the higher of which the better the word vectors are. The expression of $\\\\rho _p$ is', 'Experiments ::: Task 2: Synonym Selection Task ::: Task Description', 'This task attempts to select the semantically closest word, from the candidate answers, to the stem word. For example, given the stem word “costly” and the candidate answers “expensive, beautiful, popular, complicated”, the most similar word should be “expensive”. For each candidate answer, we compute the cosine similarity score between its word vector and that of the stem word. The candidate answer with the highest score is our final answer for a question. Here we use the TOEFL dataset BIBREF36 with 80 synonym questions and the LEX dataset with 303 questions collected by ourselves.']\n",
            "['To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites. We applied heuristics to identify the content (title, headline and body) of each article and regarded the keyphrases provided in the HTML metadata as the gold standard. A cherry-picked sample document is showcased in Figure , it allows to show present and absent keyphrases, as well as keyphrase variants (in this example News media and journalism).', 'We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm.', \"Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset.\"]\n",
            "['We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm.']\n",
            "['We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm.', \"Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset.\", 'Although in this study we concentrate only on the textual content of the news articles, it is worth noting that the HTML pages also provide additional information that can be helpful in generating keyphrases such as text style properties (e.g. bold, italic), links to related articles, or news categorization (e.g. politics, science, technology).']\n",
            "['Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline –that is, the first sentences from the document–, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.', 'The second baseline we consider, MultipartiteRank BIBREF17, represents the state-of-the-art in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke BIBREF18.']\n",
            "['Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline –that is, the first sentences from the document–, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.', 'The second baseline we consider, MultipartiteRank BIBREF17, represents the state-of-the-art in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke BIBREF18.']\n",
            "['Performance of existing models ::: Models ::: Baseline: FirstPhrase', 'Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline –that is, the first sentences from the document–, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.', 'Performance of existing models ::: Models ::: Baseline, unsupervised: MultipartiteRank', 'The second baseline we consider, MultipartiteRank BIBREF17, represents the state-of-the-art in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke BIBREF18.']\n",
            "['The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results.', 'Two models were trained to bring evidence on the necessity to have datasets from multiple domains. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes), the two models use the same architecture.']\n",
            "['The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results.']\n",
            "['The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results.', 'Two models were trained to bring evidence on the necessity to have datasets from multiple domains. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes), the two models use the same architecture.']\n",
            "['We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm.']\n",
            "['Given a document and the position of a target word, our model computes a probability distribution over possible senses related to that word. The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).']\n",
            "['Given a document and the position of a target word, our model computes a probability distribution over possible senses related to that word. The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).']\n",
            "['Given a document and the position of a target word, our model computes a probability distribution over possible senses related to that word. The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).']\n",
            "[\"In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.\"]\n",
            "[\"The first row shows the best result of the network that we described above (and depicted in Fig. FIGREF4 ). Each of the other rows shows one change that we applied to the network to see the behavior of the network in terms of F-measure. In the middle part, we are specifically concerned about the importance of the presence of a BLSTM layer in our network. So, we introduced some fundamental changes in the input or in the structure of the network. Generally, it is expected that the cosine similarities of closer words (in the context) to the true sense be larger than the incorrect senses' BIBREF17 ; however, if a series of cosine similarities can be encoded through an LSTM (or BLSTM) network should be experimented. We observe if reverse the sequential follow of information into our Bidirectional LSTM, we shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results were notably less than 72.5%.\"]\n",
            "[\"In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.\"]\n",
            "['The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .', 'The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 .', 'The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four.', 'Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams.']\n",
            "['The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .', 'The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 .', 'The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four.', 'Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams.']\n",
            "['The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .', 'The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four.', 'The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 .', 'Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams.']\n",
            "['For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods.']\n",
            "['For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods.']\n",
            "['For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods.']\n",
            "['We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model.']\n",
            "['We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model.', 'In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e.']\n",
            "['Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48).']\n",
            "['While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4.', 'The results are shown in the Table TABREF15. Across different datasets, our method achieves significant improvements over previous non-autoregressive models. Specifically, our method outperforms fertility based NART model with 6.54/7.11 BLEU score improvements on WMT En-De and De-En tasks in similar settings and achieves comparable results with state-of-the-art LSTM-based model on WMT En-De task. Furthermore, our model achieves a speedup of 30.2 (output a single sentence) or 17.8 (teacher rescoring) times over the ART counterparts. Note that our speedups significantly outperform all previous works, because of our lighter design of the NART model: without any computationally expensive module trying to improve the expressiveness.', 'FLOAT SELECTED: Table 1: Performance on WMT14 En-De, De-En and IWSLT14 De-En tasks. “/” means non-reportable.']\n",
            "['The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.', 'We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of heads as the teacher models (Figure FIGREF11). Following BIBREF5, BIBREF12, we replace the target sentences by the decoded output of the teacher models.']\n",
            "['The results are shown in the Table TABREF15. Across different datasets, our method achieves significant improvements over previous non-autoregressive models. Specifically, our method outperforms fertility based NART model with 6.54/7.11 BLEU score improvements on WMT En-De and De-En tasks in similar settings and achieves comparable results with state-of-the-art LSTM-based model on WMT En-De task. Furthermore, our model achieves a speedup of 30.2 (output a single sentence) or 17.8 (teacher rescoring) times over the ART counterparts. Note that our speedups significantly outperform all previous works, because of our lighter design of the NART model: without any computationally expensive module trying to improve the expressiveness.']\n",
            "['In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain.']\n",
            "['In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus from the Discriminating between Similar Languages task BIBREF1 . The corpus consists of 2,000 sentences per language or variety, with between 20 and 100 tokens per sentence, obtained from news headers. In Table 9 we show the results obtained with the proposed representation and the two distributed representations, Skip-gram and SenVec. It is important to notice that, in general, when a particular representation improves for one language is at cost of the other one. We can conclude that the three representations obtained comparative results and support the robustness of the low dimensionality representation.', 'FLOAT SELECTED: Table 9. Accuracy results in the development set of the DSLCC. The significance is marked in bold when some representation obtains significantly better results than the next best performing representation (e.g. results for SenVec in Portugal Portuguese are significantly higher than LDR, which at the same time are significantly higher than Skip-gram).']\n",
            "['Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.']\n",
            "['Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.', 'In this section, we compare with several event-driven stock market prediction baseline methods: (1) Word, BIBREF23 luss2012predicting use bag-of-words represent news events for stock prediction; (2) Event, BIBREF24 ding-EtAl:2014:EMNLP2014 represent events by subject-predicate-object triples for stock prediction; (3) NTN, BIBREF4 ding2015deep learn continues event vectors for stock prediction; (4) KGEB, BIBREF18 ding2016knowledge incorporate knowledge graph into event vectors for stock prediction.']\n",
            "['We compare the performance of our approach against a variety of event embedding models developed in recent years. These models can be categorized into three groups:', 'Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8.', 'Compositional Neural Network (Comp. NN) The event representation in this model is computed by feeding the concatenation of the subject, predicate, and object embedding into a two layer neural network BIBREF17, BIBREF3, BIBREF2.', 'Element-wise Multiplicative Composition (EM Comp.) This method simply concatenates the element-wise multiplications between the verb and its subject/object.', 'Neural Tensor Network This line of work use tensors to learn the interactions between the predicate and its subject/object BIBREF4, BIBREF5. According to the different usage of tensors, we have three baseline methods: Role Factor Tensor BIBREF5 which represents the predicate as a tensor, Predicate Tensor BIBREF5 which uses two tensors learning the interactions between the predicate and its subject, and the predicate and its object, respectively, NTN BIBREF4, which we used as the baseline event embedding model in this paper, and KGEB BIBREF18, which incorporates knowledge graph information in NTN.']\n",
            "[\"BIBREF22 (BIBREF22) and BIBREF21 (BIBREF21) showed that script event prediction is a challenging problem, and even 1% of accuracy improvement is very difficult. Experimental results shown in Table TABREF31 demonstrate that we can achieve more than 1.5% improvements in single model comparison and more than 1.4% improvements in multi-model integration comparison, just by replacing the input embeddings, which confirms that better event understanding can lead to better inference results. An interesting result is that the event embeddings only incorporated with intents achieved the best result against other baselines. This confirms that capturing people's intents is helpful to infer their next plan. In addition, we notice that the event embeddings only incorporated with sentiment also achieve better performance than SGNN. This is mainly because the emotional consistency does also contribute to predicate the subsequent event.\"]\n",
            "['Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.']\n",
            "['We also use ATOMIC BIBREF7 as the event sentiment labeled dataset. In this dataset, the sentiment of the event is labeled as words. For example, the sentiment of “PersonX broke vase” is labeled as “(sad, be regretful, feel sorry, afraid)”. We use SenticNet BIBREF14 to normalize these emotion words ($W=\\\\lbrace w_1, w_2, \\\\dots , w_n\\\\rbrace $) as the positive (labeled as 1) or the negative (labeled as -1) sentiment. The sentiment polarity of the event $P_e$ is dependent on the polarity of the labeled emotion words $P_W$: $P_e=1$, if $\\\\sum _i P_{w_i}>0$, or $P_e=-1$, if $\\\\sum _i P_{w_i}<0$. We use the softmax binary classifier to learn sentiment enhanced event embeddings. The input of the classifier is event embeddings, and the output is its sentiment polarity (positive or negative). The model is trained in a supervised manner by minimizing the cross entropy error of the sentiment classification, whose loss function is given below.', 'Extensive experiments show that incorporating external commonsense knowledge brings promising improvements to event embeddings, achieving 78% and 200% improvements on hard similarity small and big dataset, respectively. With better embeddings, we can achieve superior performances on script event prediction and stock market prediction compared to state-of-the-art baseline methods.', 'Except for the hard similarity task, we also evaluate our approach on the transitive sentence similarity dataset BIBREF19, which contains 108 pairs of transitive sentences: short phrases containing a single subject, object and verb (e.g., agent sell property). It also has another dataset which consists of 200 sentence pairs. In this dataset, the sentences to be compared are constructed using the same subject and object and semantically correlated verbs, such as `spell’ and `write’; for example, `pupils write letters’ is compared with `pupils spell letters’. As this dataset is not suitable for our task, we only evaluate our approach and baselines on 108 sentence pairs.', 'Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.']\n",
            "['We also use ATOMIC BIBREF7 as the event sentiment labeled dataset. In this dataset, the sentiment of the event is labeled as words. For example, the sentiment of “PersonX broke vase” is labeled as “(sad, be regretful, feel sorry, afraid)”. We use SenticNet BIBREF14 to normalize these emotion words ($W=\\\\lbrace w_1, w_2, \\\\dots , w_n\\\\rbrace $) as the positive (labeled as 1) or the negative (labeled as -1) sentiment. The sentiment polarity of the event $P_e$ is dependent on the polarity of the labeled emotion words $P_W$: $P_e=1$, if $\\\\sum _i P_{w_i}>0$, or $P_e=-1$, if $\\\\sum _i P_{w_i}<0$. We use the softmax binary classifier to learn sentiment enhanced event embeddings. The input of the classifier is event embeddings, and the output is its sentiment polarity (positive or negative). The model is trained in a supervised manner by minimizing the cross entropy error of the sentiment classification, whose loss function is given below.', 'Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.']\n",
            "['We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.']\n",
            "['Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy BIBREF8 , BIBREF9 . Ballesteros et al. ballesteros train a character-level model for parsing. Zhang et al. zhang do away with words completely, and train a convolutional neural network to do text classification directly from characters.', 'Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.', 'What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators.', 'The Char2Vec model', 'We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that incorporating morphological knowledge helps structure the embedding space in such a way that affixation corresponds to a regular shift in the embedding space. We test both hypotheses directly in § \"Capturing semantic similarity\" and § \"Capturing syntactic and semantic regularity\" respectively.', 'The starting point for our model is the skip-gram with negative sampling (SGNS) objective of Mikolov et al. word2vec2. For a vocabulary $V$ of size $|V|$ and embedding size $N$ , SGNS learns two embedding tables $W, C \\\\in \\\\mathbb {R}^{N \\\\times |V|}$ , the target and context vectors. Every time a word $w$ is seen in the corpus with a context word $c$ , the tables are updated to maximize', '$$\\\\log \\\\sigma (w \\\\cdot c) + \\\\sum _{i = 1}^{k} \\\\mathbb {E}_{\\\\tilde{c}_i \\\\sim P(w)} [\\\\log \\\\sigma (-w \\\\cdot \\\\tilde{c}_i)]$$ (Eq. 7)', 'where $P(w)$ is a noise distribution from which we draw $k$ negative samples. In the end, the target vector for a word $w$ should have high inner product with context vectors for words with which it is typically seen, and low inner products with context vectors for words it is not typically seen with. Figure 1 illustrates this for a particular example. In Mikolov et al. word2vec2, the noise distribution $P(w)$ is proportional to the unigram probability of a word raised to the 3/4th power BIBREF11 .', 'Our innovation is to replace $W$ with a trainable function $f$ that accepts a sequence of characters and returns a vector of length $N$ (i.e. $f: A^{<\\\\omega } \\\\rightarrow \\\\mathbb {R}^N$ , where $A$ is the alphabet we are considering and $A^{<\\\\omega }$ denotes the finite length strings over the alphabet $A$ ). We still keep the table of context embeddings $C$ , and our model objective is still to minimize', '$$\\\\log \\\\sigma (f(w) \\\\cdot c) + \\\\sum _{i = 1}^{k} \\\\mathbb {E}_{\\\\tilde{c}_i \\\\sim P(w)} [\\\\log \\\\sigma (-f(w) \\\\cdot \\\\tilde{c}_i)]$$ (Eq. 8)', 'where we now treat $w$ as a sequence of characters. After training, $f$ can be used to produce an embedding for any sequence of characters, even if it was not previously seen in training.', 'The process of calculating $f$ on a word is illustrated in Figure 2 . We first pad the word with beginning and end of word tokens, and then pass the characters of the word into a character lookup table. As the link between characters and morphemes is non-compositional and requires essentially memorizing a sequence of characters, we use LSTMs BIBREF21 to encode the letters in the word, as they have been shown to capture non-local and non-linear dependencies. We run a forward and a backward LSTM over the character embeddings. The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol. This is necessary to align the resulting embeddings, so that the LSTM hidden states taken together correspond to a partition of the word into two without overlap.', 'The LSTMs output two sequences of vectors $h_0^{f}, \\\\dots , h_n^f$ and $h_n^{b}, \\\\dots , h_0^b$ . We then concatenate the resulting vectors, and pass them through a shared feed-forward layer to obtain a final sequence of vectors $h_i$ . Each vector corresponds to two half-words: one half read by the forward LSTM, and the other by the backward LSTM.', 'We then learn an attention model over these hidden states: given a hidden state $h_i$ , we calculate a weight $\\\\alpha _i = a(h_i)$ such that $\\\\sum \\\\alpha _i = 1$ , and then calculate the resulting vector for the word $w$ as $f(w) = \\\\sum \\\\alpha _i h_i$ . Following Bahdanau et al. bahdanau, we calculate $a$ as', '$$a(h_i) = \\\\frac{\\\\exp (v^{T} \\\\tanh (Wh_i))}{\\\\sum _j \\\\exp (v^{T} \\\\tanh (Wh_j))}$$ (Eq. 10)', 'i.e. a softmax over the hidden states.', 'Capturing morphology via attention', 'Previous work on bidirectional LSTM character-level models used both LSTMs to read the entire word BIBREF8 , BIBREF22 . This can lead to redundancy, as both LSTMs are used to capture the full word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves. This means one of the LSTMs can specialize on word prefixes and roots, while the other memorizes possible suffixes. In addition, when dealing with an unknown word, it can be split into known and unknown components. The model can then use the semantic knowledge it has learnt for a known component to predict a representation for the unknown word as a whole.', 'We hypothesize that the natural place to split words is on morpheme boundaries, as morphemes are the smallest unit of language which carry semantic meaning. We test the splitting capabilities of our model in § \"Morphological awareness\" .', 'Experiments', 'We evaluate our model on three tasks: morphological analysis (§ \"Morphological awareness\" ), semantic similarity (§ \"Capturing semantic similarity\" ), and analogy retrieval (§ \"Capturing syntactic and semantic regularity\" ). We trained all of the models once, and then use the same trained model for all three tasks – we do not perform hyperparameter tuning to optimize performance on each task.', 'We trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006 cleaned-up dump of Wikipedia. We only trained on words which appeared more than 5 times in our corpus. We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and Theano BIBREF25 , BIBREF26 . We initialized the context lookup table using word2vec, and kept it fixed during training. In all character-level models, the character embeddings have dimension $d_C = 64$ , while the forward and backward LSTMs have dimension $d_{LSTM} = 256$ . The concatenation of both therefore has dimensionality $d = 512$ . The concatenated LSTM hidden states are then compressed down to $d_{word} = 256$ by a feed-forward layer.', 'As baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the character-level model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and backward states, passed through a feedforward layer. We refer to this model as C2V-NO-ATT. We also constructed count-based vectors using SVD on PPMI-weighted co-occurence counts, with a window size of 3. We kept the top 256 principal components in the SVD decomposition, to obtain embeddings with the same size as our other models.', 'To evaluate our model, we evaluate its use as a morphological analyzer (§ \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (§ \"Capturing semantic similarity\" ), and examine the structure of the embedding space (§ \"Capturing syntactic and semantic regularity\" ).', 'The main innovation of our Char2Vec model compared to existing recurrent character-level models is the capability to split words and model each half independently. Here we test whether our model segmentations correspond to gold-standard morphological analyses.', 'We obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project BIBREF27 . We then converted these into surface-level segmentations using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three.', 'Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ – that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above non-morpheme boundaries.', 'We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model.', 'We report results of a random baseline as a point of comparison, which randomly places morpheme boundaries inside the word. We also report the results of the Porter stemmer, where we place a morpheme boundary at the end of the stem, then randomly thereafter.', 'Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP.', 'As the test set is dominated by words with simple morphology, we also extracted all the morphologically rich words with 3 or more morphemes, and created a separate evaluation on this subsection. We report the results in Table 1 .', 'As the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes.', 'We show some model analyses against the gold standard in Table 2 .', \"Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\\\rho $ ) between model judgments and human judgments BIBREF20 .\", 'We use the WordSim353 dataset BIBREF29 , the test split of the MEN dataset BIBREF30 , and the Rare Word (RW) dataset BIBREF31 . The word pairs in the WordSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset.', 'We report results for in-corpus word pairs in Table 3 , and for all word pairs for those models able to predict vectors for unseen words in Table 4 .', 'Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process.', 'We also present some word nearest neighbours for our Char2Vec model in Table 5 , both on the whole vocabulary and then filtering the nearest neighbours to only include words which appear 100 times or more in our corpus. This corresponds to keeping the top 10k words, which is common among language models BIBREF8 , BIBREF9 . We note that nearest neighbour predictions include words that are orthographically distant but semantically similar, showing that our model has the capability to learn to compose characters into word meanings.', \"We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can `memorize' the mapping between the orthography and word semantics.\", 'Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space.', 'To do this, we use the Google analogy dataset BIBREF3 . This consists of 19544 questions of the form “A is to B as C is to X”. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (“Paris is to France as Berlin is to X) and currency-country relationships. Example syntactic questions are adjective-adverb relationships (“amazing is to amazingly as apparent is to X”) and opposites formed by prefixing a negation particle (“acceptable is to unacceptable as aware is to X”). This results in 5537 semantic analogies and 10411 syntactic analogies.', 'We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form “A is to B as C is to X”, we find the word $w$ which satisfies', '$$w = \\\\operatornamewithlimits{argmax}_{w \\\\in V - \\\\lbrace a, b, c\\\\rbrace } \\\\cos (w, b - a + c)$$ (Eq. 28)', 'where $a,\\\\, b,\\\\, c$ are the word vectors for the words A, B and C respectively.', 'We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.', 'Discussion', 'We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.', 'This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.', 'Conclusion', 'In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space.', 'We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features.', 'Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better.']\n",
            "['We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.']\n",
            "['We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.']\n",
            "[\"Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\\\rho $ ) between model judgments and human judgments BIBREF20 .\"]\n",
            "[\"Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\\\rho $ ) between model judgments and human judgments BIBREF20 .\"]\n",
            "['Concretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps. In the first one, we project the document in two different ways, one based on question-attention, one based on answer-attention and then compare the two projected document representations to determine whether the answer matches the question. In the second one, every question-answer pair is reformatted into a statement, then the whole task is treated through textual entailment.', 'We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG INLINEFORM0 BIBREF20 . Unlike accuracy which evaluates if the question is correctly answered or not, NDCG INLINEFORM1 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking.', 'Baseline Systems', 'This work focuses on the comparison with systems about distributed representation learning and deep learning:', 'NR. The Neural Reasoner BIBREF21 has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each sentence is encoded by a GRU into a vector. In each reasoning layer, NR lets the question representation interact with each fact representation as reasoning process. Finally, all temporary reasoning clues are pooled as answer representation.', 'AR. The Attentive Reader BIBREF2 is implemented by modeling the whole D as a word sequence – without specific sentence / snippet representations – using an LSTM. Attention mechanism is implemented at word representation level.', 'Overall, baselines Addition and Addition-proj do not involve complex composition and inference. NR and AR represent the top-performing deep neural networks in QA tasks.', 'Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task.', 'Dataset', 'MCTest has two subsets. MCTest-160 is a set of 160 items, each consisting of a document, four questions followed by one correct anwer and three incorrect answers (split into 70 train, 30 dev and 60 test) and MCTest-500 a set of 500 items (split into 300 train, 50 dev and 150 test).']\n",
            "['Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task.']\n",
            "['This work focuses on the comparison with systems about distributed representation learning and deep learning:', 'Addition. Directly compare question and answers without considering the D. Sentence representations are computed by element-wise addition over word representations.', 'Addition-proj. First compute sentence representations for Q, A and all D sentences as the same way as Addition, then match the two sentences in D which have highest similarity with Q and A respectively.', 'NR. The Neural Reasoner BIBREF21 has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each sentence is encoded by a GRU into a vector. In each reasoning layer, NR lets the question representation interact with each fact representation as reasoning process. Finally, all temporary reasoning clues are pooled as answer representation.', 'AR. The Attentive Reader BIBREF2 is implemented by modeling the whole D as a word sequence – without specific sentence / snippet representations – using an LSTM. Attention mechanism is implemented at word representation level.']\n",
            "['Baseline Systems', 'This work focuses on the comparison with systems about distributed representation learning and deep learning:', 'NR. The Neural Reasoner BIBREF21 has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each sentence is encoded by a GRU into a vector. In each reasoning layer, NR lets the question representation interact with each fact representation as reasoning process. Finally, all temporary reasoning clues are pooled as answer representation.', 'AR. The Attentive Reader BIBREF2 is implemented by modeling the whole D as a word sequence – without specific sentence / snippet representations – using an LSTM. Attention mechanism is implemented at word representation level.', 'Overall, baselines Addition and Addition-proj do not involve complex composition and inference. NR and AR represent the top-performing deep neural networks in QA tasks.']\n",
            "['NR. The Neural Reasoner BIBREF21 has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each sentence is encoded by a GRU into a vector. In each reasoning layer, NR lets the question representation interact with each fact representation as reasoning process. Finally, all temporary reasoning clues are pooled as answer representation.', 'AR. The Attentive Reader BIBREF2 is implemented by modeling the whole D as a word sequence – without specific sentence / snippet representations – using an LSTM. Attention mechanism is implemented at word representation level.']\n",
            "['The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.']\n",
            "['The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.']\n",
            "['First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.', \"By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality.\"]\n",
            "['In this work, we propose an acoustic model based on ResNet14 followed by an RNN to recognize phoneme sequence directly with CTC loss and train a simple RNN lastly to get posteriors for recognizing dialect category, forming a two-stage LID system. The system links the different stages by using intermediate features extracted by a shallow ResNet14 architecture. Compared with a simple network or the three-stage system, the two-stage system achieves the state-of-the-art in the Chinese dialect recognition task. We believe this idea of two-stage training can provide inspirations for learning different classes knowledge and can extend to other fields.']\n",
            "['We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task).']\n",
            "['We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task).']\n",
            "['We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task).']\n",
            "['Our LMs are Transformer BIBREF6 decoders (transformer_big) trained using the Tensor2Tensor library BIBREF19 . We delay SGD updates BIBREF20 , BIBREF21 with factor 2 to simulate 500K training steps with 8 GPUs on 4 physical GPUs. Training batches contain about 4K source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets from the WMT evaluation campaigns BIBREF22 after language detection BIBREF23 (138M sentences) and subword segmentation using byte pair encoding BIBREF12 with 32K merge operations. For decoding, we use our SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8 .']\n",
            "['We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments.']\n",
            "['We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC – a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 .']\n",
            "['We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC – a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 .']\n",
            "['Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text. We conclude that the proposed methods, while imperfect, are capable of providing useful suggestions for how subjective bias in real-world news or political text can be reduced.']\n",
            "['The Wiki Neutrality Corpus consists of aligned sentences pre and post-neutralization by English Wikipedia editors (Table TABREF3). We used regular expressions to crawl 423,823 Wikipedia revisions between 2004 and 2019 where editors provided NPOV-related justification BIBREF11, BIBREF2, BIBREF12. To maximize the precision of bias-related changes, we ignored revisions where']\n",
            "['We introduce the Wiki Neutrality Corpus (WNC). This is a new parallel corpus of 180,000 biased and neutralized sentence pairs along with contextual sentences and metadata. The corpus was harvested from Wikipedia edits that were designed to ensure texts had a neutral point of view. WNC is the first parallel corpus targeting biased and neutralized language. We also define the task of neutralizing subjectively biased text. This task shares many properties with tasks like detecting framing or epistemological bias BIBREF2, or veridicality assessment/factuality prediction BIBREF7, BIBREF8, BIBREF9, BIBREF10. Our new task extends these detection/classification problems into a generation task: generating more neutral text with otherwise similar meaning.']\n",
            "['We propose the task of neutralizing text, in which the algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed.']\n",
            "[\"According to The National Institute on Deafness, one in thousand infants is born deaf. An additional one to six per thousand are born with hearing loss at different levels BIBREF0. Sign language is commonly used by Deaf and Hard-of-Hearing (DHH) persons to communicate via hand gestures. An automatic sign language recognizer enables an ASL user to translate the sign language to written text or speech, allowing them to communicate with people who are not familiar with ASL. There is a tremendous rise in the popularity of personal digital assistants; available on user's personal and wearable devices (Google Now, Amazon Alexa and Apple Siri, etc.) and also in the form of standalone devices (Amazon Echo and Google Home smart speakers). These devices are primarily controlled through voice, and hence, their functionality is not readily available to DHH users. An automatic sign recognizer can also enable the interaction between a DHH user and a digital assistant.\", 'Most current systems have capability of ASL recognition with RGB video data BIBREF1, BIBREF2, BIBREF3. An ASL sign is performed by a combination of hand gestures, facial expressions and postures of the body. Sequential motion of specific body locations (such as hand-tip, neck and arm) provide informative cues about a sign. Using video data, it is difficult to extract different body locations and associated motion sequences from a series of RGB frames. Microsoft Kinect is a 3D camera sensor which can use the depth information of a person to capture 3D coordinates of his/her body location across a video. This sequence of 3D body location is referred by skeletal data BIBREF4. To the best of our knowledge, there is no publicly available skeletal dataset in literature for ASL recognition.', 'We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. A new data augmentation technique was introduced that allowed the LSTM networks to capture spatial dynamics among joints. Finally, a large public dataset for ASL recognition will be released to the community to spur research in this direction; and bring benefits of digital assistants to the deaf and hard of hearing community. For future research direction, we are looking into the problem of sentence level ASL recognition. We also plan to use other data modality such as wifi signals which can be complimentary to video in sign language recognition.']\n",
            "['Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal). Unlike traditional image classification or object detection models where neural networks learn hierarchical spatial features from data, sign recognition requires capture of temporal body motion.', 'RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated. The forget gate processes it to remove unnecessary information, and outputs ${f_t}$ which gets multiplied with the previously stored memory ${C_{t-1}}$ and produces a refined memory for the current time.', \"Given a sample skeletal data of $R^{T \\\\times J \\\\times 3}$, where $T$ denotes time axis, $J$ is the number of body joints and the last dimension is the 3D coordinates of each joint. We flatten every dimension except time and at each time step we can feed a vector of size $R^{3 \\\\times J}$ as input. However, we have empirically verified that learning a sequential pattern for each coordinate axis independently and combining them later shows stronger classification performance. Based on this, we trained three different 2 layer LSTMs for data from x, y, and z coordinates separately; and concatenate their final embedding to produce softmax output. In this setting, each separate LSTM receives data as $R^{T \\\\times J}$ and final embedding size is $R^{3\\\\times S}$ where $S$ is the state size of LSTM cell. Figure FIGREF15 (a) shows the architecture where as a sample arrives, just before entering into main network, data along separate axis is split and entered into three different LSTM networks. The model concatenates the final state from each of the separate LSTM networks; followed by feeding this into the softmax layer for classification. This approach is referred by Axis Independent Architecture (AI-LSTM). Implementation details such as values of T and J are provided in the `Experiments' section.\", \"AI-LSTM, described in last section, works by modeling temporal dynamics of body joints' data over time. However, there can be spatial interactions with joints at a specific time step. It fails to capture any such interaction among joints in a given time. To incorporate spatial relationship among joints, we propose a simple novel data augmentation technique for skeletal data. We do this by origin transfer. For each frame in a gesture sample, we use each wrist joints as origin and transform all other joints' data by subtracting that origin from them. In this way spatial information is added to the input. We refer this model with spatial data augmentation as Spatial AI-LSTM. This augmentation technique is depicted in Figure FIGREF21. A sample data of form $R^{T \\\\times 6 \\\\times 3}$ results in a representation of $R^{T \\\\times 5 \\\\times 3}$ after subtracting left wrist joint (origin transfer). After this augmentation process, each sample is a $R^{20 \\\\times 16 \\\\times 3}$ matrix. Hence, each separate LSTM networks in our Spatial AI-LSTM network receives an input of $R^{20 \\\\times 16}$.\", 'We hypothesize that, some signs that have mostly similar skeletal motion pattern could be distinguishable using hand shape information. We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back–propagation, both networks are trained on their own score. The combined network acts like a model ensemble and some sign classes which are confused by RNN network alone might have an improved recognition accuracy with this approach.', 'To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train.']\n",
            "['Our Approach ::: Recurrent Neural Networks (RNN)', 'RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated. The forget gate processes it to remove unnecessary information, and outputs ${f_t}$ which gets multiplied with the previously stored memory ${C_{t-1}}$ and produces a refined memory for the current time.', 'Our Approach ::: 3D Convolutional Neural Network', 'Traditional convolutional neural network (CNN) is two dimensional in which each layer has a stack of 2D feature maps generated from previous layer or from inputs in case of first layer. A layer also has a certain numbers of filters which are rectangular patches of parameters. Each filter convolves over the stack of 2D feature maps at previous layer and produces feature maps (equal to the number of filters in the current layer) at current layer. The operation is given by Equation DISPLAY_FORM17 where $F_{i,j}^{l}$ denotes the value of feature map at $l^{th}$ layer at location $(i,j)$. $\\\\odot $ represents dot product of filter $W$ and associated feature map patch in previous layer.', 'Standard CNN fails to capture the temporal information associated with data, which is important in video or any type of sequential data representation. To solve this problem, 3D convolution was introduced in BIBREF2. The key difference is that kernels are 3D and sub sampling (pooling) layers work across three dimensions.', 'Our Approach ::: Axis Independent LSTM', \"Given a sample skeletal data of $R^{T \\\\times J \\\\times 3}$, where $T$ denotes time axis, $J$ is the number of body joints and the last dimension is the 3D coordinates of each joint. We flatten every dimension except time and at each time step we can feed a vector of size $R^{3 \\\\times J}$ as input. However, we have empirically verified that learning a sequential pattern for each coordinate axis independently and combining them later shows stronger classification performance. Based on this, we trained three different 2 layer LSTMs for data from x, y, and z coordinates separately; and concatenate their final embedding to produce softmax output. In this setting, each separate LSTM receives data as $R^{T \\\\times J}$ and final embedding size is $R^{3\\\\times S}$ where $S$ is the state size of LSTM cell. Figure FIGREF15 (a) shows the architecture where as a sample arrives, just before entering into main network, data along separate axis is split and entered into three different LSTM networks. The model concatenates the final state from each of the separate LSTM networks; followed by feeding this into the softmax layer for classification. This approach is referred by Axis Independent Architecture (AI-LSTM). Implementation details such as values of T and J are provided in the `Experiments' section.\", 'Our Approach ::: Spatial AI-LSTM', \"AI-LSTM, described in last section, works by modeling temporal dynamics of body joints' data over time. However, there can be spatial interactions with joints at a specific time step. It fails to capture any such interaction among joints in a given time. To incorporate spatial relationship among joints, we propose a simple novel data augmentation technique for skeletal data. We do this by origin transfer. For each frame in a gesture sample, we use each wrist joints as origin and transform all other joints' data by subtracting that origin from them. In this way spatial information is added to the input. We refer this model with spatial data augmentation as Spatial AI-LSTM. This augmentation technique is depicted in Figure FIGREF21. A sample data of form $R^{T \\\\times 6 \\\\times 3}$ results in a representation of $R^{T \\\\times 5 \\\\times 3}$ after subtracting left wrist joint (origin transfer). After this augmentation process, each sample is a $R^{20 \\\\times 16 \\\\times 3}$ matrix. Hence, each separate LSTM networks in our Spatial AI-LSTM network receives an input of $R^{20 \\\\times 16}$.\", 'Our Approach ::: Combined Network', 'We hypothesize that, some signs that have mostly similar skeletal motion pattern could be distinguishable using hand shape information. We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back–propagation, both networks are trained on their own score. The combined network acts like a model ensemble and some sign classes which are confused by RNN network alone might have an improved recognition accuracy with this approach.']\n",
            "['For generative hate speech intervention, we evaluated the following three methods.', 'Seq2Seq BIBREF25, BIBREF24: The encoder consists of 2 bidirectional GRU layers. The decoder consists of 2 GRU layers followed by a 3-layer MLP (Multi-Layer Perceptron).', 'Variational Auto-Encoder (VAE) BIBREF26: The structure of the VAE model is similar to that of the Seq2Seq model, except that it has two independent linear layers followed by the encoder to calculate the mean and variance of the distribution of the latent variable separately. We assume the latent variable follows a multivariate Gaussian Distribution. KL annealing BIBREF27 is applied during training.', 'Reinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target. In our implementation, the function of the first part of the reward ($\\\\log p(r|c)$) is conveyed by the MLE loss. A curriculum learning strategy is adopted for the reward of $\\\\log p_{back}(c|r)$ as in BIBREF28. Same as in BIBREF21 and BIBREF28, a baseline strategy is employed to estimate the average reward. We parameterize it as a 3-layer MLP.']\n",
            "['For generative hate speech intervention, we evaluated the following three methods.', 'Seq2Seq BIBREF25, BIBREF24: The encoder consists of 2 bidirectional GRU layers. The decoder consists of 2 GRU layers followed by a 3-layer MLP (Multi-Layer Perceptron).', 'Variational Auto-Encoder (VAE) BIBREF26: The structure of the VAE model is similar to that of the Seq2Seq model, except that it has two independent linear layers followed by the encoder to calculate the mean and variance of the distribution of the latent variable separately. We assume the latent variable follows a multivariate Gaussian Distribution. KL annealing BIBREF27 is applied during training.', 'Reinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target. In our implementation, the function of the first part of the reward ($\\\\log p(r|c)$) is conveyed by the MLE loss. A curriculum learning strategy is adopted for the reward of $\\\\log p_{back}(c|r)$ as in BIBREF28. Same as in BIBREF21 and BIBREF28, a baseline strategy is employed to estimate the average reward. We parameterize it as a 3-layer MLP.']\n",
            "['For generative hate speech intervention, we evaluated the following three methods.', 'Seq2Seq BIBREF25, BIBREF24: The encoder consists of 2 bidirectional GRU layers. The decoder consists of 2 GRU layers followed by a 3-layer MLP (Multi-Layer Perceptron).', 'Variational Auto-Encoder (VAE) BIBREF26: The structure of the VAE model is similar to that of the Seq2Seq model, except that it has two independent linear layers followed by the encoder to calculate the mean and variance of the distribution of the latent variable separately. We assume the latent variable follows a multivariate Gaussian Distribution. KL annealing BIBREF27 is applied during training.', 'Reinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target. In our implementation, the function of the first part of the reward ($\\\\log p(r|c)$) is conveyed by the MLE loss. A curriculum learning strategy is adopted for the reward of $\\\\log p_{back}(c|r)$ as in BIBREF28. Same as in BIBREF21 and BIBREF28, a baseline strategy is employed to estimate the average reward. We parameterize it as a 3-layer MLP.']\n",
            "['If the worker thinks no hate speech exists in the conversation, then the answers to both questions are “n/a”. To provide context, the definition of hate speech from Facebook: “We define hate speech as a direct attack on people based on what we call protected characteristics — race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability.” is presented to the workers. Also, to prevent workers from using hate speech in the response or writing responses that are too general, such as “Please do not say that”, we provide additional instructions and rejected examples.']\n",
            "['To evaluate the quality of our sentence compression model, we used the Annotated Gigaword corpus BIBREF21 as the benchmark BIBREF22. The data includes approximately 3.8 M training samples, 400 K validation samples, and 2 K test samples. The byte pair encoding (BPE) algorithm BIBREF23 was adopted for subword segmentation, and the vocabulary size was set at 40 K for our supervised, unsupervised and semi-supervised settings BIBREF24.', 'Baseline systems include AllText and F8W BIBREF22, BIBREF25. F8W is simply the first 8 words of the input, and AllText uses the whole text as the compression output. The $F_1$ score of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) was used to evaluate this task BIBREF26. We use beam search with a beam size of 5, the length length normalization of 0.5, and the coverage penalty of 0.2.']\n",
            "['Baseline systems include AllText and F8W BIBREF22, BIBREF25. F8W is simply the first 8 words of the input, and AllText uses the whole text as the compression output. The $F_1$ score of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) was used to evaluate this task BIBREF26. We use beam search with a beam size of 5, the length length normalization of 0.5, and the coverage penalty of 0.2.']\n",
            "['Baseline systems include AllText and F8W BIBREF22, BIBREF25. F8W is simply the first 8 words of the input, and AllText uses the whole text as the compression output. The $F_1$ score of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) was used to evaluate this task BIBREF26. We use beam search with a beam size of 5, the length length normalization of 0.5, and the coverage penalty of 0.2.']\n",
            "[\"To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.\", 'According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:']\n",
            "['According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:']\n",
            "[\"To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.\"]\n",
            "[\"The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.\"]\n",
            "[\"The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.\", 'FLOAT SELECTED: Table 2: Results on CoNLL-03 and OntoNotes 5.']\n",
            "[\"The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.\"]\n",
            "['To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\\\rightarrow $ City and Massachusetts $\\\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long. Example aliases for Boston are “Beantown” and “The Cradle of Liberty”. If there are multiple types per alias, we use the sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data.', 'The Wikidata types that we obtain after processing the Wikidata dumps are fine-grained. However, certain NER tasks require coarse-grained types. For instance, CoNLL-03 task has a single Location label that consists of cities, states, countries, and other geographic location. To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure induced by the subclass_of property. Examples of subclass_of hierarchies in Wikidata are: City $\\\\rightarrow $ Human Settlement $\\\\rightarrow $ Geographic Location, and Artist $\\\\rightarrow $ Creator $\\\\rightarrow $ Person. We change the types granularity depending on the NER task by traversing up, from fine-grained types to the target coarse-grained types. For instance, we merge the Artist and Painter types to Person, and the River and Mountain types to Location.']\n",
            "['In this section, we address the issue of building a high-quality gazetteer dictionary $M$ that maps entities to types, e.g., $M$[Andy Murray] $\\\\rightarrow $ Person. In this work, we use Wikidata, an open source structured knowledge-base, as the source of gazetteers. Although, Wikidata and DBpedia are similar knowledge bases, we choose Wikidata because, as of 2019, it provides data on around 45 million entities compared to around 5 million in DBpedia.', 'To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\\\rightarrow $ City and Massachusetts $\\\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long. Example aliases for Boston are “Beantown” and “The Cradle of Liberty”. If there are multiple types per alias, we use the sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.', 'We applied three different versions of audio embedding (AUD) on the top 1000, 3000 and 5000 words with the highest frequencies in LibriSpeech: (i) phonetic embedding only obtained in Stage 1 in Subsection SECREF2 (AUD-ph); (ii) phonetic-and-semantic embedding obtained by Stages 1 and 2 in Subsections SECREF2 , SECREF11 , except the speaker characteristics not disentangled (AUD-(ph-+se)), or INLINEFORM0 , INLINEFORM1 in ( EQREF7 ), ( EQREF9 ) not considered; (iii) complete phonetic-and-semantic embedding as proposed in this paper including Stages 1 and 2 (AUD-(ph+se)). So this is for ablation study.']\n",
            "['A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic embeddings only with speaker characteristics disentangled.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.']\n",
            "['We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the “clean\" and “others\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.']\n",
            "['We ran SVM-Rank with different combinations of features listed in Table 2 , but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability.']\n",
            "['We ran SVM-Rank with different combinations of features listed in Table 2 , but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability.']\n",
            "['The idea behind the QA is that we use CNN BIBREF2 with additional features. This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems BIBREF15 , BIBREF2 , BIBREF16 , BIBREF17 and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.g. TF-IDF and LSI.', 'In our model, 10 convolution filters (length = 2) were applied to two adjacent input nodes because these nodes are the same feature type. An average pooling layer (length = 100) is then utilized to synthesize important features. To enhance the performance of CNN, two additional statistic features: TF-IDF and LSI were concatenated with the result of the pooling layer, then fed them into a 2-layer Perceptron model to predict the answer.']\n",
            "['To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model—i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation.']\n",
            "['For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.']\n",
            "['Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the current time step.']\n",
            "['Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the current time step.']\n",
            "['For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.']\n",
            "['For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.']\n",
            "['Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the current time step.']\n",
            "['To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model—i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation.', 'To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related—especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.']\n",
            "['For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.', 'To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related—especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.']\n",
            "['For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.']\n",
            "['We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Naïve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets.']\n",
            "['We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Naïve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets.']\n",
            "['However, completely automated extraction of condition-action statements does not seem possible. This is due among other things to the variety of linguistic expressions used in condition-action sentences. For example, they are not always in form of \"{if} condition {then} action”. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient\\'s clinical situation”, we have a condition-action sentence without an \"{if}\" term.']\n",
            "['The annotation of the guidelines text (the next step), focused on determining whether there were condition statements in the candidate sentences or not. The instruction to the annotators were to try to paraphrase candidate sentences as sentences with \"if condition, then consequence\". If the transformed/paraphrased sentence conveyed the same meaning as the original, we considered to be a condition-consequence sentence. Then we we could annotate condition and consequence parts. For example, we paraphrased \"Beta-blockers, including eye drops, are contraindicated in patients with asthma\" to \"If patients have asthma, then beta-blockers, including eye drops, are contraindicated\". The paraphrased sentence conveys same meaning. So it became a condition-consequence sentence in our dataset. On the other hand, for example, we cannot paraphrase \"Further, the diagnostic criteria for CKD do not consider age-related decline in kidney function as reflected in estimated GFR\" to an if-then sentence.', 'FLOAT SELECTED: Table 1: Examples of classified sentence classes']\n",
            "['$\\\\bullet $ Word-overlap Metrics (WOMs): We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .', '$\\\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet.', '$\\\\bullet $ Readability quantifies the difficulty with which a reader understands a text, as used for e.g. evaluating summarisation BIBREF27 or text simplification BIBREF28 . We measure readability by the Flesch Reading Ease score (re) BIBREF29 , which calculates a ratio between the number of characters per sentence, the number of words per sentence, and the number of syllables per word. Higher re score indicates a less complex utterance that is easier to read and understand. We also consider related measures, such as characters per utterance (len) and per word (cpw), words per sentence (wps), syllables per sentence (sps) and per word (spw), as well as polysyllabic words per utterance (pol) and per word (ppw). The higher these scores, the more complex the utterance.', '$\\\\bullet $ Grammaticality: In contrast to previous NLG methods, our corpus-based end-to-end systems can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy data. As a first approximation of grammaticality, we measure the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs). The lower the msp, the more grammatically correct an utterance is. The Stanford parser score is not designed to measure grammaticality, however, it will generally prefer a grammatical parse to a non-grammatical one. Thus, lower parser scores indicate less grammatically-correct utterances. In future work, we aim to use specifically designed grammar-scoring functions, e.g. BIBREF26 , once they become publicly available.']\n",
            "['$\\\\bullet $ Word-overlap Metrics (WOMs): We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .', '$\\\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet.', 'Grammar-based measures have been explored in related fields, such as MT BIBREF25 or grammatical error correction BIBREF26 , and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here – readability and grammaticality:']\n",
            "['$\\\\bullet $ Word-overlap Metrics (WOMs): We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .', '$\\\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet.', 'Grammar-based measures have been explored in related fields, such as MT BIBREF25 or grammatical error correction BIBREF26 , and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here – readability and grammaticality:']\n",
            "['$\\\\bullet $ SFHotel & SFRest BIBREF10 provide information about hotels and restaurants in San Francisco. There are 8 system dialogue act types, such as inform, confirm, goodbye etc. Each domain contains 12 attributes, where some are common to both domains, such as name, type, pricerange, address, area, etc., and the others are domain-specific, e.g. food and kids-allowed for restaurants; hasinternet and dogs-allowed for hotels. For each domain, around 5K human references were collected with 2.3K unique human utterances for SFHotel and 1.6K for SFRest. The number of unique system outputs produced is 1181 for SFRest and 875 for SFHotel.', '$\\\\bullet $ Bagel BIBREF17 provides information about restaurants in Cambridge. The dataset contains 202 aligned pairs of MRs and 2 corresponding references each. The domain is a subset of SFRest, including only the inform act and 8 attributes.']\n",
            "['We consider the following crowdsourced datasets, which target utterance generation for spoken dialogue systems. Table 1 shows the number of system outputs for each dataset. Each data instance consists of one MR and one or more natural language references as produced by humans, such as the following example, taken from the Bagel dataset:', '$\\\\bullet $ SFHotel & SFRest BIBREF10 provide information about hotels and restaurants in San Francisco. There are 8 system dialogue act types, such as inform, confirm, goodbye etc. Each domain contains 12 attributes, where some are common to both domains, such as name, type, pricerange, address, area, etc., and the others are domain-specific, e.g. food and kids-allowed for restaurants; hasinternet and dogs-allowed for hotels. For each domain, around 5K human references were collected with 2.3K unique human utterances for SFHotel and 1.6K for SFRest. The number of unique system outputs produced is 1181 for SFRest and 875 for SFHotel.', '$\\\\bullet $ Bagel BIBREF17 provides information about restaurants in Cambridge. The dataset contains 202 aligned pairs of MRs and 2 corresponding references each. The domain is a subset of SFRest, including only the inform act and 8 attributes.']\n",
            "['In this paper, we focus on recent end-to-end, data-driven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data ( BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 ; BIBREF13 , BIBREF13 ; BIBREF14 , BIBREF15 ). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as “ground truth\" or “targets\"), but are based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. BIBREF16 , and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems:', '$\\\\bullet $ rnnlg: The system by BIBREF10 uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far.', '$\\\\bullet $ TGen: The system by BIBREF9 learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module.', '$\\\\bullet $ lols: The system by BIBREF15 learns sentence planning and surface realisation using Locally Optimal Learning to Search (lols), an imitation learning framework which learns using bleu and rouge as non-decomposable loss functions.']\n",
            "['BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction.']\n",
            "['BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction.']\n",
            "['The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection BIBREF5 . The sentences are ranked by their cosine similarity to the centroid vector. This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 .']\n",
            "['The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection BIBREF5 . The sentences are ranked by their cosine similarity to the centroid vector. This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 .']\n",
            "['We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.']\n",
            "['We present three contributions in this paper. First, we provide a detailed investigation of a set of theoretically-grounded irony markers (e.g., tropes, morpho-syntactic, and typographic markers) in social media. We conduct the classification and frequency analysis based on their occurrence. Second, we analyze and compare the use of irony markers on two social media platforms ( INLINEFORM0 and INLINEFORM1 ). Third, we provide an analysis of markers on topically different social media content (e.g., technology vs. political subreddits).']\n",
            "['We first conduct a binary classification task to decide whether an utterance (e.g., a tweet or a INLINEFORM0 post) is ironic or non-ironic, exclusively based on the irony marker features. We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 . Table TABREF23 and Table TABREF24 present the results of the ablation tests for INLINEFORM1 and INLINEFORM2 . We report Precision ( INLINEFORM3 ), Recall ( INLINEFORM4 ) and INLINEFORM5 scores of both INLINEFORM6 and INLINEFORM7 categories.']\n",
            "['We first conduct a binary classification task to decide whether an utterance (e.g., a tweet or a INLINEFORM0 post) is ironic or non-ironic, exclusively based on the irony marker features. We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 . Table TABREF23 and Table TABREF24 present the results of the ablation tests for INLINEFORM1 and INLINEFORM2 . We report Precision ( INLINEFORM3 ), Recall ( INLINEFORM4 ) and INLINEFORM5 scores of both INLINEFORM6 and INLINEFORM7 categories.']\n",
            "['We first conduct a binary classification task to decide whether an utterance (e.g., a tweet or a INLINEFORM0 post) is ironic or non-ironic, exclusively based on the irony marker features. We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 . Table TABREF23 and Table TABREF24 present the results of the ablation tests for INLINEFORM1 and INLINEFORM2 . We report Precision ( INLINEFORM3 ), Recall ( INLINEFORM4 ) and INLINEFORM5 scores of both INLINEFORM6 and INLINEFORM7 categories.']\n",
            "['Twitter: We use a set of 350K tweets for our experiments. The ironic/sarcastic tweets are collected using hashtags, such as #irony, #sarcasm, and #sarcastic whereas the non-sarcastic tweets do not contain these hashtags, but they might include sentiment hashtags, such as #happy, #love, #sad, #hate (similar to BIBREF8 , BIBREF9 ). As pre-processing, we removed the retweets, spam, duplicates, and tweets written in languages other than English. Also, we deleted all tweets where the hashtags of interest were not located at the very end (i.e., we eliminated “#sarcasm is something that I love”). We lowercased the tweets, except the words where all the characters are uppercased.']\n",
            "['In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data.']\n",
            "['The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-of-speech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multi-layer bidirectional LSTM BIBREF4 , BIBREF5 with highway connections BIBREF6 .']\n",
            "['The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 .', 'In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.']\n",
            "['We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages—which we call polyglot training—to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages.', 'We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.', 'Our basic model adapts the span-based dependency SRL model of He2017-deepsrl. This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model— each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by marcheggiani2017lstm.', 'The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-of-speech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multi-layer bidirectional LSTM BIBREF4 , BIBREF5 with highway connections BIBREF6 .']\n",
            "['We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.']\n",
            "['However, tracking all previous utterances as the context is unwise. First, commercial chat-bots usually place high demands on efficiency. In a retrieval-based system, for example, performing a standard process of candidate retrieval and re-ranking for each previous utterance may well exceed the time limit (which is very short, e.g., 500ms). Second, we observe that not all sentences in the current conversation session are equally important. The sentence “Want to take a walk?” is irrelevant to the current context, and should not be considered when the computer synthesizes the reply. Therefore, it raises the question of session segmentation in conversation systems.']\n",
            "['To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain.', 'We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with “virtual sentences.” The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).']\n",
            "['To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain.']\n",
            "['Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one “domain document”, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) “domain document” to each test document.', 'FLOAT SELECTED: Table 3: Precision on the web snippet dataset']\n",
            "['The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:', '$$\\\\textsc {Rel}(q) = \\\\cos (v(q),v(D_{in}))$$ (Eq. 7)', 'where $v(\\\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:']\n",
            "['The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:', '$$\\\\textsc {Rel}(q) = \\\\cos (v(q),v(D_{in}))$$ (Eq. 7)', 'where $v(\\\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:']\n",
            "['We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best.', 'The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance.']\n",
            "['where $v(\\\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:', '$$\\\\textsc {AvgLM}(q) = \\\\frac{\\\\textsc {Lm}(q)}{\\\\textsc {Len}(q)}$$ (Eq. 8)', 'where $\\\\textsc {Lm}(\\\\cdot )$ is the general-domain language model score (log probability), and $\\\\textsc {Len}(\\\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.']\n",
            "['where $v(\\\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:', '$$\\\\textsc {AvgLM}(q) = \\\\frac{\\\\textsc {Lm}(q)}{\\\\textsc {Len}(q)}$$ (Eq. 8)', 'where $\\\\textsc {Lm}(\\\\cdot )$ is the general-domain language model score (log probability), and $\\\\textsc {Len}(\\\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.']\n",
            "['We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best.', 'FLOAT SELECTED: Table 2: Human ratings of generated questions', 'We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.']\n",
            "['Using our methodology, we are able to quantify gender bias with respect to how game-related interview questions are. We also provide a more fine-grained analysis of how gender differences in journalistic questioning are displayed under various scenarios. To help with further analysis of interview questions and answers, we introduce a dataset of tennis post-match interview transcripts along with corresponding match information.', 'We match interview transcripts with game statistics by date and player name, keeping only the question and answer pairs from games where the statistics are successfully merged. This gives us a dataset consisting of 6467 interview transcripts and a total of 81906 question snippets posed to 167 female players and 191 male players. To model tennis-game-specific language, we use live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/). These tend to be short, averaging around 40 words. Here is a sample, taken from the Federer-Murray match at the 2015 Wimbledon semi-final:']\n",
            "['In the remainder of the paper, we evaluate the empirical behavior of our approach to translation. Our evaluation considers two kinds of tasks: reference games and navigation games. In a reference game (e.g. fig:tasksa), both players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset BIBREF17 , BIBREF18 and the Caltech–UCSD Birds dataset BIBREF19 with accompanying natural language descriptions BIBREF20 . We use standard train / validation / test splits for both of these datasets.']\n",
            "['In the remainder of the paper, we evaluate the empirical behavior of our approach to translation. Our evaluation considers two kinds of tasks: reference games and navigation games. In a reference game (e.g. fig:tasksa), both players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset BIBREF17 , BIBREF18 and the Caltech–UCSD Birds dataset BIBREF19 with accompanying natural language descriptions BIBREF20 . We use standard train / validation / test splits for both of these datasets.', 'The final task we consider is the driving task (fig:tasksc) first discussed in the introduction. In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding. This task takes a number of steps to complete, and potentially involves a much broader range of communication strategies. To obtain human annotations for this task, we recorded both actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.']\n",
            "['The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset.']\n",
            "['The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset.']\n",
            "['Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\\\in \\\\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments.']\n",
            "['Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\\\in \\\\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments.']\n",
            "['Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\\\in \\\\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments.']\n",
            "['Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.']\n",
            "['Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.']\n",
            "['Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.']\n",
            "['The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually.']\n",
            "['The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005. Each speech segment can be referred to its debate, its speaker, the speaker’s party and the speaker’s vote which serves as the ground-truth label for the speech. The dataset was originally created in the course of the project Get out the vote UID34 . The authors used the dataset to train a classifier in order to determine whether a speech represents support of or opposition to proposed legislation. They did not only analyze the speeches individually but also investigated agreements and disagreements with the opinions of other speakers. That is, they identified references in the speech segments, determined the targets of those references, and decided whether a reference represents an instance of agreement or disagreement. However, we focus only on the individual speech segments and disregard references.']\n",
            "['The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005. Each speech segment can be referred to its debate, its speaker, the speaker’s party and the speaker’s vote which serves as the ground-truth label for the speech. The dataset was originally created in the course of the project Get out the vote UID34 . The authors used the dataset to train a classifier in order to determine whether a speech represents support of or opposition to proposed legislation. They did not only analyze the speeches individually but also investigated agreements and disagreements with the opinions of other speakers. That is, they identified references in the speech segments, determined the targets of those references, and decided whether a reference represents an instance of agreement or disagreement. However, we focus only on the individual speech segments and disregard references.']\n",
            "['For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores.', 'FLOAT SELECTED: Table 5: Evaluation criteria']\n",
            "['Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).']\n",
            "['General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark BIBREF10, a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (BIBREF11) encoder followed by two BiLSTMs.', 'Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).']\n",
            "['Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).']\n",
            "['Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus BIBREF9. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model BIBREF2 required 1 day of training on 1024 32GB V100.']\n",
            "['Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus BIBREF9. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model BIBREF2 required 1 day of training on 1024 32GB V100.']\n",
            "['Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training.', 'The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning.']\n",
            "['Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training.', 'The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning.']\n",
            "['Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training.', 'The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning.']\n",
            "['Before evaluating the language transfer result, we first experimented on the primary INLINEFORM0 model in the source language (English). The results are shown in Fig. FIGREF12 . Here we compare the representations of INLINEFORM1 and INLINEFORM2 . Furthermore, we examined the influence of the dimension of Audio Word2Vector in terms of MAP. We also compared the MAP results on large testing database (250K segments) and small database (20K).']\n",
            "['In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish. Among the four target languages, German has the highest lexical similarity with English (0.60) and the second highest is French (0.27), while for Czech and Spanish, the lexical similarity scores is 0 BIBREF48 .']\n",
            "['In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish. Among the four target languages, German has the highest lexical similarity with English (0.60) and the second highest is French (0.27), while for Czech and Spanish, the lexical similarity scores is 0 BIBREF48 .']\n",
            "['More specifically, splitting the document into chunks yields higher test accuracy than having the whole document as input. Our first model with the BiLSTM based framework and the linear classifier reaches a 97.97% accuracy with a 1.1% improvement upon the benchmark model. Similarly, the second model with the SVM classifier reaches a remarkable 98.11% accuracy with a 0.4% improvement upon the benchmark model.']\n",
            "['FLOAT SELECTED: Table 4: Performance of models of the first type (simple linear classifier) reported on validation and test set. Wc denotes the average words per chunk and best scores are shown in bold.', 'FLOAT SELECTED: Table 5: Performance of models of the second type (SVM classifier) reported on validation and test set. Wc denotes the average words per chunk and best scores are shown in bold.', 'More specifically, splitting the document into chunks yields higher test accuracy than having the whole document as input. Our first model with the BiLSTM based framework and the linear classifier reaches a 97.97% accuracy with a 1.1% improvement upon the benchmark model. Similarly, the second model with the SVM classifier reaches a remarkable 98.11% accuracy with a 0.4% improvement upon the benchmark model.']\n",
            "['We create different models with respect to the number of chunks that we divide the initial text into, in order to observe how the different number of chunks affect the efficiency of the final model. These chunks are then used to train Doc2Vec. In short, the intuition behind Doc2Vec is analogous to the intuition behind Word2Vec, where the']\n",
            "['The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages. We let it predict the sentiment labels instead of words.', 'Hierarchical attention networks BIBREF10 : It is the state-of-the-art RNN-based document classifier. Following their method, the documents were segmented into shorter sentences of 100 words, and hierarchically encoded with bi-directional RNNs.', 'FastText BIBREF11 : It is the state-of-the-art baseline for text classification, which simply takes n-gram features and classifies sentences by hierarchical softmax. We used the word embedding version but did not use the bigram version because the other models for comparison do not use bigram inputs.']\n",
            "['The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages. We let it predict the sentiment labels instead of words.', 'Hierarchical attention networks BIBREF10 : It is the state-of-the-art RNN-based document classifier. Following their method, the documents were segmented into shorter sentences of 100 words, and hierarchically encoded with bi-directional RNNs.', 'FastText BIBREF11 : It is the state-of-the-art baseline for text classification, which simply takes n-gram features and classifies sentences by hierarchical softmax. We used the word embedding version but did not use the bigram version because the other models for comparison do not use bigram inputs.']\n",
            "['The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages. We let it predict the sentiment labels instead of words.', 'Hierarchical attention networks BIBREF10 : It is the state-of-the-art RNN-based document classifier. Following their method, the documents were segmented into shorter sentences of 100 words, and hierarchically encoded with bi-directional RNNs.', 'FastText BIBREF11 : It is the state-of-the-art baseline for text classification, which simply takes n-gram features and classifies sentences by hierarchical softmax. We used the word embedding version but did not use the bigram version because the other models for comparison do not use bigram inputs.']\n",
            "[\"For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU. For WMT De-En, En-De and Zh-En we measure detokenized BLEU BIBREF14.\"]\n",
            "['For En-De, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer BIBREF3 as implemented in fairseq BIBREF17. For online decoding experiments, we do not share encoder and decoder embeddings since the source and target vocabularies were learned separately. We report average accuracy of three random initializations of a each configuration. We generally use $k_1=5$ and $k_2=10$. We tune $\\\\lambda _1$, and a length penalty on the validation set.', 'Next, we switch to n-best re-ranking where we have the full target sentence available compared to online decoding. Noisy channel model re-ranking has been used by the top ranked entries of the WMT 2019 news translation shared task for English-German, German-English, Englsh-Russian and Russian-English BIBREF19. We compare to various baselines including right-to-left sequence to sequence models which are a popular choice for re-ranking and regularly feature in successful WMT submissions BIBREF20, BIBREF21, BIBREF22.']\n",
            "['Ansari et al. BIBREF7 introduced an architecture for two code mix languages Hindi and Marathi. The architecture included language identification, feature generation and sentiment classification as major steps. Hindi and English WordNet’s and SWNs were used as there was no SWN for Marathi. The Marathi words were first translated into English and the sentiment score of the English words were found and assigned to the words. Also, classification algorithms like Random Forest, Naïve Bayes, Support Vector Machine (SVM) were used for finding the polarity in the final step. Slang identification and emoticons were also crucial steps in the study. Slang are a group of words which are used informally and in a particular language. Emoticons are the representation of different facial expressions. SVM performed the best among all the algorithms with accuracy of 90% and 70% for Marathi and Hindi language.', 'SWN is a most common lexicon-based approach used by the researchers. Haithem et al. BIBREF9 developed the Irish SWN whose accuracy was 6% greater than the accuracy obtained by transliteration of the Irish Tweets into English language. The lexicon was manually created. The accuracy difference between the systems was because of the translation carried out into the English language BIBREF9. Naidu et al. BIBREF10 carried out the SA on Telugu e-newspapers. Their system was divided in two steps. First step was subjectivity classification. Second step was sentiment classification. In the first step the sentences were divided as subjective and objective sentences. In the second step only, the subjective sentences were further classified as positive, negative and neutral. Both the steps were performed using the SWN which gave the accuracy of 74% and 81% BIBREF10.', 'Pandey et al. BIBREF12 defined a framework to carry out the SA task on the Hindi movie reviews. BIBREF12 observed that the lower accuracy was obtained by using SWN as a classification technique and hence suggested using synset replacement algorithm along with the SWN. Synset replacement algorithms groups the synonymous words having same concepts together. It helped in increasing the accuracy of the system because if the word was not present in the Hindi SWN then it found the closest word and assigned the score of that word BIBREF12. In the study, Bhargava et al. BIBREF13 completed the SA task on the FIRE 2015 dataset. The dataset consisted of code-mixed sentences in English along with 4 Indian languages (Hindi, Bengali, Tamil, Telugu). The architecture consisted of 2 main steps Language Identification and Sentiment Classification. Punctuations, hashtags were identified and handled by the CMU Ark tagger. Machine learning techniques like logistic regression and SVM were used for language identification. SWN’s of each language were used for sentiment classification. The results of the implemented system were compared with the previous language translation technique and 8% better precision was observed BIBREF13.', 'Kaur, Mangat and Krail BIBREF14 carried out their SA task on Hinglish language, which is code mix language highly popular in India. It is mainly used for the social media communication. The authors [10] had created a Hinglish corpus which contained movie reviews domain specific Hindi words. Stop-word removal, tokenization were the pre-processing techniques used in the system, along with TF-IDF as the vectorization technique. Classification algorithms like SVM and Naïve Bayes where used to carry out the classification task. As a future work, the authors in BIBREF14 are trying to find the best feature and classifier combination.', 'The study in BIBREF5 contained the speeches of different leaders who spoke about different domain topics like festivals, environment, society etc. The dataset was manually created. BIBREF15 performed SA on the Tibetan language and hence collected the data from the Tibetan micro-blog. In BIBREF6 112 Hindi text file pertaining to different domains have been collected for analysis. Authors in BIBREF18 have used the SAIL Dataset which consist of training and test data for three different languages. Approximately 1000 tweets for each language was present as a training data. BIBREF21 extracted the data from the YouTube comments. The data extracted was related to the cookery website from 2 channels. Total of 9800 comments were collected.', 'In the paper, Godino et al. BIBREF25 carried out SA on Spanish tweets using three different classifier models which are feature classifier, FastText classifier, BERT classifier. Feature classifier extracted the important features from the tweets such as the length of the tweets, number of hashtags etc. and applied these features to the traditional machine learning algorithms to carry out the sentiment classification. The traditional algorithms used where: Logistic Regression, Multinomial Naive Bayes, Decision Tree, Support Vector Machines, Random Forest, Extra Trees, AdaBoost and Gradient Boost. FastText Classifier was developed by Facebook AI research and it internally works on the neural network architecture. BERT Classifier was also applied on the tweets. The output of the three classifiers were combined using the averaging assembling. The model was evaluated using the F1 score. F1 score of 45% and 46% was obtained on the train and test data of the implemented model.', 'Joshi et al. BIBREF16 carried out SA on the Gujarati tweets. Stopword removal, stemming were the pre-processing techniques used in the implemented model. Feature extraction technique Parts of Speech (POS) tagging and the classification algorithm SVM was used in the system. SVM performed very well and gave the accuracy of 92%. Sharma et al. BIBREF17 tried to predict the Indian election results by extracting the Hindi tweets for political domain. The tweets were mainly for 5 major political parties. Three approaches where implemented to predict the winner in the election. First approach was dictionary based in which n-gram was used as a pre-processing technique and TF-IDF was used as a vectorization technique. SWN was used to classify the data and assign the polarity score to the words. Naïve Bayes algorithm and SVM were the remaining two approaches which were used. SVM and Naïve Bayes predicted party BJP (Bhartiya Janta Party) as the winner. SVM had the accuracy of 78.4% which was highest among the three implemented approaches.', 'Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.']\n",
            "['Comparison with a baseline - synthetic emails generated by Dada engine BIBREF6.']\n",
            "['Comparison with a baseline - synthetic emails generated by Dada engine BIBREF6.']\n",
            "['Comparison with a baseline - synthetic emails generated by Dada engine BIBREF6.']\n",
            "['In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:', '$ L(e;f,h) = \\\\lambda _\\\\text{emb}\\\\log q(f,e) + \\\\lambda _\\\\text{LM}\\\\log p(e|h) $', 'Here, $q(f,e)$ is a lexical score defined as:', '$ q(f,e) = \\\\frac{d(f,e) + 1}{2} $', 'where $d(f,e) \\\\in [-1,1]$ is a cosine similarity between $f$ and $e$ . It is transformed to the range $[0,1]$ to make it similar in scale with the LM probability. In our experiments, we found that this simple linear scaling is better than sigmoid or softmax functions in the final translation performance.', 'Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses.']\n",
            "['We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.']\n",
            "['We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.']\n",
            "['We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.']\n",
            "['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.']\n",
            "['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.']\n",
            "['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'Comparative Methods']\n",
            "['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .']\n",
            "['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .']\n",
            "['Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.']\n",
            "['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.']\n",
            "['There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.']\n",
            "['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.']\n",
            "['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.']\n",
            "[\"The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.\", 'Based on the same network, LGI continued to learn syntax ‘this is …’. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.']\n",
            "['Based on the same network, LGI continued to learn syntax ‘this is …’. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.', 'Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself ‘give me a 9’, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit ‘9’ instance was reconstructed via the imagination network. (3) LGI gave the command ‘rotate 180’, then the imagined digit ‘9’ was rotated upside down. (4) Following the language command ‘this is ’, LGI automatically predicted that the newly imaged object was the digit ‘6’. (5) LGI used ‘enlarge’ command to make the object bigger. (6) Finally, LGI predicted that the size was ‘big’ according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism. Figure 7.B shows the short cut imagination process, where LGI also regarded the rotated ‘9’ as digit 6, which suggests the AIT activation does not encode the digit identity, but the untangled features of input image or imagined image. Those high level cortices beyond visual cortex could be the place for identity representation.']\n",
            "['For human brain development, the visual and auditory systems mature in much earlier stages than the PFC [19]. To mimic this process, our PFC subsystem was trained separately after vision and language components had completed their functionalities. We have trained the network to accumulatively learn eight syntaxes, and the related results are shown in the following section. Finally, we demonstrate how the network forms a thinking loop with text language and imagined pictures.', 'Experiment', \"The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.\", 'Based on the same network, LGI continued to learn syntax ‘this is …’. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.', 'After that, LGI learned the syntax ‘the size is big/small’, followed by ‘the size is not small/big’. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of ‘the size is big/small’ (train steps =1000), the negative adverb ‘not’ in the language text ‘the size is not small/big’ was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being.', 'And then, LGI rapidly learned three more syntaxes: ‘give me a …’, ‘enlarge/shrink’, and ‘rotate …’, whose results are shown in Figure 6. After training (5000 steps), LGI could generate a correct digit figure given the language command ‘give me a [number]’ (Figure 6.A). The generated digit instance is somewhat the ‘averaged’ version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as ‘give me a red Arial big 9’ to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way.']\n",
            "[\"The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.\", 'Based on the same network, LGI continued to learn syntax ‘this is …’. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.', 'After that, LGI learned the syntax ‘the size is big/small’, followed by ‘the size is not small/big’. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of ‘the size is big/small’ (train steps =1000), the negative adverb ‘not’ in the language text ‘the size is not small/big’ was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being.', 'And then, LGI rapidly learned three more syntaxes: ‘give me a …’, ‘enlarge/shrink’, and ‘rotate …’, whose results are shown in Figure 6. After training (5000 steps), LGI could generate a correct digit figure given the language command ‘give me a [number]’ (Figure 6.A). The generated digit instance is somewhat the ‘averaged’ version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as ‘give me a red Arial big 9’ to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way.']\n",
            "[\"The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.\", 'Based on the same network, LGI continued to learn syntax ‘this is …’. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.', 'After that, LGI learned the syntax ‘the size is big/small’, followed by ‘the size is not small/big’. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of ‘the size is big/small’ (train steps =1000), the negative adverb ‘not’ in the language text ‘the size is not small/big’ was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being.', 'And then, LGI rapidly learned three more syntaxes: ‘give me a …’, ‘enlarge/shrink’, and ‘rotate …’, whose results are shown in Figure 6. After training (5000 steps), LGI could generate a correct digit figure given the language command ‘give me a [number]’ (Figure 6.A). The generated digit instance is somewhat the ‘averaged’ version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as ‘give me a red Arial big 9’ to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way.']\n",
            "['In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text.']\n",
            "['In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text.']\n",
            "['In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text.']\n",
            "[\"Modern autoencoder techniques could synthesize an unseen view for the desired viewpoint. Using car as an example [17], during training, the autoencoder learns the 3D characteristics of a car with a pair of images from two views of the same car together with the viewpoint of the output view. During testing, the autoencoder could predict the desired image from a single image of the car given the expected viewpoint. However, this architecture is task-specific, namely that the network can only make predictions on cars' unseen views. To include multiple tasks, we added an additional PFC layer that can receive task commands conveyed via language stream and object representation via the visual encoder pathway, and output the modulated images according to task commands and the desired text prediction associated with the images. In addition, by transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image.\"]\n",
            "['Human thinking is regarded as ‘mental ideas flow guided by language to achieve a goal’. For instance, after seeing heavy rain, you may say internally ‘holding an umbrella could avoid getting wet’, and then you will take an umbrella before leaving. In the process, we know that the visual input of ‘water drop’ is called rain, and can imagine ‘holding an umbrella’ could keep off the rain, and can even experience the feeling of being wet. This continual thinking capacity distinguishes us from the machine, even though the latter can also recognize images, process language, and sense rain-drops. Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario.']\n",
            "['We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted “typo-ness” score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes.']\n",
            "['As for the language model, we trained a character level Long Short Term Memory (LSTM) language model developed in BIBREF20 per language, which consists of a trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier. The LSTM hidden state and word embedding sizes are set to be 1000 and 200, respectively. We used 100,000 sentences from the W2C Web Corpus BIBREF21 for training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages.', 'We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\\\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets.']\n",
            "['As for the language model, we trained a character level Long Short Term Memory (LSTM) language model developed in BIBREF20 per language, which consists of a trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier. The LSTM hidden state and word embedding sizes are set to be 1000 and 200, respectively. We used 100,000 sentences from the W2C Web Corpus BIBREF21 for training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages.', 'We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted “typo-ness” score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes.']\n",
            "[\"Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset.\"]\n",
            "[\"Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset.\"]\n",
            "[\"Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset.\"]\n",
            "[\"Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:\", 'Has at least one pull request or pull request review comment event between November 2017 and September 2019,', 'Has 50 or more starts,', 'Has a size between 1MB and 1GB, and', 'Has a permissive license.', \"Note the “and” in the list above—a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition.\"]\n",
            "['The first step for collecting typos is to collect as many eligible GitHub repositories as possible from which commits and edits are extracted. A repository must meet some criteria in order to be included in the corpus, such as size (it needs to big enough to contain at least some amount of typo edits), license (it has to be distributed under a permissive license to allow derived work), and quality (it has to demonstrate some signs of quality, such as the number of stars).']\n",
            "[\"Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:\", 'Has at least one pull request or pull request review comment event between November 2017 and September 2019,', 'Has 50 or more starts,', 'Has a size between 1MB and 1GB, and', 'Has a permissive license.']\n",
            "['See Figure FIGREF27 for some examples of different edit types on each language. If one edit contains more than one type of changes, the least superficial category is assigned. For example, if there are both spell and grammatical changes in a single edit, the “grammatical” category is assigned to the edit. We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not. Thus, our goal is to identify the last type of edits as accurately as possible in a scalable manner. We will show the statistics of the annotated data in Section 6.']\n",
            "['Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics:', 'Ratio of the target perplexity over the source calculated by a language model', 'Normalized edit distance between the source and the target', 'Binary variable indicating whether the edit purely consists of changes in numbers']\n",
            "['Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics:', 'Ratio of the target perplexity over the source calculated by a language model', 'Normalized edit distance between the source and the target', 'Binary variable indicating whether the edit purely consists of changes in numbers']\n",
            "[\"As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset.\"]\n",
            "[\"Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.\"]\n",
            "['Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.']\n",
            "['Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.', \"As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset.\"]\n",
            "['Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.']\n",
            "['Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.']\n",
            "['Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.']\n",
            "['Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.']\n",
            "['We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings.']\n",
            "['We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.']\n",
            "['We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.']\n",
            "['We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.']\n",
            "['Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 . This corpus consists of twelve books (from four different publishers) whose usability and level have been confirmed by Swedish L2 teachers. The course books have been annotated both content-wise (e.g. exercises, lists) and linguistically (e.g. with POS and dependency tags) BIBREF19 . We collected a total of 867 texts (reading passages) from this corpus. We excluded texts that are primarily based on dialogues from the current experiments due to their specific linguistic structure, with the aim of scaling down differences connected to text genres rather than linguistic complexity. We plan to study the readability of dialogues and compare them to non-dialogue texts in the future.']\n",
            "['Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 . This corpus consists of twelve books (from four different publishers) whose usability and level have been confirmed by Swedish L2 teachers. The course books have been annotated both content-wise (e.g. exercises, lists) and linguistically (e.g. with POS and dependency tags) BIBREF19 . We collected a total of 867 texts (reading passages) from this corpus. We excluded texts that are primarily based on dialogues from the current experiments due to their specific linguistic structure, with the aim of scaling down differences connected to text genres rather than linguistic complexity. We plan to study the readability of dialogues and compare them to non-dialogue texts in the future.']\n",
            "['Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 . This corpus consists of twelve books (from four different publishers) whose usability and level have been confirmed by Swedish L2 teachers. The course books have been annotated both content-wise (e.g. exercises, lists) and linguistically (e.g. with POS and dependency tags) BIBREF19 . We collected a total of 867 texts (reading passages) from this corpus. We excluded texts that are primarily based on dialogues from the current experiments due to their specific linguistic structure, with the aim of scaling down differences connected to text genres rather than linguistic complexity. We plan to study the readability of dialogues and compare them to non-dialogue texts in the future.']\n",
            "['Most notably, we have found that taking into consideration multiple linguistic dimensions when assessing linguistic complexity is especially useful for sentence-level analysis. In our experiments, using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences, resulting in a 7% difference in accuracy. Besides L2 course book materials, we tested both our document- and sentence-level models also on unseen data with promising results.']\n",
            "['We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ).']\n",
            "['We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ).']\n",
            "['We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ).']\n",
            "['Although a direct comparison with other studies is difficult because of the target language, the nature of the datasets and the number of classes used, in terms of absolute numbers, our model achieves comparable performance with the state-of-the-art systems for English BIBREF9 , BIBREF12 . Other studies for non-English languages using CEFR levels include: BIBREF13 , reporting 49.1% accuracy for a French system distinguishing six classes; and BIBREF14 achieving 29.7% accuracy on a smaller Portuguese dataset with five levels.']\n",
            "['Although a direct comparison with other studies is difficult because of the target language, the nature of the datasets and the number of classes used, in terms of absolute numbers, our model achieves comparable performance with the state-of-the-art systems for English BIBREF9 , BIBREF12 . Other studies for non-English languages using CEFR levels include: BIBREF13 , reporting 49.1% accuracy for a French system distinguishing six classes; and BIBREF14 achieving 29.7% accuracy on a smaller Portuguese dataset with five levels.']\n",
            "['In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\\\sim $3.7M tweets. To represent the tweets in this baseline, we use the decoded embedding produced by the model and we feed them to the Logistic Regression classifier.']\n",
            "['In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\\\sim $3.7M tweets. To represent the tweets in this baseline, we use the decoded embedding produced by the model and we feed them to the Logistic Regression classifier.']\n",
            "['We report precision, recall and F1 score. Given the substantial class imbalance in the dataset, we use the macro weighted version of the F1 metric. We tested several classifiers and Logistic Regression showed the best F1$_{macro}$ value. We kept the default parameters values. We report results for 5-folds cross-validation.']\n",
            "['Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as “Pizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets\\' messages.', 'Previous works BIBREF7 have investigated IRA campaign efforts on Facebook, and they found that IRA pages have posted more than $\\\\sim $80K posts focused on division issues in US. Later on, the work in BIBREF2 has analyzed Facebook advertised posts by IRA and they specified the main themes that these advertisements discussed. Given the results of the previous works, we applied a topic modeling technique on our dataset to extract its main themes. We aim to detect IRA trolls by identifying their suspicious ideological changes across a set of themes.', 'Given our dataset, we applied Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8 on the tweets after a prepossessing step where we maintained only nouns and proper nouns. In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes. In addition, we found that the Crimes theme focuses more on raping crimes against children and women. Our resulted themes are generally consistent with the ones obtained from the Facebook advertised posts in BIBREF2, and this emphasizes that IRA efforts organized in a similar manner in both social media platforms.']\n",
            "['Given our dataset, we applied Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8 on the tweets after a prepossessing step where we maintained only nouns and proper nouns. In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes. In addition, we found that the Crimes theme focuses more on raping crimes against children and women. Our resulted themes are generally consistent with the ones obtained from the Facebook advertised posts in BIBREF2, and this emphasizes that IRA efforts organized in a similar manner in both social media platforms.']\n",
            "['Previous works BIBREF7 have investigated IRA campaign efforts on Facebook, and they found that IRA pages have posted more than $\\\\sim $80K posts focused on division issues in US. Later on, the work in BIBREF2 has analyzed Facebook advertised posts by IRA and they specified the main themes that these advertisements discussed. Given the results of the previous works, we applied a topic modeling technique on our dataset to extract its main themes. We aim to detect IRA trolls by identifying their suspicious ideological changes across a set of themes.', 'Given our dataset, we applied Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8 on the tweets after a prepossessing step where we maintained only nouns and proper nouns. In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes. In addition, we found that the Crimes theme focuses more on raping crimes against children and women. Our resulted themes are generally consistent with the ones obtained from the Facebook advertised posts in BIBREF2, and this emphasizes that IRA efforts organized in a similar manner in both social media platforms.']\n",
            "['Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as “Pizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets\\' messages.', 'As Twitter declared, although the IRA campaign was originated in Russia, it has been found that IRA trolls concealed their identity by tweeting in English. Furthermore, for any possibility of unmasking their identity, the majority of IRA trolls changed their location to other countries and the language of the Twitter interface they use. Thus, we propose the following features to identify these users using only their tweets text:', 'Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.', \"Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.\"]\n",
            "['Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.', \"Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.\", 'Textual Representation ::: Profiling IRA Accounts']\n",
            "['As Twitter declared, although the IRA campaign was originated in Russia, it has been found that IRA trolls concealed their identity by tweeting in English. Furthermore, for any possibility of unmasking their identity, the majority of IRA trolls changed their location to other countries and the language of the Twitter interface they use. Thus, we propose the following features to identify these users using only their tweets text:', 'Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.', \"Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.\"]\n",
            "['Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.', 'For the theme-based features, we use the following features that we believe that they change based on the themes:', \"Emotions: Since the results of the previous works BIBREF2, BIBREF7 showed that IRA efforts engineered to seed discord among individuals in US, we use emotions features to detect their emotional attempts to manipulate the public opinions (e.g. fear spreading behavior). For that, we use the NRC emotions lexicon BIBREF9 that contains $\\\\sim $14K words labeled using the eight Plutchik's emotions.\", 'Sentiment: We extract the sentiment of the tweets from NRC BIBREF9, positive and negative.', 'Bad & Sexual Cues: During the manual analysis of a sample from IRA tweets, we found that some users use bad slang word to mimic the language of a US citizen. Thus, we model the presence of such words using a list of bad and sexual words from BIBREF10.', 'Stance Cues: Stance detection has been studied in different contexts to detect the stance of a tweet reply with respect to a main tweet/thread BIBREF11. Using this feature, we aim to detect the stance of the users regarding the different topics we extracted. To model the stance we use a set of stance lexicons employed in previous works BIBREF12, BIBREF13. Concretely, we focus on the following categories: belief, denial, doubt, fake, knowledge, negation, question, and report.', 'Bias Cues: We rely on a set of lexicons to capture the bias in text. We model the presence of the words in one of the following cues categories: assertives verbs BIBREF14, bias BIBREF15, factive verbs BIBREF16, implicative verbs BIBREF17, hedges BIBREF18, report verbs BIBREF15. A previous work has used these bias cues to identify bias in suspicious news posts in Twitter BIBREF19.', 'LIWC: We use a set of linguistic categories from the LIWC linguistic dictionary BIBREF20. The used categories are: pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl.', 'Morality: Cues based on the morality foundation theory BIBREF21 where words labeled in one of a set of categories: care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation.']\n",
            "['Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.']\n",
            "['To compare embedding methods in academic paper clustering, we calculate F1, V-measure (a conditional entropy-based clustering measure BIBREF11 ), and ARI (Adjusted Rand index BIBREF12 ). As shown in Table TABREF18 , similarly to document retrieval, Paragraph Vector performed better than word2vec averagings in clustering documents, while our KeyVec consistently performed the best among all the compared methods.']\n",
            "['To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.']\n",
            "['To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.']\n",
            "['To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.', 'Document Retrieval']\n",
            "['To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.']\n",
            "['We evaluate our KeyVec on two text understanding tasks: document retrieval and document clustering. As shown in the experimental section SECREF5 , KeyVec yields generic document representations that perform better than state-of-the-art embedding models.']\n",
            "['To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.']\n",
            "['The goal of the document retrieval task is to decide if a document should be retrieved given a query. In the experiments, our document pool contained 669 academic papers published by IEEE, from which top- INLINEFORM0 relevant papers are retrieved. We created 70 search queries, each composed of the text in a Wikipedia page on a field of study (e.g., https://en.wikipedia.org/wiki/Deep_learning). We retrieved relevant papers based on cosine similarity between document embeddings of 100 dimensions for Wikipedia pages and academic papers. For each query, a good document-embedding model should lead to a list of academic papers in one of the 70 fields of study.', 'In the document clustering task, we aim to cluster the academic papers by the venues in which they are published. There are a total of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation. Each academic paper is represented as a vector of 100 dimensions.']\n",
            "['The goal of the document retrieval task is to decide if a document should be retrieved given a query. In the experiments, our document pool contained 669 academic papers published by IEEE, from which top- INLINEFORM0 relevant papers are retrieved. We created 70 search queries, each composed of the text in a Wikipedia page on a field of study (e.g., https://en.wikipedia.org/wiki/Deep_learning). We retrieved relevant papers based on cosine similarity between document embeddings of 100 dimensions for Wikipedia pages and academic papers. For each query, a good document-embedding model should lead to a list of academic papers in one of the 70 fields of study.']\n",
            "['When the programme started, there were a few available tools for Icelandic. IceNLP BIBREF27 is a suite of NLP tools containing modules for tokenisation, PoS-tagging, lemmatising, parsing and named entity recognition. Greynir BIBREF22 is a full parser which also includes a tokeniser and recognises some types of named entities. Nefnir BIBREF28 is a lemmatiser which uses suffix substitution rules, derived from the Database of Icelandic Morphology BIBREF29, giving results that outperform IceNLP. ABLTagger BIBREF30 is a PoS tagger that outperforms other taggers that have been trained for tagging Icelandic texts.', \"Previously, two tokenisers have been built for Icelandic, one is a part of IceNLP and the other a part of Greynir. As Greynir is still in active development, it will be used as a base for the LT project's development. In order to be able to test the tokenisers' accuracy, a test set that takes different tokeniser settings into account will be developed.\", 'The IceNLP and Greynir parsers will be evaluated and either one of them or both developed further. We will also adapt a UD-parser to Icelandic UD-grammar.', 'Lexicon acquisition tool. When constructing and maintaining lexical databases, such as DIM, the Icelandic wordnet or other related resources, it is vital to be able to systematically add neologies and words that are missing from the datasets, especially those commonly used in the language. Within the LT programme a flexible lexicon acquisition tool will be developed. It will be able to identify and collect unknown words and word forms, together with statistics, through structured lexical acquisition from the Icelandic Gigaword Corpus, which is constantly being updated, and other data sources in the same format.', 'Software implementation and research. The research areas are chosen so to enhance the language resource development for Icelandic. A punctuation system for Icelandic will be analysed and implemented. Compound words are common in Icelandic and the language also has a relatively rich inflection structure so it is important to address those features for language modeling. Pronunciation analysis, speaker diarization and speech analysis will also be addressed especially for Icelandic, and acoustic modelling for children and teenagers receive attention in the project.', 'Software development. The software development tasks of the spell and grammar checking project will build a working open source correction system whose development is informed by the analysis of the data sets created within the project. The spell and grammar checker will be based on the foundation for processing Icelandic text provided by the Greynir system.', 'Baseline system. In this part, three baseline MT systems will be developed. First, a statistical phrase-based MT system based on Moses BIBREF41, second, a bidirectional LSTM model using the neural translation system OpenNMT BIBREF42, and, third, a system based on an attention-based neural network BIBREF43 using Tensor2Tensor. All the three systems will be trained on ParIce, and the additional data from tasks 1 and 2 above. Eventually, the goal is to choose the best performing MT-system for further development of MT for Icelandic.', 'MT interface. An API and a web user interface for the three baseline systems, mentioned in item 3 above, will be developed to give interested parties access to the systems under development, and to establish a testing environment in which members of the public can submit their own text. Thus, results from the three systems can be compared directly, as well as to the translations produced by Google Translate. Moreover, in this part, a crowd-sourcing mechanism will be developed, i.e. a functionality to allow users to submit improved translations back to the system for inclusion in the training corpus.']\n",
            "['In recent years, there has been much international discussion on how the future of languages depends on them being usable in the digital world. This concern has led to a number of national LT programmes. We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.']\n",
            "['In recent years, there has been much international discussion on how the future of languages depends on them being usable in the digital world. This concern has led to a number of national LT programmes. We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.']\n",
            "['In recent years, there has been much international discussion on how the future of languages depends on them being usable in the digital world. This concern has led to a number of national LT programmes. We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.']\n",
            "['The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8.']\n",
            "['The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8.']\n",
            "[\"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "[\"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "[\"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "['For the Rhetorical Parsing part of our experiment, we used a special library implemented for such purposes BIBREF18 . As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 . The BoW vectorization method built a vocabulary that considers the top 50,000 terms only ordered by their frequency across the corpus, similarly to supervised learning examples presented in our previous works in BIBREF8 . We used a noun and noun phrases extractor according to part-of-speech tagger from the Spacy Python library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree.', \"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "[\"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "['For the Rhetorical Parsing part of our experiment, we used a special library implemented for such purposes BIBREF18 . As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 . The BoW vectorization method built a vocabulary that considers the top 50,000 terms only ordered by their frequency across the corpus, similarly to supervised learning examples presented in our previous works in BIBREF8 . We used a noun and noun phrases extractor according to part-of-speech tagger from the Spacy Python library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree.', \"We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\"]\n",
            "['Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.']\n",
            "['Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.', 'Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.', 'Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.']\n",
            "['We consider three datasets, two of which are a contribution of this work.', 'Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.', 'Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.']\n",
            "['Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.', 'Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.']\n",
            "['Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.', 'Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.']\n",
            "['Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.', 'Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.', 'Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.']\n",
            "[\"Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$.\", 'Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data – the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.', 'Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores.']\n",
            "[\"Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$.\", 'Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data – the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.', 'Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores.']\n",
            "['Here, we describe the experimental method used to validate the effectiveness of the NJM. We compare the proposed method with two other methods of generating funny captions: 1) human generated captions, which are highly ranked on Bokete (indicated by “Human\" in Table TABREF10 ), and 2) Japanese image caption generation using CNN+LSTM pre-trained by STAIR caption BIBREF7 . Based on the captions provided by MS COCO, the STAIR caption is translated from English to Japanese (indicated by “STAIR caption” in Table TABREF10 ). We use a questionnaire as the evaluation method. We selected a total of 30 themes from the Bokete Ogiri website that included “people”, “two or more people”, “animals”, “landscape”, “inorganics”, and “illustrations”. The questionnaire asks respondents to rank the captions provided by humans, the NJM, and STAIR caption in order of “funniness”. The questionnaire does not reveal the origins of the captions.']\n",
            "['We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.']\n",
            "['We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.']\n",
            "['We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .']\n",
            "['We downloaded this survey data and hand crafted a total of 293 textual questions BIBREF13 which could answer the survey data. A set of 6 people (L2 English) generated 50 queries each with the only constraint that these queries should be able to answer the survey data. In all a set of 300 queries were crafted of which duplicate queries were removed to leave 293 queries in all. Of these, we chose 250 queries randomly and distributed among 5 Indian speakers, who were asked to read aloud the queries into a custom-built audio data collecting application. So, in all we had access to 250 audio queries spoken by 5 different Indian speakers; each speaking 50 queries.']\n",
            "['We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .']\n",
            "['We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .']\n",
            "['Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets). We show that training on more languages consistently improves performance on word discrimination, and that the improvement is not simply due to more training data: an equivalent amount of data from one language fails to give the same benefit. In fact, we observe the largest gain in performance when adding the second training language, which is already better than adding three times as much data from the same language. Moreover, when compared to our best results from training unsupervised on target language data only, we find that bnfs trained on just a single other language already outperform the target-language-only training, with multilingual bnfs doing better by a wide margin.']\n",
            "['We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.']\n",
            "[\"For multilingual training, we closely follow the existing Kaldi recipe for the Babel corpus. We train a tdnn BIBREF36 with block softmax BIBREF37 , i.e. all hidden layers are shared between languages, but there is a separate output layer for each language. For each training instance only the error at the corresponding language's output layer is used to update the weights. This architecture is illustrated in Figure FIGREF17 . The tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer with ReLU activations and batch normalization. Each language then has its own 625-dimensional affine and a softmax layer. The inputs to the network are 40-dimensional MFCCs with all cepstral coefficients to which we append i-vectors for speaker adaptation. The network is trained with stochastic gradient descent for 2 epochs with an initial learning rate of INLINEFORM0 and a final learning rate of INLINEFORM1 .\"]\n",
            "['In this work we use the cae in our experiments on unsupervised representation learning, since it performed well in the 2015 ZRSC, achieved some of the best-reported results on the same-different task (which we also consider), and has readily available code. As noted above, the cae attempts to normalize out non-linguistic factors such as speaker, channel, gender, etc., by using top-down information from pairs of similar speech segments. Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase. Each pair is then aligned at the frame level using dtw, and pairs of aligned frames are presented as the input INLINEFORM0 and target output INLINEFORM1 of a dnn. After training, a middle layer INLINEFORM2 is used as the learned feature representation.', 'FLOAT SELECTED: Fig. 1. Correspondence autoencoder training procedure (see section II-A).']\n",
            "['We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.']\n",
            "['We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.', 'We picked another 10 languages (different from the target languages described in Section SECREF7 ) with a combined 198.3 hours of speech from the GlobalPhone corpus. We consider these as high-resource languages, for which transcriptions are available to train a supervised asr system. The languages and dataset sizes are listed in Table TABREF16 . We also use the English wsj corpus BIBREF35 which is comparable to the GlobalPhone corpus. It contains a total of 81 hours of speech, which we either use in its entirety or from which we use a 15 hour subset; this allows us to compare the effect of increasing the amount of data for one language with training on similar amounts of data but from different languages.', 'In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.']\n",
            "['The results above were presented as part of an earlier conference version of this paper BIBREF3 . Here, we expand upon that work in several ways. First, we include new results on the corpora and evaluation measures used in the zrsc, to allow more direct comparisons with other work. In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures. This permits the two measures themselves to be better compared. Finally, we provide both a qualitative analysis of the differences between the different features we extract, and a quantitative evaluation on the downstream target-language task of unsupervised full-coverage speech segmentation and clustering using the system of BIBREF4 . This is the first time that multilingual features are used in such a system, which performs a complete segmentation of input speech into hypothesized words. As in our intrinsic evaluations, we find that the multilingual bnfs consistently outperform the best unsupervised cae features, which in turn outperform or do similarly to MFCCs.', 'In this work we use the cae in our experiments on unsupervised representation learning, since it performed well in the 2015 ZRSC, achieved some of the best-reported results on the same-different task (which we also consider), and has readily available code. As noted above, the cae attempts to normalize out non-linguistic factors such as speaker, channel, gender, etc., by using top-down information from pairs of similar speech segments. Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase. Each pair is then aligned at the frame level using dtw, and pairs of aligned frames are presented as the input INLINEFORM0 and target output INLINEFORM1 of a dnn. After training, a middle layer INLINEFORM2 is used as the learned feature representation.']\n",
            "['The results above were presented as part of an earlier conference version of this paper BIBREF3 . Here, we expand upon that work in several ways. First, we include new results on the corpora and evaluation measures used in the zrsc, to allow more direct comparisons with other work. In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures. This permits the two measures themselves to be better compared. Finally, we provide both a qualitative analysis of the differences between the different features we extract, and a quantitative evaluation on the downstream target-language task of unsupervised full-coverage speech segmentation and clustering using the system of BIBREF4 . This is the first time that multilingual features are used in such a system, which performs a complete segmentation of input speech into hypothesized words. As in our intrinsic evaluations, we find that the multilingual bnfs consistently outperform the best unsupervised cae features, which in turn outperform or do similarly to MFCCs.']\n",
            "['All experiments in this section are evaluated using the same-different task BIBREF26 , which tests whether a given speech representation can correctly classify two speech segments as having the same word type or not. For each word pair in a pre-defined set INLINEFORM0 the dtw cost between the acoustic feature vectors under a given representation is computed. Two segments are then considered a match if the cost is below a threshold. Precision and recall at a given threshold INLINEFORM1 are defined as INLINEFORM2', 'where INLINEFORM0 is the number of sw, swdp or all discovered matches at that threshold and INLINEFORM1 is the number of actual swdp pairs in INLINEFORM2 . We can compute a precision-recall curve by varying INLINEFORM3 . The final evaluation metric is the ap or the area under that curve. We generate evaluation sets of word pairs for the GlobalPhone development and test sets from all words that are at least 5 characters and 0.5 seconds long, except that we now also include different-word pairs.']\n",
            "['Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. We see much-improved model performances when trained and tested on in-domain Multi-News data. The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU. Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model). Our PG-MMR results correspond to PG-MMR w Cosine reported in lebanoff18mds. We trained their sentence regression model on Multi-News data and leave the investigation of transferring regression models from SDS to Multi-News for future work.']\n",
            "['Our dataset, which we call Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. We will release stable Wayback-archived links, and scripts to reproduce the dataset from these links. Our dataset is notably the first large-scale dataset for MDS on news articles. Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources). A total of 20 editors contribute to 85% of the total summaries on newser.com. Thus we believe that this dataset allows for the summarization of diverse source documents and summaries.']\n",
            "['However, data sparsity has largely been the bottleneck of the development of neural MDS systems. The creation of large-scale multi-document summarization dataset for training has been restricted due to the sparsity and cost of human-written summaries. liu18wikisum trains abstractive sequence-to-sequence models on a large corpus of Wikipedia text with citations and search engine results as input documents. However, no analogous dataset exists in the news domain. To bridge the gap, we introduce Multi-News, the first large-scale MDS news dataset, which contains 56,216 articles-summary pairs. We also propose a hierarchical model for neural abstractive multi-document summarization, which consists of a pointer-generator network BIBREF1 and an additional Maximal Marginal Relevance (MMR) BIBREF14 module that calculates sentence ranking scores based on relevancy and redundancy. We integrate sentence-level MMR scores into the pointer-generator model to adapt the attention weights on a word-level. Our model performs competitively on both our Multi-News dataset and the DUC 2004 dataset on ROUGE scores. We additionally perform human evaluation on several system outputs.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.']\n",
            "['Table TABREF14 shows overall results on the two datasets spanning broad domains of newswires, broadcast, telephone, and social media. The models proposed in this paper significantly surpassed previous comparable models by 1.4% on OntoNotes and 4.6% on WNUT. Compared to the re-implemented Baseline-BiLSTM-CNN, the cross-structures brought 0.7% and 2.2% improvements on OntoNotes and WNUT. More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms.']\n",
            "['Table TABREF16 shows significant results per entity type compared to Baseline ($>$3% absolute F1 differences for either Cross or Att). It could be seen that harder entity types generally benefitted more from the cross-structures. For example, work-of-art/creative-work entities could in principle take any surface forms – unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media. Such mentions require models to learn a deep, generalized understanding of their context to accurately identify their boundaries and disambiguate their types. Both cross-structures were more capable in dealing with such hard entities (2.1%/5.6%/3.2%/2.0%) than the prevalently used, problematic Baseline.', 'Moreover, disambiguating fine-grained entity types is also a challenging task. For example, entities of language and NORP often take the same surface forms. Figure FIGREF19 shows an example containing \"Dutch\" and \"English\". While \"English\" was much more frequently used as a language and was identified correctly, the \"Dutch\" mention was tricky for Baseline. The attention heat map (Figure FIGREF24) further tells the story that Att has relied on its attention head to make context-aware decisions. Overall, both cross-structures were much better at disambiguating these fine-grained types (4.1%/0.8%/3.3%/3.4%).']\n",
            "['This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models cross-context in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism BIBREF5, BIBREF6.']\n",
            "['Many have attempted tackling the NER task with LSTM-based sequence encoders BIBREF7, BIBREF0, BIBREF1, BIBREF8. Among these, the most sophisticated, state-of-the-art is the BiLSTM-CNN proposed by BIBREF1. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. This paper largely follows their work in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, the baseline model does perform better than their formulation.']\n",
            "['Many have attempted tackling the NER task with LSTM-based sequence encoders BIBREF7, BIBREF0, BIBREF1, BIBREF8. Among these, the most sophisticated, state-of-the-art is the BiLSTM-CNN proposed by BIBREF1. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. This paper largely follows their work in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, the baseline model does perform better than their formulation.']\n",
            "['We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo). Specifically, we use AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets. Since classes for Yelp and Amazon datasets have similar semantics (product ratings), we merge the classes for these two datasets. In total, we have 33 classes in our experiments. These datasets have varying sizes. For example, AGNews is ten times smaller than Yahoo. We create a balanced version all datasets used in our experiments by randomly sampling 115,000 training examples and 7,600 test examples from all datasets (i.e., the size of the smallest training and test sets). We leave investigations of lifelong learning in unbalanced datasets to future work. In total, we have 575,000 training examples and 38,000 test examples.']\n",
            "['We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo). Specifically, we use AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets. Since classes for Yelp and Amazon datasets have similar semantics (product ratings), we merge the classes for these two datasets. In total, we have 33 classes in our experiments. These datasets have varying sizes. For example, AGNews is ten times smaller than Yahoo. We create a balanced version all datasets used in our experiments by randomly sampling 115,000 training examples and 7,600 test examples from all datasets (i.e., the size of the smallest training and test sets). We leave investigations of lifelong learning in unbalanced datasets to future work. In total, we have 575,000 training examples and 38,000 test examples.']\n",
            "['We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo). Specifically, we use AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets. Since classes for Yelp and Amazon datasets have similar semantics (product ratings), we merge the classes for these two datasets. In total, we have 33 classes in our experiments. These datasets have varying sizes. For example, AGNews is ten times smaller than Yahoo. We create a balanced version all datasets used in our experiments by randomly sampling 115,000 training examples and 7,600 test examples from all datasets (i.e., the size of the smallest training and test sets). We leave investigations of lifelong learning in unbalanced datasets to future work. In total, we have 575,000 training examples and 38,000 test examples.']\n",
            "['Our model is augmented with an episodic memory module that stores previously seen examples throughout its lifetime. The episodic memory module is used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer. We first describe the architecture of our episodic memory module, before discussing how it is used at training and inference (prediction) time in § SECREF3 .']\n",
            "['A bidirectional RNN with GRU BIBREF19 cells are used in this model where the number of cells is a hyperparameter. Among the tested range (50-150 with intervals of 25), best accuracy on validation set is obtained by 150 cells in English and 100 cells in Spanish and Arabic. An attention mechanism is used on word-level in addition to tweet-level to capture the important parts of each tweet as shown in Figure FIGREF2.']\n",
            "['In order to support this mechanism, we use a two-layered cached FST for decoding. The first layer is public cache which represents $T_P$. It is a static cache created by pre-initialization. The second layer is the private cache, which is owned by a particular user and constructed on-the-fly. Figure FIGREF9 shows the architecture of our two-layer FST. The solid box denotes the static graph and the dashed ones show the dynamic graph. Personalized states will appear only in $T_D$.', 'The static public cache stores the most frequent states, which greatly reduces the run time factor (RTF) of online decoding. Since $T_D$ has a smaller size than a fully dynamic graph, the marginal memory efficiency for multi-threaded service will be better.', 'Furthermore, the private cache will not be freed after decoding a single utterance. The lifetime of a private cache actually can last for the entire dialog section for a specific user. The private cache keeps updating during the dialog session, making processing the subsequent utterances faster as more states are composed and stored in $T_D$. With this accumulated dynamic cache, a longer dialog can expect a better RTF in theory. In general, the static public cache serves all threads, while the private cache boosts the performance within a dialog session. The private cache will be freed at the end of the dialog.']\n",
            "[\"Speech input is now a common feature for smart devices. In many cases, the user's query involves entities such as a name from a contact list, a location, or a music title. Recognizing entities is particularly challenging for speech recognition because many entities are infrequent or out of the main vocabulary of the system. One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities. Because each user can have their own personalized LM, it is vital that the speech decoder be able to efficiently load the model on the fly, so it can be used in decoding, without any noticeable increase in latency.\", 'Many state-of-the-art speech recognition decoders are based on the weighted finite state transducer (WFST) paradigm BIBREF0, BIBREF1. A conventional WFST decoder searches a statically composed $H C L G$ graph, where $H$ is the graph that translates HMM states to CD phones, $C$ translates CD phones to graphemes, $L$ translates graphemes to words and $G$ is graph that represents the language model. Using a statically composed graph has two limitations. First, it is both compute and memory intensive when the vocabulary and LM are large. Second, the static graph approach makes it hard to handle personalized language models BIBREF2. Many common tasks a user may want to perform with a voice assistant such as making phone calls, messaging to a specific contact or playing favorite music require a personalized language model. A dynamic WFST decoder is better suited for such cases. As denoted in Eq (DISPLAY_FORM1), in a dynamic WFST decoder, $HCL$ is composed and optimized offline, while $G$ is composed on the fly with lazy (on-demand) composition, denoted as $\\\\circ $.']\n",
            "['We conduct a qualitative analysis of system errors and the cases where author profiling leads to the correct classification of previously misclassified examples. Table 3 shows examples of hateful tweets from the dataset that are misclassified by the lr method, but are correctly classified upon the addition of author profiling features, i.e., by the lr + auth method. It is worth noting that some of the wins scored by the latter are on tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues that are indicative of hate speech per se. The addition of author profiling features may then be viewed as a proxy for wider discourse information, thus allowing us to correctly resolve the cases where lexical and semantic features alone are insufficient.']\n",
            "[\"While the attributes vector INLINEFORM0 can be used to encode any information of interest about a user, we choose to experiment with the use of personal values because of their theoretical connection to human activities BIBREF6 . In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value. Since users' profiles often contain value-related content, we use the Distributed Dictionary Representations (DDR) method BIBREF25 to compute a score, INLINEFORM1 for each value dimension, INLINEFORM2 , using cosine similarity as follows: INLINEFORM3\"]\n",
            "['The paper makes the following main contributions. First, starting with a set of nearly 30,000 human activity patterns, we compile a very large dataset of more than 200,000 users undertaking one of the human activities matching these patterns, along with over 500 million total tweets from these users. Second, we use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and create a set of activity clusters of variable granularity. Third, we explore a neural model that can predict human activities based on natural language data, and in the process also investigate the relationships between everyday human activities and other social variables such as personal values.']\n",
            "['Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .']\n",
            "['Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .']\n",
            "['Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .']\n",
            "['Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .']\n",
            "['This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.']\n",
            "[\"Essentially we ask the following question: “What are the core properties encoded in the given tweet representation?”. We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al. [5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders. We differ from their work in two ways: (1) While they focus on sentences, we focus on social media posts which opens up the challenge of considering multiple salient properties such as hashtags, named entities, conversations and so on. (2) While they work with only unsupervised representation-learning models, we investigate the traditional unsupervised methods (BOW, LDA), unsupervised representation learning methods (Siamese CBOW, Tweet2Vec), as well as supervised methods (CNN, BLSTM).\", \"Bag Of Words (BOW) [17] - This simple representation captures the TF-IDF value of an n-gram. We pick top 50K n-grams, with the value of `n' going up to 5.\", 'Latent Dirichlet Allocation (LDA) [18] - We use the topic distribution resulting by running LDA with number of topics as 200, as tweet representation.', 'Bag Of Means (BOM) - We take the average of the word embeddings obtained by running the Glove [12] model on 2 billion tweets with embedding size as 200.', 'Deep Structured Semantic Models (DSSM) [9] - This is a deep encoder trained to represent query and document in common space, for document ranking. We use the publicly available pre-trained encoder to encode the tweets.', 'Convolutional DSSM (CDSSM) [10] - This is the convolutional variant of DSSM.', 'Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it. We use the BOW variant with embedding size and window size of 200 and 10 respectively.', 'Skip-Thought Vectors (STV) [6] - This is a GRU [16] encoder trained to predict adjacent sentences in a books corpus. We use the recommended combine-skip (4800-dimensional) vectors from the publicly available encoder.', 'Tweet2Vec (T2V) [3] - This is a character composition model working directly on the character sequences to predict the user-annotated hashtags in a tweet. We use publicly available encoder, which was trained on 2 million tweets.', 'Siamese CBOW (SCBOW) [2] - This model uses averaging of word vectors to represent a sentence, and the objective and data used here is the same as that for STV. Note that this is different from BOW because the word vectors here are optimized for sentence representation.', 'Convolutional Neural Network (CNN) - This is a simple CNN proposed in [7].', 'Long Short Term Memory Network (LSTM) [14] - This is a vanilla LSTM based recurrent model, applied from start to the end of a tweet, and the last hidden vector is used as tweet representation.', 'Bi-directional LSTM (BLSTM) [14] - This extends LSTM by using two LSTM networks, processing a tweet left-to-right and right-to-left respectively. Tweet is represented by concatenating the last hidden vector of both the LSTMs.', 'FastText (FT) [8] - This is a simple architecture which averages the n-gram vectors to represent a tweet, followed by the softmax in the final layer. This simple model has been shown to be effective for the text classification task.', 'This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.']\n",
            "[\"Essentially we ask the following question: “What are the core properties encoded in the given tweet representation?”. We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al. [5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders. We differ from their work in two ways: (1) While they focus on sentences, we focus on social media posts which opens up the challenge of considering multiple salient properties such as hashtags, named entities, conversations and so on. (2) While they work with only unsupervised representation-learning models, we investigate the traditional unsupervised methods (BOW, LDA), unsupervised representation learning methods (Siamese CBOW, Tweet2Vec), as well as supervised methods (CNN, BLSTM).\"]\n",
            "['Alternatively, we can adopt a two-tiered model, familiar from phrase-based translation BIBREF4 , first training INLINEFORM0 and then training INLINEFORM1 while keeping the parameters of INLINEFORM2 fixed, possibly on a smaller dataset. A variety of methods, like minimum error rate training BIBREF14 , BIBREF5 , are possible, but keeping with the globally-normalized negative log-likelihood, we obtain, for the constant word reward, the gradient: INLINEFORM3', 'where INLINEFORM0 is the 1-best translation. Then the stochastic gradient descent update is just the familiar perceptron rule: INLINEFORM1', 'although below, we update on a batch of sentences rather than a single sentence. Since there is only one parameter to train, we can train it on a relatively small dataset.']\n",
            "[\"The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines. Across all language pairs, reward and norm have close METEOR scores, though the reward method wins out slightly. BLEU scores for reward and norm also increase over the baseline in most cases, despite BLEU's inherent bias towards shorter sentences. Most notably, whereas the baseline Russian–English system lost more than 20 BLEU points when the beam was increased to 1000, our tuned reward score resulted in a BLEU gain over any baseline beam size. Whereas in our baseline systems, the length ratio decreases with larger beam sizes, our tuned word reward results in length ratios of nearly 1.0 across all language pairs, mitigating many of the issues of the brevity problem.\"]\n",
            "['Length normalization divides the score by INLINEFORM0 BIBREF0 , BIBREF1 , BIBREF2 : INLINEFORM1', \"Google's NMT system BIBREF3 relies on a more complicated correction: INLINEFORM0\", 'Finally, some systems add a constant word reward BIBREF5 : INLINEFORM0']\n",
            "['We note that the beam problem in NMT exists for relatively small beam sizes – especially when compared to traditional beam sizes in SMT systems. On our medium-resource Russian–English system, we investigate the full impact of this problem using a much larger beam size of 1000. In Table TABREF10 , we can see that the beam problem is particularly pronounced. The first row of the table shows the uncorrected, baseline score. From a beam of 10 to a beam of 1000, the drop in BLEU scores is over 20 points. This is largely due to the brevity problem discussed earlier. The second row of the table shows the length of the translated outputs compared to the lengths of the correct translations. Though the problem persists even at a beam size of 10, at a beam size of 1000, our baseline system generates less than one third the number of words that are in the correct translations. Furthermore, 37.3% of our translated outputs have sentences of length 0. In other words, the most likely translation is to immediately generate the stop symbol. This is the problem visualized in Figure FIGREF4 .']\n",
            "[\"As in our label-bias example, greedy search would prune the incorrect empty translation. More generally, consider beam search: at time step INLINEFORM0 , only the top INLINEFORM1 partial or complete translations are retained while the rest are pruned. (Implementations of beam search vary in the details, but this variant is simplest for the sake of argument.) Even if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem. Our claim, hinted at by BIBREF0 , is that the brevity problem and the beam problem are essentially the same, and that solving one will solve the other.\"]\n",
            "[\"Following yang-EtAl:2015:EMNLP2, we applied Random Forest BIBREF12 to perform humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations ( INLINEFORM0 ). More details can be found in BIBREF3 .\"]\n",
            "['Convolutional Neural Networks (CNNs) have recently been successfully used in several text categorization tasks (e.g., review rating, sentiment recognition, and question type recognition). Kim2014,Johnson2015,Zhang2015 suggested that using a simple CNN setup, which entails one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks. Deep learning recently has been applied to computational humor research BIBREF5 , BIBREF6 . In Bertero2016LREC, CNN was found to be the best model that uses both acoustic and lexical cues for humor recognition. By using Long Short Time Memory (LSTM) cells BIBREF7 , Bertero2016NAACL showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) BIBREF8 .']\n",
            "['Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 “one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.']\n",
            "['Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 “one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.']\n",
            "['In the context of artificial intelligence (AI), commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate BIBREF3 , BIBREF4 , BIBREF5 . Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods.']\n",
            "['Researchers have also proposed several methods to incorporate knowledge as external memory into the Seq2Seq framework. BIBREF15 incorporated the topic words of the message obtained from a pre-trained latent Dirichlet allocation (LDA) model into the context vector through a joint attention mechanism. BIBREF1 mined FoodSquare tips to be searched by an input message in the food domain and encoded such tips into the context vector through one-turn hop. The model we propose in this work shares similarities with BIBREF16 , which encoded unstructured textual knowledge with a recurrent neural network (RNN). Our work distinguishes itself from previous research in that we consider a large heterogeneous commonsense knowledge base in an open-domain retrieval-based dialogue setting.']\n",
            "['In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.']\n",
            "['Based on the Open Mind Common Sense project BIBREF23 , ConceptNet not only contains objective facts such as “Paris is the capital of France” that are constantly true, but also captures informal relations between common concepts that are part of everyday knowledge such as “A dog is a pet”. This feature of ConceptNet is desirable in our experiments, because the ability to recognize the informal relations between common concepts is necessary in the open-domain conversation setting we are considering in this paper.', 'In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.']\n",
            "['In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.']\n",
            "['In contrast, we propose in this work HotelRec, a novel large-scale hotel recommendation dataset based on hotel reviews from TripAdvisor, and containing approximately 50 million reviews. A sample review is shown in Figure FIGREF1. To the best of our knowledge, HotelRec is the largest publicly available hotel review dataset (at least 60 times larger than previous datasets). Furthermore, we analyze various aspects of the HotelRec dataset and benchmark the performance of different models on two tasks: rating prediction and recommendation performance. Although reasonable performance is achieved by a state-of-the-art method, there is still room for improvement. We believe that HotelRec will offer opportunities to apply and develop new large recommender systems, and push furthermore the recommendation for hotels, which differs from traditional datasets.']\n",
            "[\"Everyday a large number of people write hotel reviews on on-line platforms (e.g., Booking, TripAdvisor) to share their opinions toward multiple aspects, such as their Overall experience, the Service, or the Location. Among the most popular platforms, we selected TripAdvisor: according to their third quarterly report of November 2019, on the U.S. Securities and Exchange Commission website, TripAdvisor is the world's largest online travel site with approximately $1.4$ million hotels. Consequently, we created our dataset HotelRec based on TripAdvisor hotel reviews. The statistics of the HotelRec dataset, the 5-core, and 20-core versions are shown in Table TABREF2; each contains at least $k$ reviews for each user or item.\"]\n",
            "['Moreover, to investigate the effect of only char INLINEFORM0 -MS-vec, we ignore INLINEFORM1 in Equation EQREF5 . We refer to this setting as “Remove word embeddings INLINEFORM2 ” in Table TABREF24 . Table TABREF24 shows cahr3-MS-vec and char4-MS-vec are superior to char2-MS-vec. In the view of perplexity, char3-MS-vec and char4-MS-vec achieved comparable scores to each other. On the other hand, char3-MS-vec is composed of much smaller parameters. Furthermore, we decreased the embedding size INLINEFORM3 to adjust the number of parameters to the same size as the baseline (“Same #Params as baseline” in Table TABREF24 ). In this setting, char3-MS-vec achieved the best perplexity. Therefore, we consider that char3-MS-vec is more useful than char4-MS-vec, which is the answer to the fourth research question. We use the combination of the char3-MS-vec INLINEFORM4 and word embedding INLINEFORM5 in the following experiments.']\n",
            "['Table TABREF15 shows perplexities of the baselines and the proposed method. We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams. Thus, we answer yes to the first research question.']\n",
            "['For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 .']\n",
            "['For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 .']\n",
            "['For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 .']\n",
            "['BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.']\n",
            "['Although our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.']\n",
            "['Although our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.']\n",
            "['As shown in Section SECREF13 , some classification performance drop happens when mitigation methods. We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time. Some previous works BIBREF17 , BIBREF18 employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Although those approaches are not directly comparable to our methods, it would be interesting to explore adversarial training to tackle this problem in the future.']\n",
            "['Debiased Word Embeddings (DE) BIBREF9 proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.', 'Gender Swap (GS) We augment the training data by identifying male entities and swapping them with equivalent female entities and vice-versa. This simple method removes correlation between gender and classification decision and has proven to be effective for correcting gender biases in co-reference resolution task BIBREF13 .', 'Bias fine-tuning (FT) We propose a method to use transfer learning from a less biased corpus to reduce the bias. A model is initially trained with a larger, less-biased source corpus with a same or similar task, and fine-tuned with a target corpus with a larger bias. This method is inspired by the fact that model bias mainly rises from the imbalance of labels and the limited size of data samples. Training the model with a larger and less biased dataset may regularize and prevent the model from over-fitting to the small, biased dataset.']\n",
            "['To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of “attending” model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.']\n",
            "['We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:']\n",
            "['We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:']\n",
            "['We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:']\n",
            "['We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged.']\n",
            "['We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged.']\n",
            "['For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 . INLINEFORM1']\n",
            "['For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 . INLINEFORM1']\n",
            "['For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 . INLINEFORM1']\n",
            "['We conduct our experiments on the public benchmark dataset of “Quora Question Pairs” for the task of paraphrase identification. Experimental results demonstrate that our model outperforms other baselines extensively and significantly, which verifies our theory about the aligned unmatched parts and illustrates the effectiveness of our methodology.']\n",
            "['We conduct our experiments on the public benchmark dataset of “Quora Question Pairs” for the task of paraphrase identification. Experimental results demonstrate that our model outperforms other baselines extensively and significantly, which verifies our theory about the aligned unmatched parts and illustrates the effectiveness of our methodology.']\n",
            "['We conduct our experiments on the public benchmark dataset of “Quora Question Pairs” for the task of paraphrase identification. Experimental results demonstrate that our model outperforms other baselines extensively and significantly, which verifies our theory about the aligned unmatched parts and illustrates the effectiveness of our methodology.']\n",
            "['We propose an alternative to the Extract first, Abstract later (EA) approach which eliminates the need for an extractive model and enables the use of all input documents when generating the summary. Figure FIGREF5 illustrates our Condense-Abstract (CA) framework. In lieu of an integrated encoder-decoder, we generate summaries using two separate models. The Condense model returns document encodings for $N$ input documents, while the Abstract model uses these encodings to create an abstractive summary. This two-step approach has at least three advantages for multi-document summarization. Firstly, optimization is easier since parameters for the encoder and decoder weights are learned separately. Secondly, CA-based models are more space-efficient, since $N$ documents in the cluster are not treated as one very large instance but as $N$ separate instances when training the Condense model. Finally, it is possible to generate customized summaries targeting specific aspects of the input since the Abstract model operates over the encodings of all available documents.', 'Let $\\\\mathcal {D}$ denote a cluster of $N$ documents about a specific target (e.g., a movie or product). For each document $X=\\\\lbrace w_1,w_2,...,w_M\\\\rbrace \\\\in \\\\mathcal {D}$, the Condense model learns an encoding $d$, and word-level encodings $h_1, h_2, ..., h_M$. We use a BiLSTM autoencoder as the Condense model. Specifically, we employ a Bidirectional Long Short Term Memory (BiLSTM) encoder BIBREF31:', 'The Abstract model first fuses the multiple encodings obtained from the Condense stage and then generates a summary using a decoder.', 'The decoder generates summaries conditioned on the reduced document encoding $d^{\\\\prime }$ and reduced word-level encodings $h^{\\\\prime }_1,h^{\\\\prime }_2,...,h^{\\\\prime }_V$. We use a simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32. We set the first hidden state $s_0$ to $d^{\\\\prime }$, and run an LSTM to calculate the current hidden state using the previous hidden state $s_{t-1}$ and word $y^{\\\\prime }_{t-1}$ at time step $t$:']\n",
            "['We performed experiments on the Rotten Tomatoes dataset provided in BIBREF16. It contains 3,731 movies; for each movie we are given a large set of reviews (99.8 on average) written by professional critics and users and a gold-standard consensus, i.e. a summary written by an editor (see an example in Figure FIGREF1). On average, reviews are 19.7 tokens long, while the summary length is 19.6 tokens. The dataset is divided into 2,458 movies for training, 536 movies for development, and 737 movies for testing. Following previous work BIBREF16, we used a generic label for movie titles during training which we replace with the original movie names at test time.']\n",
            "[\"There is a void of hybrid summarizers; there haven't been many studies made in the area.Wong BIBREF13 conducted some preliminary research but there isn't much there on benchmark tests to our knowledge. We use a mixture of statistical and semantic models, assign weights among them by training on field-specific corpora. As there is a significant variation in choices among different fields. We support our proposal with expectations that shortcomings posed by one model can be filled with positives from others. We deploy experimental analysis to test our proposition.\"]\n",
            "['For Statistical analysis we use Similarity matrices, word co-occurrence/ n-gram model, andTF/IDF matrix. For semantic analysis we use custom Glove based model, WordNet based Model and Facebook InferSent BIBREF4 based Model. For Multi-Document Summarization,after training on corpus, we assign weights among the different techniques .We store the sense vector for documents, along with weights, for future reference. For Single document summarization, firstly we calculate the sense vector for that document and calculate the nearest vector from the stored Vectors, we use the weights of the nearest vector. We will describe the flow for semantic and statistical models separately.', 'Now for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.', 'Here, INLINEFORM0 denotes INLINEFORM1 for INLINEFORM2 model in INLINEFORM3 document.', 'We now obtain cWeight as we did previously, and formulate cumulative summary, capturing the consensus of different models. We hence used a supervised learning algorithm to capture the mean performances of different models over the training data to fine-tune our summary.']\n",
            "['We implement our question answering system using state-of-the-art open source components. Our pipeline is based on the Rasa natural language understanding (NLU) framework BIBREF21 which offers two standard pipelines for text classification: spacy_sklearn and tensorflow_embedding. The main difference is that spacy_sklearn uses Spacy for feature extraction with pre-trained word embedding models and Scikit-learn BIBREF22 for text classification. In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend. Figure FIGREF5 shows the general structure of both pipelines. We train QA models using both pipelines with the pre-defined set of hyper-parameters. For tensorflow_embedding, we additionally monitor changes in system performance using different epoch configurations. Further, we compare the performances of pipelines with or without a spellchecker and investigate whether model training benefits from additional user examples by training models with the three different versions of our training corpus including no additional samples (kw), samples from 1 user (kw+1u) or samples from 2 users (kw+2u) (see section Corpora). All training conditions are summarized in Table TABREF4 . Next, we describe the implementation details of our QA system as shown in Figure FIGREF5 : the spellchecker module, the subsequent pre-processing and feature encoding, and the text classification. We include descriptions for both pipelines.']\n",
            "['Evaluation Corpus.', 'The performance of the implemented QA system and of our re-ranking approach is assessed using a separate test corpus. It includes 3084 real user requests from a chat-log of T-Mobile Austria, which are assigned to suitable answers from the training corpus (at most three). The assignment was performed manually by domain experts of the wireless network provider. We use this corpus for estimating the baseline performance of the QA pipeline using different pipeline configurations and different versions of the training corpus. In addition, we use the corpus for evaluating our re-ranking approach per cross-validation: we regard the expert annotations as offline human feedback. The queries in this corpus contain a lot of spelling mistakes. We address this in our QA pipeline generation by implementing a custom spell-checking component.']\n",
            "['The performance of the implemented QA system and of our re-ranking approach is assessed using a separate test corpus. It includes 3084 real user requests from a chat-log of T-Mobile Austria, which are assigned to suitable answers from the training corpus (at most three). The assignment was performed manually by domain experts of the wireless network provider. We use this corpus for estimating the baseline performance of the QA pipeline using different pipeline configurations and different versions of the training corpus. In addition, we use the corpus for evaluating our re-ranking approach per cross-validation: we regard the expert annotations as offline human feedback. The queries in this corpus contain a lot of spelling mistakes. We address this in our QA pipeline generation by implementing a custom spell-checking component.']\n",
            "['Existing KGE approaches based on the skip-gram model such as RDF2Vec BIBREF4 submit paths built using random walks to a Word2Vec algorithm. Instead, we preprocess the input knowledge base by converting each triple into a small sentence of three words. Our method is faster as it allows us to avoid the path generation step. The generated text corpus is thus processed by the skip-gram model as follows.']\n",
            "['We adapt the skip-gram model BIBREF21 to deal with our small sequences of length three. In this work, we only consider URIs and discard literals, therefore we compute a vector for each element $u \\\\in E \\\\cap R$ . Considering a triple as a sequence of three URIs $T = \\\\lbrace u_s, u_p, u_o$ }, the aim is to maximize the average log probability', '$$\\\\frac{1}{3} \\\\sum _{u \\\\in T} \\\\sum _{u^{\\\\prime } \\\\in T \\\\setminus u} \\\\log p(u | u^{\\\\prime })$$ (Eq. 9)', 'which means, in other words, to adopt a context window of 2, since the sequence size is always $|T|=3$ . The probability above is theoretically defined as:']\n",
            "['Evaluating conversational agents is an open research problem in the field. With the inclusion of emotion component in the modern conversation agents, evaluating such models has become even more complex.The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. In the paper by Liu et al, 2016 BIBREF31 , the authors discuss about how not to evaluate the dialogue system. They provide quantitative and qualitative results highlighting specific weaknesses in existing metrics and provide recommendations for the future development of better automatic evaluation metrics for dialogue systems.', 'According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.']\n",
            "['Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.', 'Even though most of the modern work in the field is built on this approach there is a significant drawback to this idea. This model can theoretically never solve the problem of modelling dialogues due to various simplifications, the most important of them being the objective function that is being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. It is important to see that optimizing an agent to generate text based on what it sees in the two-turn conversation dataset that it is trained on does not mean that the agent would be able to generalize to human level conversation across contexts. Nevertheless in absence of a better way to capture human communication this approach laid the foundation of most of the modern advances in the field. Another problem that plagues this paper and the field in general is Evaluation. As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing. In this paper to show the efficacy of their model the authors publish snippets of conversations across different datasets. We discuss this general problem in evaluation later.', 'Evaluating conversational agents is an open research problem in the field. With the inclusion of emotion component in the modern conversation agents, evaluating such models has become even more complex.The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. In the paper by Liu et al, 2016 BIBREF31 , the authors discuss about how not to evaluate the dialogue system. They provide quantitative and qualitative results highlighting specific weaknesses in existing metrics and provide recommendations for the future development of better automatic evaluation metrics for dialogue systems.', 'According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.']\n",
            "['Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.']\n",
            "['Evaluating conversational agents is an open research problem in the field. With the inclusion of emotion component in the modern conversation agents, evaluating such models has become even more complex.The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. In the paper by Liu et al, 2016 BIBREF31 , the authors discuss about how not to evaluate the dialogue system. They provide quantitative and qualitative results highlighting specific weaknesses in existing metrics and provide recommendations for the future development of better automatic evaluation metrics for dialogue systems.']\n",
            "['Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.']\n",
            "['Reinforcement Learning based models', 'After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.', 'Initial reinforcement methods', 'One of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 . They used an on-policy Monte Carlo method and the objective function they used was a linear combination of the solution quality (S) and the dialog length (L), taking the form: o(S,I) = INLINEFORM0 - INLINEFORM1 .', 'End-to-End Reinforcement Learning of Dialogue Agents for Information Access', 'Let’s have a look at KB-InfoBot (by Dhingra et al, 2017 BIBREF26 ): a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. In this paper, they replace the symbolic queries (which break the differentiability of the system and prevent end-to-end training of neural dialogue agents) with an induced ‘soft’ posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users.', 'In the paper by Su et al, 2017 BIBREF27 , they proposed a sample-efficient actor-critic reinforcement learning with supervised data for dialogue management. Just for a heads up, actor-critic algorithms are the algorithms that have an actor stores the policy according to which the action is taken by the agent and a critic that critiques the actions chosen by the actor (that is, the rewards obtained after the action are sent to the critic using which it calculates value functions).', 'To speed up the learning process, they presented two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER). Both models employ off-policy learning with experience replay to improve sample-efficiency. For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence.', 'In the paper by Li et al, 2017 BIBREF28 , the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones. The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines. The outputs from the discriminator are then used as rewards for the generative model pushing the system to generate dialogues that mostly resemble human dialogues.']\n",
            "['Recently, generative adversarial networks are being explored and how they can be used in the dialog agents. Although generative adversarial networks are a topic in itself to explore. However, the paper mentioned below used uses reinforcement learning along with generative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans.', 'In the paper by Li et al, 2017 BIBREF28 , the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones. The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines. The outputs from the discriminator are then used as rewards for the generative model pushing the system to generate dialogues that mostly resemble human dialogues.', 'The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. The policy gradient methods are used to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator is used as a reward for the generator, which is trained to maximize the expected reward of generated utterances using the REINFORCE algorithm.']\n",
            "['Recently, generative adversarial networks are being explored and how they can be used in the dialog agents. Although generative adversarial networks are a topic in itself to explore. However, the paper mentioned below used uses reinforcement learning along with generative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans.', 'In the paper by Li et al, 2017 BIBREF28 , the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones. The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines. The outputs from the discriminator are then used as rewards for the generative model pushing the system to generate dialogues that mostly resemble human dialogues.']\n",
            "['Sequence to Sequence approaches for dialogue modelling', 'The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.', 'Language Model based approaches for dialogue modelling', 'Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses.']\n",
            "['Sequence to Sequence approaches for dialogue modelling', 'The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.', 'Language Model based approaches for dialogue modelling', 'Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses.']\n",
            "['Sequence to Sequence approaches for dialogue modelling', 'Language Model based approaches for dialogue modelling']\n",
            "['The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing. They wrote the grammar based on a combination of their own intuition and a small scale Wizard-of-Oz experiment they ran. The grammar rules used to identify bus stops were generated automatically from the schedule database. After this, they trained a statistical language model on the artificial corpus. In order to make the parsing grammar robust enough to parse fairly ungrammatical, yet understandable sentences, it was kept as general as possible. On making it public, they initially achieved a task success rate of 43.3% for the whole corpus and 43.6 when excluding sessions that did not contain any system-directed speech.', 'After this they tried to increase the performance of the system (Raux et al, 2006 BIBREF10 ). They retrained their acoustic models by performing Baum-Welch optimization on the transcribed data (starting from their original models). Unfortunately, this only brought marginal improvement because the models (semi-continuous HMMs) and algorithms they were using were too simplistic for this task. They improved the turn-taking management abilities of the system by closely analysing the feedback they received. They added more specific strategies, aiming at dealing with problems like noisy environments, too loud or too long utterances, etc. They found that they were able to get a success rate of 79% for the complete dialogues (which was great).']\n",
            "['Maxine Eskenazi and her team initially wanted to build spoken dialog system for the less general sections of the population, such as the elderly and non-native speakers of English. They came up with Let’s Go project (Raux et al, 2003 BIBREF8 ) that was designed to provide Pittsburgh area bus information. Later, this was opened to the general public (Raux et al, 2005 BIBREF9 ). Their work is important in terms of the techniques they used.', 'The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing. They wrote the grammar based on a combination of their own intuition and a small scale Wizard-of-Oz experiment they ran. The grammar rules used to identify bus stops were generated automatically from the schedule database. After this, they trained a statistical language model on the artificial corpus. In order to make the parsing grammar robust enough to parse fairly ungrammatical, yet understandable sentences, it was kept as general as possible. On making it public, they initially achieved a task success rate of 43.3% for the whole corpus and 43.6 when excluding sessions that did not contain any system-directed speech.']\n",
            "['Early Techniques', 'Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times).']\n",
            "['Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times).', 'In 1995, two researchers (Ball et al, 1995 BIBREF4 ) at Microsoft developed a conversational assistant called Persona which was one of the first true personal assistant similar to what we have in recent times (like Siri, etc). It allowed users the maximum flexibility to express their requests in whatever syntax they found most natural and the interface was based on a broad-coverage NLP system unlike the system discussed in the previous paragraph. In this, a labelled semantic graph is generated from the speech input which encodes case frames or thematic roles. After this, a sequence of graph transformations is applied on it using the knowledge of interaction scenario and application domain. This results into a normalized application specific structure called as task graph which is then matched against the templates (in the application) which represent the normalized task graphs corresponding to all the possible user statements that the assistant understands and the action is then executed. The accuracy was not that good and they did not bother to calculate it. Also, due to the integrated nature of conversational interaction in Persona, the necessary knowledge must be provided to each component of the system. Although it had limitations, it provided a very usable linguistic foundation for conversational interaction.', 'Maxine Eskenazi and her team initially wanted to build spoken dialog system for the less general sections of the population, such as the elderly and non-native speakers of English. They came up with Let’s Go project (Raux et al, 2003 BIBREF8 ) that was designed to provide Pittsburgh area bus information. Later, this was opened to the general public (Raux et al, 2005 BIBREF9 ). Their work is important in terms of the techniques they used.']\n",
            "['Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times).']\n",
            "['Early Techniques', 'Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times).']\n",
            "['Laszlo and Petrović BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrović described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matković. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matković was documented in his doctoral dissertation but remained unpublished until 1959.']\n",
            "['Beginnings of Machine Translation and Artificial Intelligence in the USA and USSR', 'There were many other centers for research in machine translation: Gorkovsky University (Omsk), 1st Moscow Institute for Foreign Languages, Computing Centre of the Armenian SSR and at the Institute for Automatics and Telemechanics of the Georgian SSR BIBREF7. It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal.']\n",
            "[\"In the USSR, there were four major approaches to machine translation in the late 1950s BIBREF7. The first one was the research at the Institute for Precise Mechanics and Computational Technology of the USSR Academy of Sciences. Their approach was mostly experimental and not much different from today's empirical methods. They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. The second approach, as noted by Mulić BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8. It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.\", 'The step which was needed here was to eliminate the notion of structure alignment and just seek sentential alignment. This, in theory, can be done by using only entropy. A simple alignment could be made by using word entropies in both languages and aligning the words by decreasing entropy. This would work better for translating into a language with no articles. A better approach, which was not beyond the thinking of the group since it was already proposed by Matković in his dissertation from 1957 BIBREF20, would be to use word bigrams and align them. It is worth mentioning that, although the idea of machine translation in the 1950s in Croatia did not have a significant influence on development of the field, it shows that Croatian linguists had contemporary views and necessary competencies for its development. But, unfortunately, the development of machine translation in Croatia had been stopped because of the previously discussed circumstances. In 1964, Laszlo went to the USA, where he spent the next seven years, and after returning to Croatia, he was active as a university professor, but because of disagreement with the ruling political option regarding Croatian language issues, he published very rarely and was mainly focused on other linguistic issues in that period, but his work was a major influence on the later development of computational linguistics in Croatia.']\n",
            "['Laszlo and Petrović BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrović described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matković. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matković was documented in his doctoral dissertation but remained unpublished until 1959.']\n",
            "['The Russian machine translation pioneer Andreev expressed hope that the Yugoslav (Croatian) research group could create a prototype, but sadly, due to the lack of federal funding, this never happened BIBREF10. Unlike their colleagues in the USA and the USSR, Laszlo’s group had to manage without an actual computer (which is painfully obvious in BIBREF12), and the results remained mainly theoretical. Appealing probably to the political circles of the time, Laszlo and Petrović note that, although it sounds strange, research in computational linguistics is mainly a top-priority military effort in other countries BIBREF11. There is a quote from BIBREF10 which perhaps best delineates the optimism and energy that the researchers in Zagreb had:']\n",
            "['Finka and Laszlo envisioned three main data preparation tasks that are needed before prototype development could commence BIBREF10. The first task is to compile a dictionary of words sorted from the end of the word to the beginning. This would enable the development of what is now called stemming and lemmatization modules: a knowledge base with suffixes so they can be trimmed, but also a systematic way to find the base of the word (lemmatization) (p. 121). The second task would be to make a word frequency table. This would enable focusing on a few thousand most frequent words and dropping the rest. This is currently a good industrial practice for building efficient natural language processing systems, and in 1962, it was a computational necessity. The last task was to create a good thesaurus, but such a thesaurus where every data point has a \"meaning\" as the key, and words (synonyms) as values. The prototype would then operate on these meanings when they become substituted for words.']\n",
            "['Separation of the dictionary from the MT algorithm', 'Separation of the understanding and generation modules of the MT algorithms', 'All words need to be lemmatized', 'The word lemma should be the key of the dictionary, but other forms of the word must be placed as a list in the value next to the key', 'Use context to determine the meaning of polysemous words.']\n",
            "[\"In the USSR, there were four major approaches to machine translation in the late 1950s BIBREF7. The first one was the research at the Institute for Precise Mechanics and Computational Technology of the USSR Academy of Sciences. Their approach was mostly experimental and not much different from today's empirical methods. They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. The second approach, as noted by Mulić BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8. It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.\"]\n",
            "['One of the first recorded attempts of producing a machine translation system in the USSR was in 1954 BIBREF6, and the attempt was applauded by the Communist party of the Soviet Union, by the USSR Committee for Science and Technology and the USSR Academy of Sciences. The source does not specify how this first system worked, but it does delineate that the major figures of machine translation of the time were N. Andreev of the Leningrad State University, O. Kulagina and I. Melchuk of the Steklov Mathematical Institute. There is information on an Indonesian-to-Russian machine translation system by Andreev, Kulagina and Melchuk from the early 1960s, but it is reported that the system was ultimately a failure, in the same way early USA systems were. The system had statistical elements set forth by Andreev, but the bulk was logical and knowledge-heavy processing put forth by Kulagina and Melchuk. The idea was to have a logical intermediate language, under the working name “Interlingua”, which was the connector of both natural languages, and was used to model common-sense human knowledge. For more details, see BIBREF6.']\n",
            "['One of the first recorded attempts of producing a machine translation system in the USSR was in 1954 BIBREF6, and the attempt was applauded by the Communist party of the Soviet Union, by the USSR Committee for Science and Technology and the USSR Academy of Sciences. The source does not specify how this first system worked, but it does delineate that the major figures of machine translation of the time were N. Andreev of the Leningrad State University, O. Kulagina and I. Melchuk of the Steklov Mathematical Institute. There is information on an Indonesian-to-Russian machine translation system by Andreev, Kulagina and Melchuk from the early 1960s, but it is reported that the system was ultimately a failure, in the same way early USA systems were. The system had statistical elements set forth by Andreev, but the bulk was logical and knowledge-heavy processing put forth by Kulagina and Melchuk. The idea was to have a logical intermediate language, under the working name “Interlingua”, which was the connector of both natural languages, and was used to model common-sense human knowledge. For more details, see BIBREF6.', \"In the USSR, there were four major approaches to machine translation in the late 1950s BIBREF7. The first one was the research at the Institute for Precise Mechanics and Computational Technology of the USSR Academy of Sciences. Their approach was mostly experimental and not much different from today's empirical methods. They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. The second approach, as noted by Mulić BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8. It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.\"]\n",
            "['We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.']\n",
            "['Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.']\n",
            "['We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.']\n",
            "['We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .']\n",
            "['We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .']\n",
            "['The results of the automatic evaluation are displayed in Table TABREF15 . We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.']\n",
            "[\"Our models were tuned on the development sets, either with BLEU BIBREF28 that scores the output by counting INLINEFORM0 -gram matches with the reference, or SARI BIBREF12 that compares the output against both the reference and the input sentence. Both measures are commonly used to automatically evaluate the quality of simplification output. We noticed that SARI should be used with caution when tuning neural Seq2seq simplification models. Since SARI depends on the differences between a system's output and the input sentence, large differences may yield very good SARI even though the output is ungrammatical. Thus, when tuning with SARI, we ignored epochs in which the BLEU score of the output is too low, using a threshold INLINEFORM1 . We set INLINEFORM2 to 22 on Newsela, 33 on WikiSmall, and 77 on WikiLarge.\"]\n",
            "['We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.']\n",
            "['We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.']\n",
            "['Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.']\n",
            "['Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.']\n",
            "['Despise the promising results achieved in last years, machine translation (MT) is still far from producing high-quality translations BIBREF11. Therefore, a human agent has to supervise these translation in a post-editing stage. IMT was introduced with the goal of combining the knowledge of a human translator and the efficiency of an MT system. Although many protocols have been proposed in recent years BIBREF12, BIBREF13, BIBREF14, BIBREF15, the prefix-based remains as one of the most successful approaches BIBREF5, BIBREF16, BIBREF17. In this approach, the user corrects the leftmost wrong word from the translation hypothesis, inherently validating a correct prefix. With each new correction, the system generates a suffix that completes the prefix to produce a new translation.']\n",
            "['The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version—considering this as the original document—and the 1888 version—considering 19$^{\\\\mathrm {th}}$ century Dutch as modern Dutch.', 'We selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\\\\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\\\\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote—despite the original documents belonging to different time periods—in order to modernize El Conde Lucanor.']\n",
            "['The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version—considering this as the original document—and the 1888 version—considering 19$^{\\\\mathrm {th}}$ century Dutch as modern Dutch.', 'We selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\\\\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\\\\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote—despite the original documents belonging to different time periods—in order to modernize El Conde Lucanor.']\n",
            "['Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\\\\mathbf {x}$, SMT aims at finding its most likely translation $\\\\hat{\\\\mathbf {y}}$ BIBREF18:', 'For years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).', 'Prefix-based IMT proposed a user–computer collaboration that starts with the system proposing an initial translation $\\\\mathbf {y}$ of length $I$. Then, the user corrects the leftmost wrong word $y_i$, inherently validating all preceding words. These words form a validated prefix $\\\\tilde{\\\\mathbf {y}}_p$, that includes the corrected word $\\\\tilde{y}_i$. The system reacts to this user feedback, generating a suffix $\\\\hat{\\\\mathbf {y}}_s$ that completes $\\\\tilde{\\\\mathbf {y}}_p$ to obtain a new translation of $\\\\mathbf {x}:\\\\hat{\\\\mathbf {y}}~=~\\\\tilde{\\\\mathbf {y}}_p\\\\,\\\\hat{\\\\mathbf {y}}_s$. This process is repeated until the user accepts the complete system suggestion. fi:IMT illustrates this protocol.', 'Interactive Machine Translation ::: Neural Machine Translation', 'In NMT, eq:SMT is modeled by a neural network with parameters $\\\\mathbf {\\\\Theta }$:', 'This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.', 'Interactive Machine Translation ::: Prefix-based Interactive Neural Machine Translation', \"The prefix-based IMT protocol (see se:PBIMT) can be naturally included into NMT systems since sentences are generated from left to right. In order to take into account the user's feedback and generate compatible hypothesis, the search space must be constraint. Given a prefix $\\\\tilde{\\\\mathbf {y}}_p$, only a single path accounts for it. The branching of the search process starts once this path has been covered. Introducing the validated prefix $\\\\tilde{\\\\mathbf {y}}_p$, eq:NMT becomes:\", 'which implies a search over the space of translations, but constrained by the validated prefix $\\\\tilde{\\\\mathbf {y}}_p$ BIBREF15.']\n",
            "['SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model—smoothed with the improved KneserNey method—using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.', 'We built our NMT systems using NMT-Keras BIBREF32. We used long short-term memory units BIBREF33, with all model dimensions set to 512. We trained the system using Adam BIBREF34 with a fixed learning rate of $0.0002$ and a batch size of 60. We applied label smoothing of $0.1$ BIBREF35. At inference time, we used beam search with a beam size of 6. We applied joint byte pair encoding to all corpora BIBREF36, using $32,000$ merge operations.', 'Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras.']\n",
            "['The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version—considering this as the original document—and the 1888 version—considering 19$^{\\\\mathrm {th}}$ century Dutch as modern Dutch.', 'We selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\\\\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\\\\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\\\\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote—despite the original documents belonging to different time periods—in order to modernize El Conde Lucanor.']\n",
            "['Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.']\n",
            "['Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.']\n",
            "['We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.']\n",
            "['We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.']\n",
            "['We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.']\n",
            "['We compare with the following baseline methods:', 'RB (Rule based method): The rule based method proposed in BIBREF33 .', 'CB (Common-sense based method): This is the knowledge based method proposed by BIBREF34 . We use the Chinese Emotion Cognition Lexicon BIBREF35 as the common-sense knowledge base. The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.', 'RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .', 'SVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31', 'Word2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.', 'Multi-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper.', 'CNN: The convolutional neural network for sentence classification BIBREF5 .', 'Memnet: The deep memory network described in Section SECREF3 . Word embeddings are pre-trained by skip-grams. The number of hops is set to 3.']\n",
            "['Evaluation and Comparison', 'We compare with the following baseline methods:', 'RB (Rule based method): The rule based method proposed in BIBREF33 .', 'CB (Common-sense based method): This is the knowledge based method proposed by BIBREF34 . We use the Chinese Emotion Cognition Lexicon BIBREF35 as the common-sense knowledge base. The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.', 'RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .', 'SVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31', 'Word2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.', 'Multi-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper.', 'CNN: The convolutional neural network for sentence classification BIBREF5 .']\n",
            "['RB (Rule based method): The rule based method proposed in BIBREF33 .', 'CB (Common-sense based method): This is the knowledge based method proposed by BIBREF34 . We use the Chinese Emotion Cognition Lexicon BIBREF35 as the common-sense knowledge base. The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.', 'RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .', 'SVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31', 'Word2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.', 'Multi-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper.', 'CNN: The convolutional neural network for sentence classification BIBREF5 .']\n",
            "['We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause.']\n",
            "['We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause.']\n",
            "['We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause.']\n",
            "['Here, the size of INLINEFORM0 is INLINEFORM1 . Since the prediction vector is a concatenation of three outputs. We implement a concatenation operation rather than averaging or other operations because the parameters in different memory slots can be updated [id=lq]respectively in this way by back propagation. The concatenation of three output vectors forms a sequence-level feature which can be used in the training. Such a feature is important especially [id=lq]when the size of annotated training data is small.']\n",
            "['In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1 . We specify flexible, parameterised model families with the ability to adjust embedding and recurrent cell sizes for a given parameter budget and with fine grain control over regularisation and learning hyperparameters.', 'Our aim is strictly to do better model comparisons for these architectures and we thus refrain from including techniques that are known to push perplexities even lower, but which are believed to be largely orthogonal to the question of the relative merits of these recurrent cells. In parallel work with a remarkable overlap with ours, BIBREF5 demonstrate the utility of adding a Neural Cache BIBREF6 . Building on their work, BIBREF7 show that Dynamic Evaluation BIBREF8 contributes similarly to the final perplexity.']\n",
            "['Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .', 'Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.', 'In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task.']\n",
            "['In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1 . We specify flexible, parameterised model families with the ability to adjust embedding and recurrent cell sizes for a given parameter budget and with fine grain control over regularisation and learning hyperparameters.', 'Once hyperparameters have been properly controlled for, we find that LSTMs outperform the more recent models, contra the published claims. Our result is therefore a demonstration that replication failures can happen due to poorly controlled hyperparameter variation, and this paper joins other recent papers in warning of the under-acknowledged existence of replication failure in deep learning BIBREF2 , BIBREF3 . However, we do show that careful controls are possible, albeit at considerable computational cost.']\n",
            "['In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task.', 'We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.']\n",
            "['Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .', 'Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.']\n",
            "['Dropout is applied to feedforward connections denoted by dashed arrows in the figure. From the bottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout), to the combined and the down-projected outputs (output dropout). All these dropouts have random masks drawn independently per time step, in contrast to the dropout on recurrent states where the same mask is used for all time steps in the sequence.', 'The same dropout variants are applied to all three model types, with the exception of intra-layer dropout which does not apply to RHNs since only the recurrent state is passed between the layers. For the recurrent states, all architectures use either variational dropout BIBREF11 or recurrent dropout BIBREF12 , unless explicitly noted otherwise.']\n",
            "['In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1 . We specify flexible, parameterised model families with the ability to adjust embedding and recurrent cell sizes for a given parameter budget and with fine grain control over regularisation and learning hyperparameters.']\n",
            "['For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27. Training is done on 8 GPUs, with accumulated gradients over 10 batches BIBREF27, and a max batch size of 3500 tokens per GPU. We train for 20 epochs, while saving a checkpoint every 2500 updates ($\\\\approx \\\\frac{2}{5}$ epoch on UGC) and average the 5 best checkpoints according to their perplexity on a validation set (a held-out subset of UGC).']\n",
            "['For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27. Training is done on 8 GPUs, with accumulated gradients over 10 batches BIBREF27, and a max batch size of 3500 tokens per GPU. We train for 20 epochs, while saving a checkpoint every 2500 updates ($\\\\approx \\\\frac{2}{5}$ epoch on UGC) and average the 5 best checkpoints according to their perplexity on a validation set (a held-out subset of UGC).']\n",
            "['After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid.']\n",
            "['We sampled 11.5k French reviews from Foursquare, mostly in the food category, split them into 18k sentences, and grouped them into train, valid and test sets (see Table TABREF6). The French reviews contain on average 1.5 sentences and 17.9 words. Then, we hired eight professional translators to translate them to English. Two of them created the training set by post-editing (PE) the outputs of baseline NMT systems. The other six translated the valid and test sets from scratch. They were asked to translate (or post-edit) the reviews sentence-by-sentence (to avoid any alignment problem), but they could see the full context. We manually filtered the test set to remove translations that were not satisfactory. The full reviews and additional metadata (e.g., location and type of the restaurant) are also available as part of this resource, to encourage research on contextual machine translation.']\n",
            "['After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid.']\n",
            "['We took the first 300 test sentences to create 6 tasks of 50 sentences each. Then we asked bilingual colleagues to rank the output of 4 models by their translation quality. They were asked to do one or more of these tasks. The judge did not know about the list of models, nor the model that produced any given translation. We got 12 answers. The inter-judge Kappa coefficient ranged from 0.29 to 0.63, with an average of 0.47, which is a good value given the difficulty of the task. Table TABREF63 gives the results of the evaluation, which confirm our observations with BLEU.']\n",
            "['We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.']\n",
            "['In addition to BLEU, we do an indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task, a human evaluation, and a task-related evaluation based on polysemous words.', 'During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.']\n",
            "['The optimised models are evaluated against two baseline systems: i) an unoptimised linear-kernel SVM (configured with default parameter settings) based on word INLINEFORM0 -grams only and, ii) a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms.']\n",
            "['The optimised models are evaluated against two baseline systems: i) an unoptimised linear-kernel SVM (configured with default parameter settings) based on word INLINEFORM0 -grams only and, ii) a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms.']\n",
            "['The classifier was optimised for feature type (cf. Section SECREF38 ) and hyperparameter combinations (cf. Table TABREF37 ). Model selection was done using 10-fold cross validation in grid search over all possible feature types (i.e., groups of similar features, like different orders of INLINEFORM0 -gram bag-of-words features) and hyperparameter configurations. The best performing hyperparameters are selected by F INLINEFORM1 -score on the positive class. The winning model is then retrained on all held-in data and subsequently tested on a hold-out test set to assess whether the classifier is over- or under-fitting. The holdout represents a random sample ( INLINEFORM2 ) of all data. The folds were randomly stratified splits over the hold-in class distribution. Testing all feature type combinations is a rudimentary form of feature selection and provides insight into which types of features work best for this particular task.']\n",
            "['After pre-processing of the corpus, the following feature types were extracted:', 'Word INLINEFORM0 -gram bag-of-words: binary features indicating the presence of word unigrams, bigrams and trigrams.', 'Character INLINEFORM0 -gram bag-of-words: binary features indicating the presence of character bigrams, trigrams and fourgrams (without crossing word boundaries). Character INLINEFORM1 -grams provide some abstraction from the word level and provide robustness to the spelling variation that characterises social media data.', \"Term lists: one binary feature derived for each one out of six lists, indicating the presence of an item from the list in a post: proper names, `allness' indicators (e.g. always, everybody), diminishers (e.g. slightly, relatively), intensifiers (e.g. absolutely, amazingly), negation words and aggressive language and profanity words. Person alternation is a binary feature indicating whether the combination of a first and second person pronoun occurs in order to capture interpersonal intent.\", 'Subjectivity lexicon features: positive and negative opinion word ratios, as well as the overall post polarity were calculated using existing sentiment lexicons. For Dutch, we made use of the Duoman BIBREF61 and Pattern BIBREF62 lexicons. For English, we included the Hu and Liu opinion lexicon BIBREF63 , the MPQA lexicon BIBREF64 , General Inquirer Sentiment Lexicon BIBREF65 , AFINN BIBREF66 , and MSOL BIBREF67 . For both languages, we included the relative frequency of all 68 psychometric categories in the Linguistic Inquiry and Word Count (LIWC) dictionary for English BIBREF68 and Dutch BIBREF69 .', 'Topic model features: by making use of the Gensim topic modelling library BIBREF70 , several LDA BIBREF71 and LSI BIBREF72 topic models with varying granularity ( INLINEFORM0 = 20, 50, 100 and 200) were trained on data corresponding to each fine-grained category of a cyberbullying event (e.g. threats, defamations, insults, defenses). The topic models were based on a background corpus (EN: INLINEFORM1 tokens, NL: INLINEFORM2 tokens) scraped with the BootCAT BIBREF73 web-corpus toolkit. BootCaT collects ASKfm user profiles using lists of manually determined seed words that are characteristic of the cyberbullying categories.']\n",
            "['After pre-processing of the corpus, the following feature types were extracted:', 'Word INLINEFORM0 -gram bag-of-words: binary features indicating the presence of word unigrams, bigrams and trigrams.', 'Character INLINEFORM0 -gram bag-of-words: binary features indicating the presence of character bigrams, trigrams and fourgrams (without crossing word boundaries). Character INLINEFORM1 -grams provide some abstraction from the word level and provide robustness to the spelling variation that characterises social media data.', \"Term lists: one binary feature derived for each one out of six lists, indicating the presence of an item from the list in a post: proper names, `allness' indicators (e.g. always, everybody), diminishers (e.g. slightly, relatively), intensifiers (e.g. absolutely, amazingly), negation words and aggressive language and profanity words. Person alternation is a binary feature indicating whether the combination of a first and second person pronoun occurs in order to capture interpersonal intent.\", 'Subjectivity lexicon features: positive and negative opinion word ratios, as well as the overall post polarity were calculated using existing sentiment lexicons. For Dutch, we made use of the Duoman BIBREF61 and Pattern BIBREF62 lexicons. For English, we included the Hu and Liu opinion lexicon BIBREF63 , the MPQA lexicon BIBREF64 , General Inquirer Sentiment Lexicon BIBREF65 , AFINN BIBREF66 , and MSOL BIBREF67 . For both languages, we included the relative frequency of all 68 psychometric categories in the Linguistic Inquiry and Word Count (LIWC) dictionary for English BIBREF68 and Dutch BIBREF69 .', 'Topic model features: by making use of the Gensim topic modelling library BIBREF70 , several LDA BIBREF71 and LSI BIBREF72 topic models with varying granularity ( INLINEFORM0 = 20, 50, 100 and 200) were trained on data corresponding to each fine-grained category of a cyberbullying event (e.g. threats, defamations, insults, defenses). The topic models were based on a background corpus (EN: INLINEFORM1 tokens, NL: INLINEFORM2 tokens) scraped with the BootCAT BIBREF73 web-corpus toolkit. BootCaT collects ASKfm user profiles using lists of manually determined seed words that are characteristic of the cyberbullying categories.']\n",
            "[\"Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consists of question-answer pairs published on a user's profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013. After language filtering (i.e., non-English or non-Dutch content was removed), the experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.\"]\n",
            "[\"Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consists of question-answer pairs published on a user's profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013. After language filtering (i.e., non-English or non-Dutch content was removed), the experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.\"]\n",
            "['Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.', 'Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 .', \"We acquired a publicly available dataset of tweets posted by known pro-ISIS Twitter accounts that was published during the 2015 Paris attacks by Kaggle data science community. The dataset consists of around INLINEFORM0 tweets posted by more than 100 users. These tweets were labelled as being pro-ISIS by looking at specific indicators, such as a set of keywords used (in the user's name, description, tweet text), their network of follower/following of other known radical accounts, and sharing of images of the ISIS flag or some radical leaders. To validate that these accounts are indeed malicious, we checked the current status of the users' accounts in the dataset and found that most of them had been suspended by Twitter. This suggests that they did, in fact, possess a malicious behaviour that opposes the Twitter platform terms of use which caused them to be suspended. We filter out any tweets posted by existing active users and label this dataset as known-bad.\"]\n",
            "['Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.']\n",
            "['Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.']\n",
            "['Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.']\n",
            "['Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.']\n",
            "[\"This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .\"]\n",
            "[\"This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .\"]\n",
            "[\"This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .\"]\n",
            "['We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines.']\n",
            "['We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines.']\n",
            "['We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 . Moreover, since ISIS supporters typically advocate for violent behaviour and tend to use offensive curse words, we use dictionaries of violent words and curse words to record the ratio of such words in the tweet. We also count the frequency of words with all capital letters as they are traditionally used to convey yelling behaviour.']\n",
            "['We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 . Moreover, since ISIS supporters typically advocate for violent behaviour and tend to use offensive curse words, we use dictionaries of violent words and curse words to record the ratio of such words in the tweet. We also count the frequency of words with all capital letters as they are traditionally used to convey yelling behaviour.']\n",
            "['In practice, many hotels/items appear infrequently or never in historical data. Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data. Apart from the obvious negative impacts on searchability and sales, neglecting these items can introduce a feedback loop. That is, the less these items are recommended, or the more they are recommended in inappropriate circumstances, the more the data reinforces their apparent lack of popularity.']\n",
            "[\"A robust metric for evaluating a set of hotel embeddings (or, more generally, any set of items displayed to a user in response to an information need) is its ability to predict a user's next click/selection. In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions.\"]\n",
            "['Experimental Results ::: Quantitative Analysis ::: Hits@k for hotel context prediction', \"A robust metric for evaluating a set of hotel embeddings (or, more generally, any set of items displayed to a user in response to an information need) is its ability to predict a user's next click/selection. In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions.\", 'Experimental Results ::: Quantitative Analysis ::: Comparison using cosine similarity', \"In this section, rather than using the model's output probabilities to induce a ranking over hotels, we measure hits@k over the ranking induced using cosine similarity of the embedding vectors. This is useful in scenarios where it isn't feasible to directly use the model's probabilities. Table TABREF21 shows the results for various embeddings. We show that using the enriched vectors one achieves the highest performance.\", 'Experimental Results ::: Quantitative Analysis ::: Average intra/inter market embedding similarities', 'We expect hotels in the same market to be more similar to each other than to hotels in other markets. To evaluate how well this market-level information is encoded by the learned embeddings, we calculate the average similarity between pairs of markets, with the expectation that we should see a strong diagonal component in the similarity matrix. We note that our model is not explicitly trained to learn this kind of market information. However, it is able to learn this by combining the click sessions and hotel attribute information. Figure FIGREF13 shows the average similarity scores between hotels in multiple famous cities using two of the embedding vectors. As Figure FIGREF13 clearly depicts, there is a strong similarity between hotels of the same city. Also, markets that are closer to each other (all US cities vs European vs Asian), or for reasons other than geographic proximity are expected to be more similar (e.g., Las Vegas and Macao, or Tokyo and Paris) do indeed have a higher similarity. For comparison, Figure FIGREF13 shows the average cosine similarity between and within markets for the session-only model embeddings. This model captures within-market similarity well but is not as effective as the enriched model for capturing cross-market similarity. For instance, the session-only model fails to recover the similarity between Las Vegas and Macao.', 'Experimental Results ::: Qualitative Analysis ::: Visualization of embeddings', 'To further illuminate the nature of the embeddings learned by the hotel2vec model, we examine a low-dimensional projection of hotel embeddings in the Miami market (Figures FIGREF25 and FIGREF25). The colors signify the grouping of hotels into various competing subcategories (i.e., similar hotels), manually annotated by a human domain expert. The enriched model is significantly better at clustering similar hotels than the session-only model.', 'Experimental Results ::: Qualitative Analysis ::: Most similar hotels', 'A common scenario is finding similar hotels to a target hotel in other destinations. For example, when the user searches for a specific hotel name (e.g., Hotel Beacon, NY) we would like to be able to recommend a few similar hotels. The learned embeddings can be used to find top-k most similar hotels to a given one. Given a target hotel $h$, we compute the cosine similarity of every other hotel with $h$ and pick the most similar hotels. Rigid evaluation of this system requires A/B testing; here we show a few examples comparing our enriched embeddings and the session-only embeddings in Figure FIGREF29 to provide some intuition for the behavior of the two models.', 'Experimental Results ::: Qualitative Analysis ::: Algebraic operations on hotel embeddings', 'We also investigate whether we can perform meaningful algebraic operations on trained hotel embeddings (similar to the semantic analogy task in BIBREF0). We pose the question \"$h_1$ is to $h_2$ as $h_3$ is to $h_x$\" and find $h_x$ as the hotel with the closest vector to $\\\\mathbf {V_{e_1}}-\\\\mathbf {V_{e_2}}+\\\\mathbf {V_{e_3}}$. Figure FIGREF31 shows an example of such analogy. $h_1$ is a Marriott hotel in NY, $h_2$ is a Hilton in NY, and $h_3$ is a Marriott in LA (near airport). The obtained $h_x$, is a Hilton hotel in LA near the airport, showing the amount of information captured by the enriched embeddings.']\n",
            "['Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1.']\n",
            "['Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1.']\n",
            "['To provide an idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results.', 'We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks BIBREF26, BIBREF27, BIBREF28. We use the IOB2 label encoding for sources, targets, and polar expressions, including the polarity of the latter, giving us nine tags in total. This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored.', 'Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution.']\n",
            "['Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution.']\n",
            "['In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\\\text{\\\\textit {fine}}$, the first such dataset available in Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 – a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc. In Mae:Bar:Ovr:2019, a subset of the documents, dubbed NoReC$_\\\\text{\\\\textit {eval}}$, were annotated at the sentence-level, indicating whether or not a sentence contains an evaluation or not. These prior annotations did not include negative or positive polarity, however, as this can be mixed at the sentence-level. In this work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of other attributes of the expressions. In addition to discussing annotation principles and examples, we also present the first experimental results on the dataset.', 'Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\\\text{\\\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.']\n",
            "['Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\\\text{\\\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.']\n",
            "['Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\\\text{\\\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.']\n",
            "['In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\\\text{\\\\textit {fine}}$, the first such dataset available in Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 – a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc. In Mae:Bar:Ovr:2019, a subset of the documents, dubbed NoReC$_\\\\text{\\\\textit {eval}}$, were annotated at the sentence-level, indicating whether or not a sentence contains an evaluation or not. These prior annotations did not include negative or positive polarity, however, as this can be mixed at the sentence-level. In this work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of other attributes of the expressions. In addition to discussing annotation principles and examples, we also present the first experimental results on the dataset.']\n",
            "['In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\\\text{\\\\textit {fine}}$, the first such dataset available in Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 – a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc. In Mae:Bar:Ovr:2019, a subset of the documents, dubbed NoReC$_\\\\text{\\\\textit {eval}}$, were annotated at the sentence-level, indicating whether or not a sentence contains an evaluation or not. These prior annotations did not include negative or positive polarity, however, as this can be mixed at the sentence-level. In this work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of other attributes of the expressions. In addition to discussing annotation principles and examples, we also present the first experimental results on the dataset.']\n",
            "['We evaluate our model on SParC BIBREF0, a new large-scale dataset for cross-domain semantic parsing in context consisting of coherent question sequences annotated with SQL queries over 200 databases in 138 domains. Experiment results show that by generating from the previous query, our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves.']\n",
            "['We evaluate our model on SParC BIBREF0, a new large-scale dataset for cross-domain semantic parsing in context consisting of coherent question sequences annotated with SQL queries over 200 databases in 138 domains. Experiment results show that by generating from the previous query, our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves.']\n",
            "['Spider. We compare with the results as reported in yu2018syntaxsqlnet. Furthermore, we also include recent results from lee2019recursive who propose to use recursive decoding procedure, bogin2019representing introducing graph neural networks for encoding schemas, and guo2019towards who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries.']\n",
            "['We perform experiments on two tasks, PersonaChat and Wizard of Wikipedia, which evaluate different aspects of conversational ability. We first optimize the questions to maximize worker agreement, and then benchmark existing state-of-the-art models on each task.']\n",
            "[\"PersonaChat BIBREF5 is a chitchat dialogue task involving two participants (two humans or a human and a bot). Each participant is given a persona – a short collection of personal traits such as I'm left handed or My favorite season is spring – and are instructed to get to know each other by chatting naturally using their designated personas, for 6–8 turns. The original dataset contains nearly 9000 human-human training conversations; most models are pretrained with a larger corpus, and then fine-tuned on this set.\", 'Wizard of Wikipedia BIBREF7 is a chitchat dialogue task where two speakers discuss a topic in depth, chosen from 1247 topics. One speaker (termed the Wizard) is meant to be both engaging and knowledgeable on the topics, and has access to an information retrieval system over Wikipedia to supplement their own knowledge. The other speaker (the Apprentice) is meant to be curious and eager to learn about the topic. The original dataset contains over 18,000 human-human dialogues, and has been used to train various kinds of models to imitate the human wizards. These include the Memory Network Transformer, in both generative and retrieval versions that employs the retrieved knowledge by attending over it before producing an utterance (GK and RK respectively), and baselines that do not have access to the knowledge (GU and RU). See Figure FIGREF25 for an example chat. We use the human-model logs from that paper (100 conversations for each model) on unseen test topics and evaluate them against humans (H), using both engagingness and knowledgeability questions. We note the original paper tested engagingness only.']\n",
            "['Our methods enable to select relevant data for the task while requiring only a small set of monolingual in-domain data. As they are based solely on the representations learned by self-supervised LMs, they do not require additional domain labels which are usually vague and over-simplify the notion of domain in textual data. We evaluate our method on data selection for neural machine translation (NMT) using the multi-domain German-English parallel corpus composed by BIBREF8. Our data selection methods enable to train NMT models that outperform those trained using the well-established cross-entropy difference method of BIBREF4 across five diverse domains, achieving a recall of more than 95% in all cases with respect to an oracle that selects the “true” in-domain data.']\n",
            "['As shown in the previous section, using the right data is critical for achieving good performance on an in-domain test set, and more data is not necessarily better. However, in real-world scenarios, the availability of data labeled by domain is limited, e.g. when working with large scale, web-crawled data. In this section we focus on a data-selection scenario where only a very small number of in-domain sentences are used to select data from a larger unlabeled parallel corpus. An established method for data selection was proposed by BIBREF4, which was also used in training the winning systems in WMT 2019 BIBREF39, BIBREF40. This method compares the cross-entropy, according to domain-specific and non-domain-specific language models, for each candidate sentence for selection. The sentences are then ranked by the cross-entropy difference, and only the top sentences are selected for training.']\n",
            "['To evaluate the unsupervised domain clustering we used the multi-domain corpus proposed by BIBREF8 which includes textual data in five diverse domains: subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software). This dataset includes parallel sentences in English and German; for this experiment we used the English portion of the data. See more details on the dataset in Section SECREF22. We used 2000 distinct sentences from each domain. To evaluate whether the resulting clusters indeed capture the domains the data was drawn from we measure the clustering purity, which is a well-known metric for evaluating clustering BIBREF24. To measure the clustering purity, we assign each unsupervised cluster with the most common “true” domain in the sentences assigned to that cluster, and then compute the accuracy according to this majority-based cluster-domain assignment (note that in this case several unsupervised clusters can be assigned to the same domain). In cases where randomness is involved we run each experiment five times with different initializations and report the mean and variance of the purity metric for each model.']\n",
            "['To evaluate the unsupervised domain clustering we used the multi-domain corpus proposed by BIBREF8 which includes textual data in five diverse domains: subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software). This dataset includes parallel sentences in English and German; for this experiment we used the English portion of the data. See more details on the dataset in Section SECREF22. We used 2000 distinct sentences from each domain. To evaluate whether the resulting clusters indeed capture the domains the data was drawn from we measure the clustering purity, which is a well-known metric for evaluating clustering BIBREF24. To measure the clustering purity, we assign each unsupervised cluster with the most common “true” domain in the sentences assigned to that cluster, and then compute the accuracy according to this majority-based cluster-domain assignment (note that in this case several unsupervised clusters can be assigned to the same domain). In cases where randomness is involved we run each experiment five times with different initializations and report the mean and variance of the purity metric for each model.']\n",
            "['To evaluate the unsupervised domain clustering we used the multi-domain corpus proposed by BIBREF8 which includes textual data in five diverse domains: subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software). This dataset includes parallel sentences in English and German; for this experiment we used the English portion of the data. See more details on the dataset in Section SECREF22. We used 2000 distinct sentences from each domain. To evaluate whether the resulting clusters indeed capture the domains the data was drawn from we measure the clustering purity, which is a well-known metric for evaluating clustering BIBREF24. To measure the clustering purity, we assign each unsupervised cluster with the most common “true” domain in the sentences assigned to that cluster, and then compute the accuracy according to this majority-based cluster-domain assignment (note that in this case several unsupervised clusters can be assigned to the same domain). In cases where randomness is involved we run each experiment five times with different initializations and report the mean and variance of the purity metric for each model.']\n",
            "['For MLM-based models we use BERT BIBREF10, DistilBERT BIBREF18 and RoBERTa BIBREF11 (in both the base and large versions). For autoregressive models we use GPT-2 BIBREF19 and XLNet BIBREF20. In all cases we use the implementations from the HuggingFace Transformers toolkit BIBREF21. We also evaluated three additional, simpler baselines. The first is using representations from word2vec BIBREF22, where we average-pooled the word vectors for the tokens that were present in the model vocabulary. The second is using Latent Dirichlet Allocation (LDA, BIBREF23), which is a classic approach to unsupervised clustering of text. We also report results for a baseline which assigns sentences by sampling randomly from a uniform distribution over the clusters.']\n",
            "['For MLM-based models we use BERT BIBREF10, DistilBERT BIBREF18 and RoBERTa BIBREF11 (in both the base and large versions). For autoregressive models we use GPT-2 BIBREF19 and XLNet BIBREF20. In all cases we use the implementations from the HuggingFace Transformers toolkit BIBREF21. We also evaluated three additional, simpler baselines. The first is using representations from word2vec BIBREF22, where we average-pooled the word vectors for the tokens that were present in the model vocabulary. The second is using Latent Dirichlet Allocation (LDA, BIBREF23), which is a classic approach to unsupervised clustering of text. We also report results for a baseline which assigns sentences by sampling randomly from a uniform distribution over the clusters.']\n",
            "['For MLM-based models we use BERT BIBREF10, DistilBERT BIBREF18 and RoBERTa BIBREF11 (in both the base and large versions). For autoregressive models we use GPT-2 BIBREF19 and XLNet BIBREF20. In all cases we use the implementations from the HuggingFace Transformers toolkit BIBREF21. We also evaluated three additional, simpler baselines. The first is using representations from word2vec BIBREF22, where we average-pooled the word vectors for the tokens that were present in the model vocabulary. The second is using Latent Dirichlet Allocation (LDA, BIBREF23), which is a classic approach to unsupervised clustering of text. We also report results for a baseline which assigns sentences by sampling randomly from a uniform distribution over the clusters.']\n",
            "['There is an effective and novel way to improve the performance of fake news detection combined with stance analysis, which is to build multi-task learning models to jointly train both tasks BIBREF13, BIBREF14, BIBREF15. These approaches model information sharing and representation reinforcement between the two tasks, which expands valuable features for their respective tasks. However, prominent drawback to these methods and even typical multi-task learning methods, like the shared-private model, is that the shared features in the shared layer are equally sent to their respective tasks without filtering, which causes that some useless and even adverse features are mixed in different tasks, as shown in Figure FIGREF2(a). By that the network would be confused by these features, interfering effective sharing, and even mislead the predictions.']\n",
            "['To address the above problems, we design a sifted multi-task learning model with filtering mechanism (Figure FIGREF2(b)) to detect fake news by joining stance detection task. Specifically, we introduce a selected sharing layer into each task after the shared layer of the model for filtering shared features. The selected sharing layer composes of two cells: gated sharing cell for discarding useless features and attention sharing cell for focusing on features that are conducive to their respective tasks. Besides, to better capture long-range dependencies and improve the parallelism of the model, we apply transformer encoder module BIBREF16 to our model for encoding input representations of both tasks. Experimental results reveal that the proposed model outperforms the compared methods and gains new benchmarks.']\n",
            "['To find out more about the length deficiency we constrained exact search to certain translation lengths. Constraining search that way increases the run time as the INLINEFORM0 -bounds are lower. Therefore, all results in this section are conducted on only a subset of the test set to keep the runtime under control. We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space. Although this mitigates the problem slightly (Fig. FIGREF16 ), it still results in a peak in the INLINEFORM1 cluster. This suggests that the problem of empty translations is the consequence of an inherent model bias towards shorter hypotheses and cannot be fixed with a length constraint.', 'We then constrained exact search to either the length of the best Beam-10 hypothesis or the reference length. Tab. TABREF18 shows that exact search constrained to the Beam-10 hypothesis length does not improve over beam search, suggesting that any search errors between beam search score and global best score for that length are insignificant enough so as not to affect the BLEU score. The oracle experiment in which we constrained exact search to the correct reference length (last row in Tab. TABREF18 ) improved the BLEU score by 0.9 points.']\n",
            "['We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl. Our pre-processing is as described by BIBREF15 and includes joint subword segmentation using byte pair encoding BIBREF16 with 32K merges. We report cased BLEU scores. An open-source implementation of our exact inference scheme is available in the SGNMT decoder BIBREF17 , BIBREF4 .']\n",
            "['We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl. Our pre-processing is as described by BIBREF15 and includes joint subword segmentation using byte pair encoding BIBREF16 with 32K merges. We report cased BLEU scores. An open-source implementation of our exact inference scheme is available in the SGNMT decoder BIBREF17 , BIBREF4 .']\n",
            "['We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl. Our pre-processing is as described by BIBREF15 and includes joint subword segmentation using byte pair encoding BIBREF16 with 32K merges. We report cased BLEU scores. An open-source implementation of our exact inference scheme is available in the SGNMT decoder BIBREF17 , BIBREF4 .']\n",
            "['As in earlier NLG researches, we use the BLEU-4 score BIBREF26 and the slot error rate (ERR) as evaluation metrics. ERR is computed by the ratio of the sum of the number of missing and redundant slots in a generated utterance divided by the total number of slots in the DA. We randomly sampled target low-resource task five times for each experiment and reported the average score.']\n",
            "['Metrics: Given a DA and a reference utterance in a low-resource target domain with adaptation size 500, two responses generated by Meta-NLG and MTL-NLG were presented to three human annotators to score each of them in terms of informativeness and naturalness (rating out of 3), and also indicate their pairwise preferences (Win-Tie-Lose) on Meta-NLG against MTL-NLG. Informativeness is defined as whether the generated utterance captures all the information, including multiple slots and probably multiple DA types, specified in the DA. Naturalness measures whether the utterance is plausibly generated by a human.']\n",
            "['As in earlier NLG researches, we use the BLEU-4 score BIBREF26 and the slot error rate (ERR) as evaluation metrics. ERR is computed by the ratio of the sum of the number of missing and redundant slots in a generated utterance divided by the total number of slots in the DA. We randomly sampled target low-resource task five times for each experiment and reported the average score.', 'Metrics: Given a DA and a reference utterance in a low-resource target domain with adaptation size 500, two responses generated by Meta-NLG and MTL-NLG were presented to three human annotators to score each of them in terms of informativeness and naturalness (rating out of 3), and also indicate their pairwise preferences (Win-Tie-Lose) on Meta-NLG against MTL-NLG. Informativeness is defined as whether the generated utterance captures all the information, including multiple slots and probably multiple DA types, specified in the DA. Naturalness measures whether the utterance is plausibly generated by a human.']\n",
            "['We included different model settings as baseline:', 'Scratch-NLG: Train INLINEFORM0 with only low-resource target task data, ignoring all high-resource source task data.', 'MTL-NLG: Train INLINEFORM0 using a multi-task learning paradigm with source task data, then fine-tune on the low-resource target task.', 'Zero-NLG: Train INLINEFORM0 using multi-task learning (MTL) with source task data, then directly test on a target task without a fine-tuning step. This corresponds to a zero-shot learning scenario.', 'Supervised-NLG: Train INLINEFORM0 using MTL with full access to high-resource data from both source and target tasks. Its performance serves an upper bound using multi-task learning without the low-resource restriction.']\n",
            "['Scratch-NLG: Train INLINEFORM0 with only low-resource target task data, ignoring all high-resource source task data.', 'MTL-NLG: Train INLINEFORM0 using a multi-task learning paradigm with source task data, then fine-tune on the low-resource target task.', 'Zero-NLG: Train INLINEFORM0 using multi-task learning (MTL) with source task data, then directly test on a target task without a fine-tuning step. This corresponds to a zero-shot learning scenario.', 'Supervised-NLG: Train INLINEFORM0 using MTL with full access to high-resource data from both source and target tasks. Its performance serves an upper bound using multi-task learning without the low-resource restriction.']\n",
            "[\"Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of “SUPPORTS” label against a supervised approach – HexaF. Results from Table TABREF17 suggests that our approach is comparable to HexaF for $\\\\phi $ = 0.76.\"]\n",
            "[\"Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of “SUPPORTS” label against a supervised approach – HexaF. Results from Table TABREF17 suggests that our approach is comparable to HexaF for $\\\\phi $ = 0.76.\"]\n",
            "['Table TABREF16 shows the performance of our Fact Checking system on the “SUPPORTS” label, the output of our system. We compare the results against two different classification thresholds. Table TABREF3 shows that on an average there are 3 questions generated per claim. Here, $\\\\phi $ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while $\\\\phi $ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as “SUPPORTS”.']\n",
            "['Table TABREF16 shows the performance of our Fact Checking system on the “SUPPORTS” label, the output of our system. We compare the results against two different classification thresholds. Table TABREF3 shows that on an average there are 3 questions generated per claim. Here, $\\\\phi $ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while $\\\\phi $ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as “SUPPORTS”.']\n",
            "[\"In our case, FEVER claims are derived from Wikipedia. We first collect all the claims from the FEVER dataset along with “id”, “label” and “verifiable” fields. We don't perform any normalization on the claims such as lowercasing, transforming the spaces to underscore or parenthesis to special characters as it may decrease the accuracy of the NER tagger. These claims are then processed by the NER tagger to identify the named entities and their type. The named entities are then used to generate the questions by masking the entities for the subsequent stage.\"]\n",
            "['In this section, we explain the design and all the underlying methods that our system has adopted. Our system is a pipeline consisting of three stages: (1) Question Generation, (2) Question Answering, (3) Label Classification. The question generation stage attempts to convert the claims into appropriate questions and answers. It generates questions similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank. Question Answering stage predicts the masked blanks in an unsupervised manner. The respective predictions are then compared with the original answers and exported into a file for label classification. The label classifier calculates the predicted label based on a threshold.']\n",
            "['Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO”, based on the evidence set.']\n",
            "['Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO”, based on the evidence set.']\n",
            "['We now present a dataset developed for the task. Our dataset is derived from a recent large table-to-document corpus BIBREF0 which consists of box-score tables of NBA basketball games and associated documents as game reports. The corpus is originally used for studying supervised game report generation which has attracted increasing research interest BIBREF18 , BIBREF0 .', 'To obtain our data, we first split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record. A record can contain a varying number of tuples, with each tuple containing three fields, namely a data type, a value, and an associated player or team, e.g., (team_points, 106, Lakers). As the original corpus is already largely clean, we found some simple rules are sufficient to obtain high-quality results in this step. Please see the supplementary materials for more details. Each of the resulting record-sentence pairs is treated as a pair of INLINEFORM0 , namely (content record, auxiliary sentence). The next step is to find a suitable reference sentence INLINEFORM1 for each content record INLINEFORM2 . As defined above, the reference sentence should cover similar but not the same content as in record INLINEFORM3 . We achieve this by retrieving from the data another record-sentence pair using INLINEFORM4 , where the retrieved record is designated to have a slightly different structure than that of INLINEFORM5 by having less or more tuples and different data types. More details of the retrieval method are deferred to supplements. The retrieved record-sentence pair thus plays the role of INLINEFORM6 and is paired with INLINEFORM7 to form an instance.']\n",
            "['Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes. To apply to our setting, we treat content record INLINEFORM0 as the attributes. The method is based on back-translation BIBREF23 that first generates a target sentence INLINEFORM1 conditioning on INLINEFORM2 , and then treat it as the reference to reconstruct INLINEFORM3 conditioning on INLINEFORM4 . Auxiliary sentence INLINEFORM5 is used in an extra auto-encoding loss.', 'Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations.']\n",
            "['We compare with a diverse set of approaches:', '[leftmargin=*]', 'AttnCopy-S2S. We first evaluate a base sequence-to-sequence BIBREF22 model with the above attention-copy mechanism, which takes in record INLINEFORM0 and generates its descriptive sentence INLINEFORM1 . The evaluation provides a sense of the difficulty in describing desired content.', 'Rule-based Method. A straightforward way for text content manipulation is to match between INLINEFORM0 , INLINEFORM1 and INLINEFORM2 with certain rules, and replace corresponding portions in INLINEFORM3 with those in INLINEFORM4 . Specifically, we first build a mapping between the tuples of INLINEFORM5 and INLINEFORM6 through their data types, and a mapping between INLINEFORM7 and INLINEFORM8 through data values, types and indicative tokens (e.g., “12 points” in INLINEFORM9 indicates 12 is of type player points or team_points). The two mappings connect INLINEFORM10 and INLINEFORM11 , enabling us to swap appropriate text in INLINEFORM12 to express content INLINEFORM13 .', 'In theory, rule-based method sets the best possible style preservation performance, as it only replaces content related tokens (particularly numbers) without modifying other parts of the reference sentence. The output, however, tends to miss or contain extra content compared to the content record of interest.', 'Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes. To apply to our setting, we treat content record INLINEFORM0 as the attributes. The method is based on back-translation BIBREF23 that first generates a target sentence INLINEFORM1 conditioning on INLINEFORM2 , and then treat it as the reference to reconstruct INLINEFORM3 conditioning on INLINEFORM4 . Auxiliary sentence INLINEFORM5 is used in an extra auto-encoding loss.', 'Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations.']\n",
            "['Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes. To apply to our setting, we treat content record INLINEFORM0 as the attributes. The method is based on back-translation BIBREF23 that first generates a target sentence INLINEFORM1 conditioning on INLINEFORM2 , and then treat it as the reference to reconstruct INLINEFORM3 conditioning on INLINEFORM4 . Auxiliary sentence INLINEFORM5 is used in an extra auto-encoding loss.', 'Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations.']\n",
            "['Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall. Table 4 shows the results of our system on the test set of 2016 and 2017.']\n",
            "['As shown in Table TABREF11, our method has the highest IS and MP values on both the CUB and COCO datasets compared with the state-of-the-art approaches, which demonstrates that (1) our method can produce high-quality manipulated results, and (2) our method can better generate new attributes matching the given text, and also effectively reconstruct text-irrelevant contents of the original image.', 'Our model is evaluated on the CUB bird BIBREF10 and more complicated COCO BIBREF11 datasets, comparing with two state-of-the-art approaches SISGAN BIBREF8 and TAGAN BIBREF9 on image manipulation using natural language descriptions.']\n",
            "['Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .']\n",
            "['Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .']\n",
            "['We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.']\n",
            "['We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.']\n",
            "['Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.']\n",
            "['Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.']\n",
            "['Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.']\n",
            "['Our dataset comprises of the following - 1) Occupation Data 2) Names Data. We will iterate over each of this one by one.', 'Occupation Data: We gathered occupation lists from different sources on the internet including crowdsourced lists and government lists. Then, we classified the occupations into 2 categories - gender-specific occupation and gender-neutral occupations. These are used in the algorithm for bias checking which will be explained in the next sub-section.', 'Names Data: We created a corpus of 5453 male and 6990 female names sourced from [ref: CMU repository of names]. For the dataset to map names to a gender, we referred to the NLTK data set and the records of baby names and their genders.']\n",
            "['We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:', 'Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:', 'To examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:', 'Model proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:', 'Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):', 'Model proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):', 'For this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):', 'Given the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected.']\n",
            "['We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:', 'Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:', 'To examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:', 'Model proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.', 'Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:', 'Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):', 'Model proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):', 'For this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.', 'Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):', 'Given the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected.']\n",
            "['We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset. For each dataset, we randomly select 200 instances as a held-out test set for evaluation.']\n",
            "['We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset. For each dataset, we randomly select 200 instances as a held-out test set for evaluation.']\n",
            "['We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset. For each dataset, we randomly select 200 instances as a held-out test set for evaluation.', 'Datasets ::: Gender-Specific Dialogue Dataset', 'We use a publicly available gender-specific dialogue dataset BIBREF33. In this dataset, each response contains one specific gender preference including Female, Male and Neutral.', 'Datasets ::: Emotion-Specific Dialogue Dataset', 'We use a publicly available emotion-specific dataset BIBREF11 which contains responses with 6 different emotions including Like, Disgust, Happy, Anger, Sad and Other.', 'Datasets ::: Sentiment-Specific Dialogue Dataset', 'To construct this dataset, we first build a classifier on the basis of BERT BIBREF34 and finetuned it on the the SemEval-2017 Subtask A dataset BIBREF35. This dataset consists of twitter instances with different sentiments including Positive, Negative and Neutral.']\n",
            "['While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare’s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare’s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (“The view of earthly glory: men might say”) to lines ending with an extra unstressed one (“Till this time pomp was single, but now married”), pointing out that the distribution of values across scenes is strongly bimodal.']\n",
            "['While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare’s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare’s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (“The view of earthly glory: men might say”) to lines ending with an extra unstressed one (“Till this time pomp was single, but now married”), pointing out that the distribution of values across scenes is strongly bimodal.']\n",
            "['Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.']\n",
            "['More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding’s original attribution concerning re-attribution of several parts of supposedly Fletcher’s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare’s and Fletcher’s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding’s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.']\n",
            "['More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding’s original attribution concerning re-attribution of several parts of supposedly Fletcher’s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare’s and Fletcher’s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding’s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.']\n",
            "['More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding’s original attribution concerning re-attribution of several parts of supposedly Fletcher’s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare’s and Fletcher’s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding’s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.']\n",
            "['paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.', \"Next, the work on surrogate data has recently seen a surge in activity. Increasingly more health-related texts are produced in social media BIBREF40 , and patient-generated data are available online. Admittedly, these may not resemble the clinical discourse, yet they bear to the same individuals whose health is documented in the clinical reports. Indeed, linking individuals' health information from online resources to their health records to improve documentation is an active line of research BIBREF41 . Although it is generally easier to obtain access to social media data, the use of social media still requires similar ethical considerations as in the clinical domain. See for example the influential study on emotional contagion in Facebook posts by Kramer et al. KramerEtAl2014, which has been criticized for not properly gaining prior consent from the users who were involved in the study BIBREF42 .\", 'Another way of reducing sensitivity of data and improving chances for IRB approval is to work on derived data. Data that can not be used to reconstruct the original text (and when sanitized, can not directly re-identify the individual) include text fragments, various statistics and trained models. Working on randomized subsets of clinical notes may also improve the chances of obtaining the data. When we only have access to trained models from disparate sources, we can refine them through ensembling and creation of silver standard corpora, cf. Rebholz-Schuhmann et al. RebholzSchuhmannEtAl2011.', 'Finally, clinical NLP is also possible on veterinary texts. Records of companion animals are perhaps less likely to involve legal issues, while still amounting to a large pool of data. As an example, around 40M clinical documents from different veterinary clinics in UK and Australia are stored centrally in the VetCompass repository. First NLP steps in this direction were described in the invited talk at the Clinical NLP 2016 workshop BIBREF43 .']\n",
            "['paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.', \"Next, the work on surrogate data has recently seen a surge in activity. Increasingly more health-related texts are produced in social media BIBREF40 , and patient-generated data are available online. Admittedly, these may not resemble the clinical discourse, yet they bear to the same individuals whose health is documented in the clinical reports. Indeed, linking individuals' health information from online resources to their health records to improve documentation is an active line of research BIBREF41 . Although it is generally easier to obtain access to social media data, the use of social media still requires similar ethical considerations as in the clinical domain. See for example the influential study on emotional contagion in Facebook posts by Kramer et al. KramerEtAl2014, which has been criticized for not properly gaining prior consent from the users who were involved in the study BIBREF42 .\", 'Another way of reducing sensitivity of data and improving chances for IRB approval is to work on derived data. Data that can not be used to reconstruct the original text (and when sanitized, can not directly re-identify the individual) include text fragments, various statistics and trained models. Working on randomized subsets of clinical notes may also improve the chances of obtaining the data. When we only have access to trained models from disparate sources, we can refine them through ensembling and creation of silver standard corpora, cf. Rebholz-Schuhmann et al. RebholzSchuhmannEtAl2011.', 'Finally, clinical NLP is also possible on veterinary texts. Records of companion animals are perhaps less likely to involve legal issues, while still amounting to a large pool of data. As an example, around 40M clinical documents from different veterinary clinics in UK and Australia are stored centrally in the VetCompass repository. First NLP steps in this direction were described in the invited talk at the Clinical NLP 2016 workshop BIBREF43 .']\n",
            "['Unlocking knowledge from free text in the health domain has a tremendous societal value. However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. The question is therefore what the most important biases are and how to overcome them, not only out of ethical but also legal responsibility. Related to the question of bias is so-called algorithm transparency BIBREF44 , BIBREF45 , as this right to explanation requires that influences of bias in training data are charted. In addition to sampling bias, which we introduced in section 2, we discuss in this section further sources of bias. Unlike sampling bias, which is a corpus-level bias, these biases here are already present in documents, and therefore hard to account for by introducing larger corpora.', 'paragraph4 0.9ex plus1ex minus.2ex-1em Data quality Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made BIBREF47 . If we fail to detect a medical concept during automated processing, this can not necessarily be a sign of negative evidence. Work on identifying and imputing missing values holds promise for reducing incompleteness, see Lipton et al. LiptonEtAl2016 for an example in sequential modeling applied to diagnosis classification.', \"paragraph4 0.9ex plus1ex minus.2ex-1em Reporting bias Clinical texts may include bias coming from both patient's and clinician's reporting. Clinicians apply their subjective judgments to what is important during the encounter with patients. In other words, there is separation between, on the one side, what is observed by the clinician and communicated by the patient, and on the other, what is noted down. Cases of more serious illness may be more accurately documented as a result of clinician's bias (increased attention) and patient's recall bias. On the other hand, the cases of stigmatized diseases may include suppressed information. In the case of traffic injuries, documentation may even be distorted to avoid legal consequences BIBREF48 .\", 'We need to be aware that clinical notes may reflect health disparities. These can originate from prejudices held by healthcare practitioners which may impact patients\\' perceptions; they can also originate from communication difficulties in the case of ethnic differences BIBREF49 . Finally, societal norms can play a role. Brady et al. BradyEtAl2016 find that obesity is often not documented equally well for both sexes in weight-addressing clinics. Young males are less likely to be recognized as obese, possibly due to societal norms seeing them as “stocky\" as opposed to obese. Unless we are aware of such bias, we may draw premature conclusions about the impact of our results.', 'paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes.', \"paragraph4 0.9ex plus1ex minus.2ex-1em Dual use We have already mentioned linking personal health information from online texts to clinical records as a motivation for exploring surrogate data sources. However, this and many other applications also have potential to be applied in both beneficial and harmful ways. It is easy to imagine how sensitive information from clinical notes can be revealed about an individual who is present in social media with a known identity. More general examples of dual use are when the NLP tools are used to analyze clinical notes with a goal of determining individuals' insurability and employability.\"]\n",
            "['To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely', '[leftmargin=*]', 'EM INLINEFORM0 : evaluates the overall accuracy of the generated outline based on exact matching. That is, if both the predicted section boundaries and the generated section headings in a document exactly match with the ground-truth, we treat the document as a positive sample. Otherwise the document is a negative sample.', 'EM INLINEFORM0 : evaluates the accuracy of the section boundary prediction based on exact matching. Namely, if the predicted section boundaries in a document exactly match with the ground-truth, we treat the document as a positive sample. Otherwise the document is a negative sample.', 'Rouge INLINEFORM0 evaluates the similarities between generated headings and referenced headings only for the correctly predicted sections. Specifically, we employ Rouge-1 BIBREF39 to measure the uni-gram recall on the reference headings.']\n",
            "['Affect-LM can be used to generate sentences conditioned on the input affect category, the affect strength $\\\\beta $ , and the context words. For our experiments, we have chosen the following affect categories - positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety). As described in Section \"Conclusions and Future Work\" , the affect strength $\\\\beta $ defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently after model training we can change $\\\\beta $ to control the degree of how “emotionally colored\" a generated utterance is, varying from $\\\\beta =0$ (neutral; baseline model) to $\\\\beta =\\\\infty $ (the generated sentences only consist of emotionally colored words, with no grammatical structure). When Affect-LM is used for generation, the affect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\\\mathbf {e}$ (this is obtained by setting $\\\\mathbf {e}$ to a binary vector encoding the desired emotion and works even for neutral sentence beginnings). Given an initial starting set of $M$ words $w_1,w_2,...,w_M$ to complete, affect strength $\\\\beta $ , and the number of words $\\\\beta $0 to generate each $\\\\beta $1 -th generated word is obtained by sampling from $\\\\beta $2 for $\\\\beta $3 .', 'Our proposed model learns a generative model of the next word $w_t$ conditioned not only on the previous words $w_1,w_2,...,w_{t-1}$ but also on the affect category $\\\\mathbf {e_{t-1}}$ which is additional information about emotional content. During model training, the affect category is inferred from the context data itself. Thus we define a suitable feature extractor which can utilize an affective lexicon to infer emotion in the context. For our experiments, we have utilized the Linguistic Inquiry and Word Count (LIWC) text analysis program for feature extraction through keyword spotting. Introduced by BIBREF11 pennebaker2001linguistic, LIWC is based on a dictionary, where each word is assigned to a predefined LIWC category. The categories are chosen based on their association with social, affective, and cognitive processes. For example, the dictionary word worry is assigned to LIWC category anxiety. In our work, we have utilized all word categories of LIWC corresponding to affective processes: positive emotion, angry, sad, anxious, and negative emotion. Thus the descriptor $\\\\mathbf {e_{t-1}}$ has five features with each feature denoting presence or absence of a specific emotion, which is obtained by binary thresholding of the features extracted from LIWC. For example, the affective representation of the sentence i will fight in the war is $\\\\mathbf {e_{t-1}}=$ {“sad\":0, “angry\":1, “anxiety\":0, “negative emotion\":1, “positive emotion\":0}.']\n",
            "['We depart from BERT BIBREF12, and we design three baselines.', 'BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.', 'BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).', 'BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the $L_{g_2}$ layer for FLC, we concatenate $L_{g_1}$ and $L_{g_2}$, and we add an extra 19-dimensional classification layer $L_{g_{1,2}}$ on top of that concatenation to perform the prediction for FLC (cf. Figure FIGREF7-c).']\n",
            "['We depart from BERT BIBREF12, and we design three baselines.', 'BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.', 'BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).', 'BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the $L_{g_2}$ layer for FLC, we concatenate $L_{g_1}$ and $L_{g_2}$, and we add an extra 19-dimensional classification layer $L_{g_{1,2}}$ on top of that concatenation to perform the prediction for FLC (cf. Figure FIGREF7-c).']\n",
            "[\"We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience —such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6— to using logical fallacies —such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data). Some of these techniques weren studied in tasks such as hate speech detection and computational argumentation BIBREF9.\"]\n",
            "[\"We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience —such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6— to using logical fallacies —such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data). Some of these techniques weren studied in tasks such as hate speech detection and computational argumentation BIBREF9.\"]\n",
            "[\"We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience —such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6— to using logical fallacies —such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data). Some of these techniques weren studied in tasks such as hate speech detection and computational argumentation BIBREF9.\"]\n",
            "['Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post–evaluation runs than the tweet data.']\n",
            "['Table 2 shows results from the development stage. These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization. While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548). We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543). For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive. Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.']\n",
            "['Table 2 shows results from the development stage. These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization. While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548). We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543). For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive. Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.']\n",
            "['The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first. If the scores are based on the tweet language model then they are sorted in ascending order since the log probability value closest to 0 indicates the tweet that is most like the (funny) tweets model. However, if the log probability scores are based on the news data then they are sorted in descending order since the largest value will have the smallest probability associated with it and is therefore least like the (unfunny) news model.']\n",
            "['After training the N-gram language models, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. Table 1 shows an example of two scored tweets from hashtag file Bad_Job_In_5_Words.tsv based on the tweet data trigram language model. Note that KenLM reports the log of the probability of the N-grams rather than the actual probabilities so the value closer to 0 (-19) has the higher probability and is associated with the tweet judged to be funnier.', 'The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first. If the scores are based on the tweet language model then they are sorted in ascending order since the log probability value closest to 0 indicates the tweet that is most like the (funny) tweets model. However, if the log probability scores are based on the news data then they are sorted in descending order since the largest value will have the smallest probability associated with it and is therefore least like the (unfunny) news model.']\n",
            "['Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.']\n",
            "['Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.']\n",
            "['Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.']\n",
            "['For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a “1”. If the second tweet is funnier it outputs the tweet_ids followed by a “0”. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest.']\n",
            "['The contexts in which males and females are written about can differ; for instance, on Wikipedia women are more often written about with words related to sexuality than men BIBREF25. Counterfactual Data Augmentation (CDA) mitigates these contextual biases. CDA consists of replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, then training on the union of the original and augmented corpora . This equalizes the contexts for feminine and masculine words; if previously 100 doctors were referred to as he and 50 as she, in the new training set he and she will refer to doctor 150 times each.']\n",
            "['Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations. While gender-swapping did slightly better at decreasing that difference for the spouse relation, debiased embeddings mitigated bias better for the birthDate and hypernym relations. We note that using debiased embeddings increases absolute scores just like gender-swapping, though it increases them slightly less.']\n",
            "['Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations. While gender-swapping did slightly better at decreasing that difference for the spouse relation, debiased embeddings mitigated bias better for the birthDate and hypernym relations. We note that using debiased embeddings increases absolute scores just like gender-swapping, though it increases them slightly less.']\n",
            "[\"Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.\"]\n",
            "[\"Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.\"]\n",
            "[\"To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation and we find the sentence He and Michelle were married in Barack's Wikipedia article, then we assume that sentence expresses the (Barack, spouse, Michelle) relation. This assumption is similar to that made by BIBREF20 and allows us to scalably create the dataset.\"]\n",
            "[\"To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation and we find the sentence He and Michelle were married in Barack's Wikipedia article, then we assume that sentence expresses the (Barack, spouse, Michelle) relation. This assumption is similar to that made by BIBREF20 and allows us to scalably create the dataset.\"]\n",
            "['Stance annotation. We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim. An ETS was only considered to express a stance if it explicitly referred to the claim and either expressed support for it or refuted it. In all other cases, the ETS was considered as having no stance.']\n",
            "['Stance annotation. We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim. An ETS was only considered to express a stance if it explicitly referred to the claim and either expressed support for it or refuted it. In all other cases, the ETS was considered as having no stance.']\n",
            "['Stance annotation. We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim. An ETS was only considered to express a stance if it explicitly referred to the claim and either expressed support for it or refuted it. In all other cases, the ETS was considered as having no stance.']\n",
            "['Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims. A simple fact-checking instance from the Snopes website is shown in Figure FIGREF14. At the top of the page, the claim and the verdict (rating) are given. The fact-checkers additionally provide a resolution (origin), which backs up the verdict. Evidence in the resolution, which we call evidence text snippets (ETSs), is marked with a yellow bar. As additional validation support, Snopes fact-checkers provide URLs for original documents (ODCs) from which the ETSs have been extracted or which provide additional information.', 'Our crawler extracts the claims, verdicts, ETSs, the resolution, as well as ODCs along with their URLs, thereby enriching the ETSs with useful contextual information. Snopes is almost entirely focused on claims made on English speaking websites. Our corpus therefore only features English fact-checking instances.']\n",
            "['Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims. A simple fact-checking instance from the Snopes website is shown in Figure FIGREF14. At the top of the page, the claim and the verdict (rating) are given. The fact-checkers additionally provide a resolution (origin), which backs up the verdict. Evidence in the resolution, which we call evidence text snippets (ETSs), is marked with a yellow bar. As additional validation support, Snopes fact-checkers provide URLs for original documents (ODCs) from which the ETSs have been extracted or which provide additional information.', 'Our crawler extracts the claims, verdicts, ETSs, the resolution, as well as ODCs along with their URLs, thereby enriching the ETSs with useful contextual information. Snopes is almost entirely focused on claims made on English speaking websites. Our corpus therefore only features English fact-checking instances.']\n",
            "['In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.']\n",
            "['3) For evidence extraction, stance detection, and claim validation we evaluate the performance of high-scoring systems from the FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8 as well as the Bidirectional Transformer model BERT BIBREF9 on our data. To facilitate the development of future fact-checking systems, we release the code of our experiments.']\n",
            "['Related work', 'Below, we give a comprehensive overview of existing fact-checking corpora, summarized in Table TABREF7. We focus on their key parameters: fact-checking sub-task coverage, annotation quality, corpus size, and domain. It must be acknowledged that a fair comparison between the datasets is difficult to accomplish since the length of evidence and documents, as well as the annotation quality, significantly varies between the corpora.', 'PolitiFact14 BIBREF4 analyzed the fact-checking problem and constructed a corpus on the basis of the fact-checking blog of Channel 4 and the Truth-O-Meter from PolitiFact. The corpus includes additional evidence, which has been used by fact-checkers to validate the claims, as well as metadata including the speaker ID and the date when the claim was made. This is early work in automated fact-checking and BIBREF4 mainly focused on the analysis of the task. The corpus therefore only contains 106 claims, which is not enough to train high-performing machine learning systems.', \"Emergent16 A more comprehensive corpus for automated fact-checking was introduced by BIBREF5. The dataset is based on the project Emergent which is a journalist initiative for rumor debunking. It consists of 300 claims that have been validated by journalists. The corpus provides 2,595 news articles that are related to the claims. Each article is summarized into a headline and is annotated with the article's stance regarding the claim. The corpus is well suited for training stance detection systems in the news domain and it was therefore chosen in the Fake News Challenge BIBREF8 for training and evaluation of competing systems. However, the number of claims in the corpus is relatively small, thus it is unlikely that sophisticated claim validation systems can be trained using this corpus.\", 'PolitiFact17 BIBREF10 extracted 12,800 validated claims made by public figures in various contexts from Politifact. For each statement, the corpus provides a verdict and meta information, such as the name and party affiliation of the speaker or subject of the debate. Nevertheless, the corpus does not include evidence and thus the models can only be trained on the basis of the claim, the verdict, and meta information.', 'RumourEval17 BIBREF6 organized the RumourEval shared task, for which they provided a corpus of 297 rumourous threads from Twitter, comprising 4,519 tweets. The shared task was divided into two parts, stance detection and veracity prediction of the rumors, which is similar to claim validation. The large number of stance-annotated tweets allows for training stance detection systems reaching a relatively high score of about 0.78 accuracy. However, since the number of claims (rumours) is relatively small, and the corpus is only based on tweets, this dataset alone is not suitable to train generally applicable claim validation systems.', 'Snopes17 A corpus featuring a substantially larger number of validated claims was introduced by BIBREF2. It contains 4,956 claims annotated with verdicts which have been extracted from the Snopes website as well as the Wikipedia collections of proven hoaxes and fictitious people. For each claim, the authors extracted about 30 associated documents using the Google search engine, resulting in a collection of 136,085 documents. However, since the documents were not annotated by fact-checkers, irrelevant information is present and important information for the claim validation might be missing.', 'CLEF-2018 Another corpus concerned with political debates was introduced by BIBREF11 and used for the CLEF-2018 shared task. The corpus consists of transcripts of political debates in English and Arabic and provides annotations for two tasks: identification of check-worthy statements (claims) in the transcripts, and validation of 150 statements (claims) from the debates. However, as for the corpus PolitiFact17, no evidence for the validation of these claims is available.', 'FEVER18 The FEVER corpus introduced by BIBREF1 is the largest available fact-checking corpus, consisting of 185,445 validated claims. The corpus is based on about 50k popular Wikipedia articles. Annotators modified sentences in these articles to create the claims and labeled other sentences in the articles, which support or refute the claim, as evidence. The corpus is large enough to train deep learning systems able to retrieve evidence from Wikipedia. Nevertheless, since the corpus only covers Wikipedia and the claims are created synthetically, the trained systems are unlikely to be able to extract evidence from heterogeneous web-sources and validate claims on the basis of evidence found on the Internet.']\n",
            "['In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.']\n",
            "['We report the performance of the following models: AtheneMLP is a feature-based multi-layer perceptron BIBREF19, which has reached the second rank in the Fake News Challenge. DecompAttent BIBREF20 is a neural network with a relatively small number of parameters that uses decomposable attention, reaching good results on the Stanford Natural Language Inference task BIBREF21. USE+Attent is a model which uses the Universal Sentence Encoder (USE) BIBREF22 to extract representations for the sentences of the ETSs and the claim. For the classification of the stance, an attention mechanism and a MLP is used.']\n",
            "['In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.']\n",
            "['In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.']\n",
            "[\"Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\\\kappa = 0.7$. The agreement between the experts' annotations and the computed gold annotations from the crowd workers is also substantial, $\\\\kappa = 0.683$.\", \"FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15.\"]\n",
            "['Figure 1 shows the validation curve while training. Perplexity values from various model output are plotted. The perplexity of baseline model, “scheme_1\", decreases until around epoch 10, and then it starts to increase because model is over-fitted to training data. The proposed “scheme_2\" and “scheme_3\", however, show continuous decreasing tendency and reach lower perplexity values compared to that of the baseline model. It is interesting that proposed methods achieve lower perplexity than baseline while saving computing power with reduced parameters.', 'The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from “Friends,\" a famous American television sitcom. In the figures, “character_1\" to “character_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character\\'s dialogue was calculated, and the higher value was calculated among the different character\\'s dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set.']\n",
            "[\"To resolve these limitations, we propose fast transfer learning schemes. It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are proposed for achieving the result. In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model. With this approach, the final model can mimic target user's language style with proper syntax.\", 'In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, “Friends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users\\' dialogues.', \"We also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT'14 corpus for 10 days, is used. English bible data is added and employed in training for another 4 hours using proposed transfer learning schemes.\"]\n",
            "['FLOAT SELECTED: Table 2: Sample model output of general language model and personalized language model. The general language model used WMT’14 data, personalized language model 1 used “Friends” drama data, and personalized language model 2 used the English bible data. Scheme 1 to scheme 3 are relearn whole, surplus layer and fixed-n layer, respectively. The output was generated with the given input sequence, “It is possible, however”']\n",
            "['Table 5 indicates the cross entropy measure between the output of “scheme_1\" to “scheme_3\" model and that of the target corpus, the “friends\" drama corpus, the “chandler\" corpus, and the “bible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the “Friends” corpus and the “Chandler” corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the “bible” as expected because it is written in different style than dialogues in drama script. For the drama script case, the cross entropy measured with the “chandler\" corpus shows the lowest value among schemes. This result reveals that the personalized language model is trained properly from the general language model. Thus it is more similar in style to the target data corpus than the general language model. The “epoch 0\" case means the initial model state trained from general language corpus, “friends\" corpus. Thus cross entropy with “friends\" target corpus shows lower value than that of “chandler\" and “bible\" target corpus cases.']\n",
            "['In this work, we scale the investigation of category learning and representation along two axes: (1) the complexity of the learning environment, and consequently the richness of learnable concept and category representations, and (2) the diversity of languages and cultures considered in evaluation. We present a novel knowledge-lean, cognitively motivated Bayesian model which learns categories and their structured features jointly from large natural language text corpora in five diverse languages: Arabic, Chinese, English, French, and German. We approximate the learning environment using large corpora of natural language text. Language has been shown to redundantly encode much of the non-linguistic information in the natural environment BIBREF20 , and to influence the emergence of categories BIBREF4 , BIBREF5 . Besides text corpora can cover arbitrarily semantically complex domains, and are available across languages, providing an ideal test environment for studying categorization at scale.', 'Our work exemplifies the opportunities that arise from computational models and large data sets for investigating the mechanisms with which conceptual representations emerge, as well as the representations themselves in a broader context. We simulate the acquisition of categories comprising hundreds of concepts by approximating the learning environment with natural language text. Language has been shown to redundantly encode much of the non-linguistic information in the natural environment BIBREF20 , as well as human-like biases BIBREF33 , and to influence the emergence of categories BIBREF4 , BIBREF5 . Text corpora are a prime example of naturally occurring large-scale data sets BIBREF34 , BIBREF35 , BIBREF36 . In analogy to real-world situations, they encapsulate rich, diverse, and potentially noisy, information. The wide availability of corpora allows us to train and evaluate cognitive models on data from diverse languages and cultures. We test our model on corpora from five languages, derived from the online encyclopedia Wikipedia in Arabic, Chinese, French, English, and German. Wikipedia is a valuable resource for our study because it (a) discusses concepts and their properties explicitly and can thus serve as a proxy for the environment speakers of a language are exposed to; and (b) allows us to construct corpora which are highly comparable in their content across languages, controlling for effects of genre or style.']\n",
            "['Our work exemplifies the opportunities that arise from computational models and large data sets for investigating the mechanisms with which conceptual representations emerge, as well as the representations themselves in a broader context. We simulate the acquisition of categories comprising hundreds of concepts by approximating the learning environment with natural language text. Language has been shown to redundantly encode much of the non-linguistic information in the natural environment BIBREF20 , as well as human-like biases BIBREF33 , and to influence the emergence of categories BIBREF4 , BIBREF5 . Text corpora are a prime example of naturally occurring large-scale data sets BIBREF34 , BIBREF35 , BIBREF36 . In analogy to real-world situations, they encapsulate rich, diverse, and potentially noisy, information. The wide availability of corpora allows us to train and evaluate cognitive models on data from diverse languages and cultures. We test our model on corpora from five languages, derived from the online encyclopedia Wikipedia in Arabic, Chinese, French, English, and German. Wikipedia is a valuable resource for our study because it (a) discusses concepts and their properties explicitly and can thus serve as a proxy for the environment speakers of a language are exposed to; and (b) allows us to construct corpora which are highly comparable in their content across languages, controlling for effects of genre or style.']\n",
            "['In this work, we scale the investigation of category learning and representation along two axes: (1) the complexity of the learning environment, and consequently the richness of learnable concept and category representations, and (2) the diversity of languages and cultures considered in evaluation. We present a novel knowledge-lean, cognitively motivated Bayesian model which learns categories and their structured features jointly from large natural language text corpora in five diverse languages: Arabic, Chinese, English, French, and German. We approximate the learning environment using large corpora of natural language text. Language has been shown to redundantly encode much of the non-linguistic information in the natural environment BIBREF20 , and to influence the emergence of categories BIBREF4 , BIBREF5 . Besides text corpora can cover arbitrarily semantically complex domains, and are available across languages, providing an ideal test environment for studying categorization at scale.']\n",
            "[\"In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.\"]\n",
            "[\"In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.\"]\n",
            "['The power of neural networks comes from their ability to find data representations that are useful for classification. Recurrent Neural Networks (RNN) are a special type of neural network, which can be thought of as the addition of loops to the architecture. RNNs use back propagation in the training process to update the network weights in every layer. In our experimentation we used a powerful type of RNN known as Long Short-Term Memory Network (LSTM). Inspired by the work by BIBREF15 , we experiment with combining various LSTM models enhanced with a number of novel features in an ensemble. More specifically we introduce:']\n",
            "[\"As can be seen in Table TABREF24 , the work by BIBREF12 , in which character n-grams and gender information were used as features, obtained the quite low F-score of INLINEFORM0 . Later work by the same author BIBREF5 investigated the impact of the experience of the annotator in the performance, but still obtaining a lower F-score than ours. Furthermore, while the first part of the two step classification BIBREF16 performs quite well (reported an F-score of 0.9520), it falls short in detecting the particular class the abusive text belongs to. Finally, we observe that applying a simple LSTM classification with no use of additional features (denoted `single classifier (i)' in Table TABREF24 ), achieves an F-score that is below 0.93, something that is in line with other researchers in the field, see BIBREF15 .\", 'In comparison to the approach by BIBREF13 , which focuses on various classes of Sexism, the results show that our deep learning model is doing better as far as detecting Sexism in general, outperforming the FastText algorithm they include in their experiments (F=0.87). The inferiority of FastText over LSTM is also reported in the work by BIBREF15 , as well as being inferior over CNN in, BIBREF16 . In general, through our ensemble schemes is confirmed that deep learning can outperform any NLP-based approaches known so far in the task of abusive language detection.']\n",
            "['To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.']\n",
            "['To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.']\n",
            "['To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.']\n",
            "['A commonly known limitation of the current ASR systems is their inability to recognize long sequences of words precisely. In this paper, we propose a new method of incorporating domain knowledge into automatic speech recognition which alleviates this weakness. Our approach allows performing fast ASR domain adaptation by providing a library of intent examples used for lattice rescoring. The method guides the best lattice path selection process by increasing the probability of intent recognition. At the same time, the method does not rescore paths of unessential turns which do not contain intent examples. As a result, our approach improves the understanding of spontaneous conversations by recognizing semantically important transcription segments while adding minimal computational overhead. Our method is domain agnostic and can be easily adapted to a new one by providing the library of intent examples expected to appear in the new domain. The increased intent annotation coverage allows us to train more sophisticated models for downstream tasks, opening the prospects of true spoken language understanding.']\n",
            "['To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.']\n",
            "['The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\\\alpha $-weighted sum.', 'The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.']\n",
            "['Morfessor Baseline is initialized with a seed lexicon of whole words. The Morfessor Baseline training algorithm is a greedy local search. During training, in addition to storing the model parameters, the current best segmentation for the corpus is stored in a graph structure. The segmentation is iteratively refined, by looping over all the words in the corpus in a random order and resegmenting them. The resegmentation is applied by recursive binary splitting, leading to changes in other words that share intermediary units with the word currently being resegmented. The search converges to a local optimum, and is known to be sensitive to the initialization BIBREF11.', 'English, Finnish and Turkish data are from the Morpho Challenge 2010 data set BIBREF17, BIBREF18. The training sets contain ca 878k, 2.9M and 617k word types, respectively. As test sets we use the union of the 10 official test set samples. For North Sámi, we use a list of ca 691k word types extracted from Den samiske tekstbanken corpus (Sametinget, 2004) and the 796 word type test set from version 2 of the data set collected by BIBREF19, BIBREF20.', 'We perform an error analysis, with the purpose of gaining more insight into the ability of the methods to model particular aspects of morphology. We follow the procedure used by ruokolainen2016comparative. It is based on a categorization of morphs into the categories prefix, stem, and suffix. The category labels are derived from the original morphological analysis labels in the English and Finnish gold standards, and directly correspond to the annotation scheme used in the North Sámi test set.', 'Table contains the error analysis for English, Finnish and North Sámi. For English and North Sámi, EM+Prune results in less under-segmentation but worse over-segmentation. For Finnish these results are reversed. However, the suffixes are often better modeled, as shown by lower under-segmentation on SUF-SUF (all languages) and STM-SUF (English and North Sámi).', 'FLOAT SELECTED: Table 2: Morfessor cost results for English. α = 0.9. FS is short for forcesplit, W-sum for weighted sum of prior and likelihood. ↓means that lower values are better. The bolded method is our primary configuration.', 'FLOAT SELECTED: Table 4: Morfessor cost results for Turkish. α = 0.4', 'FLOAT SELECTED: Table 5: Morfessor cost results for North Sámi. α = 1.0', 'FLOAT SELECTED: Table 3: Morfessor cost results for Finnish. α = 0.02.', 'FLOAT SELECTED: Table 10: Error analysis for English (eng, α = 0.9), Finnish (fin, α = 0.02), and North Sámi (sme, α = 1.0). All results without forcesplit. Over-segmentation and under-segmentation errors reduce precision and recall, respectively.']\n",
            "['The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\\\alpha $-weighted sum.', 'The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.']\n",
            "['The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.', 'As each of these features can be turned on or off individually we consider the effects of each for the quality of the alignment between Princeton WordNet and Wikipedia. For this we used the 200NS dataset from BIBREF43 , and computed the precision, recall and F-measure of the mapping for various settings, using 10-fold cross validation, as follows:', 'Only Jaccard score of concatenated label', 'Only smoothed Jaccard ( INLINEFORM0 ) of concatenated label', 'As above with basic features (Jaccard, Dice, Containment, Length Ratio, Average Word Length Ratio, Negation and Number) for the most similar label', 'As above with basic features for label and description', 'As above with GloVe Similarity', 'As above with LSTM score', 'As 5, but using superterm labels', 'As 7, but using superterm labels']\n",
            "['The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.']\n",
            "['The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.', 'As each of these features can be turned on or off individually we consider the effects of each for the quality of the alignment between Princeton WordNet and Wikipedia. For this we used the 200NS dataset from BIBREF43 , and computed the precision, recall and F-measure of the mapping for various settings, using 10-fold cross validation, as follows:']\n",
            "['These role-specific speaker IDs are modeled by a speaker embedding layer of the same dimensions as the transformer hidden state, injected into the transformer input layer. We fine-tune GPT2 (Speaker GPT2) and DialoGPT (Speaker DialoGPT) on our dataset with speaker embeddings. We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.']\n",
            "['In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.']\n",
            "['We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service.']\n",
            "['In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.']\n",
            "['We compare the performance of state-of-the-art language models fine-tuned on Interview and other popular conversational datasets, demonstrating that Interview contains more complex dialog and better models the characteristics of natural spoken conversations. Our dataset is an order of magnitude larger than existing high-quality natural dialog datasets and contains speaker role annotations for each turn, facilitating the development of conversational agents and assistive systems for settings involving specific speaker roles, such as doctor-patient interviews or hosted talk shows.']\n",
            "['Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.']\n",
            "['We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999–2019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.']\n",
            "['These role-specific speaker IDs are modeled by a speaker embedding layer of the same dimensions as the transformer hidden state, injected into the transformer input layer. We fine-tune GPT2 (Speaker GPT2) and DialoGPT (Speaker DialoGPT) on our dataset with speaker embeddings. We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.']\n",
            "['In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.']\n",
            "['We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service.']\n",
            "['We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999–2019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.']\n",
            "['We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999–2019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.']\n",
            "['In the experiments, three regular text datasets and three short text datasets were used:', 'Reuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed. There are 11,367 documents and 120 labels. Each document is associated with multiple labels. The vocabulary size is 8,817 and the average document length is 73.', '20NG, 20 Newsgroup, a widely used dataset consists of 18,846 news articles with 20 categories. The vocabulary size is 22,636 and the average document length is 108.', 'NYT, New York Times is extracted from the documents in the category “Top/News/Health” in the New York Times Annotated Corpus. There are 52,521 documents and 545 unique labels. Each document is with multiple labels. The vocabulary contains 21,421 tokens and there are 442 words in a document on average.', 'WS, Web Snippet, used in BIBREF7 , contains 12,237 web search snippets and each snippet belongs to one of 8 categories. The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average.', 'TMN, Tag My News, used in BIBREF5 , consists of 32,597 English RSS news snippets from Tag My News. With a title and a short description, each snippet belongs to one of 7 categories. There are 13,370 tokens in the vocabulary and the average length of a snippet is 18.', 'AN, ABC News, is a collection of 12,495 short news descriptions and each one is in multiple of 194 categories. There are 4,255 tokens in the vocabulary and the average length of a description is 13.']\n",
            "['In the experiments, three regular text datasets and three short text datasets were used:', 'Reuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed. There are 11,367 documents and 120 labels. Each document is associated with multiple labels. The vocabulary size is 8,817 and the average document length is 73.', '20NG, 20 Newsgroup, a widely used dataset consists of 18,846 news articles with 20 categories. The vocabulary size is 22,636 and the average document length is 108.', 'NYT, New York Times is extracted from the documents in the category “Top/News/Health” in the New York Times Annotated Corpus. There are 52,521 documents and 545 unique labels. Each document is with multiple labels. The vocabulary contains 21,421 tokens and there are 442 words in a document on average.', 'WS, Web Snippet, used in BIBREF7 , contains 12,237 web search snippets and each snippet belongs to one of 8 categories. The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average.', 'TMN, Tag My News, used in BIBREF5 , consists of 32,597 English RSS news snippets from Tag My News. With a title and a short description, each snippet belongs to one of 7 categories. There are 13,370 tokens in the vocabulary and the average length of a snippet is 18.', 'AN, ABC News, is a collection of 12,495 short news descriptions and each one is in multiple of 194 categories. There are 4,255 tokens in the vocabulary and the average length of a description is 13.']\n",
            "['In the experiments, three regular text datasets and three short text datasets were used:', 'Reuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed. There are 11,367 documents and 120 labels. Each document is associated with multiple labels. The vocabulary size is 8,817 and the average document length is 73.', '20NG, 20 Newsgroup, a widely used dataset consists of 18,846 news articles with 20 categories. The vocabulary size is 22,636 and the average document length is 108.', 'NYT, New York Times is extracted from the documents in the category “Top/News/Health” in the New York Times Annotated Corpus. There are 52,521 documents and 545 unique labels. Each document is with multiple labels. The vocabulary contains 21,421 tokens and there are 442 words in a document on average.', 'WS, Web Snippet, used in BIBREF7 , contains 12,237 web search snippets and each snippet belongs to one of 8 categories. The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average.', 'TMN, Tag My News, used in BIBREF5 , consists of 32,597 English RSS news snippets from Tag My News. With a title and a short description, each snippet belongs to one of 7 categories. There are 13,370 tokens in the vocabulary and the average length of a snippet is 18.', 'AN, ABC News, is a collection of 12,495 short news descriptions and each one is in multiple of 194 categories. There are 4,255 tokens in the vocabulary and the average length of a description is 13.', 'All the datasets were tokenised by Mallet and we removed the words that exist in less than 5 documents and more than 95% documents.']\n",
            "['LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 : two models that make use of multiple document labels. The original implementation is used.', 'DMR, LDA with Dirichlet Multinomial Regression BIBREF8 : a model that can use multiple document labels. The Mallet implementation of DMR based on SparseLDA was used. Following Mallet, we set the mean of INLINEFORM0 to 0.0 and set the variances of INLINEFORM1 for the default label and the document labels to 100.0 and 1.0 respectively.', 'WF-LDA, Word Feature LDA BIBREF16 : a model with word features. We implemented it on top of Mallet and used the default settings in Mallet for the optimisation.', 'LF-LDA, Latent Feature LDA BIBREF5 : a model that incorporates word embeddings. The original implementation was used. Following the paper, we used 1500 and 500 MCMC iterations for initialisation and sampling respectively and set INLINEFORM0 to 0.6, and used the original 50-dimensional GloVe word embeddings as word features.', 'GPU-DMM, Generalized Pólya Urn DMM BIBREF7 : a model that incorporates word semantic similarity. The original implementation was used. The word similarity was generated from the distances of the word embeddings. Following the paper, we set the hyper-parameters INLINEFORM0 and INLINEFORM1 to 0.1 and 0.7 respectively, and the symmetric document Dirichlet prior to INLINEFORM2 .']\n",
            "['LDA BIBREF0 : the baseline model. The Mallet implementation of SparseLDA BIBREF30 is used.', 'LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 : two models that make use of multiple document labels. The original implementation is used.', 'DMR, LDA with Dirichlet Multinomial Regression BIBREF8 : a model that can use multiple document labels. The Mallet implementation of DMR based on SparseLDA was used. Following Mallet, we set the mean of INLINEFORM0 to 0.0 and set the variances of INLINEFORM1 for the default label and the document labels to 100.0 and 1.0 respectively.', 'WF-LDA, Word Feature LDA BIBREF16 : a model with word features. We implemented it on top of Mallet and used the default settings in Mallet for the optimisation.', 'LF-LDA, Latent Feature LDA BIBREF5 : a model that incorporates word embeddings. The original implementation was used. Following the paper, we used 1500 and 500 MCMC iterations for initialisation and sampling respectively and set INLINEFORM0 to 0.6, and used the original 50-dimensional GloVe word embeddings as word features.', 'GPU-DMM, Generalized Pólya Urn DMM BIBREF7 : a model that incorporates word semantic similarity. The original implementation was used. The word similarity was generated from the distances of the word embeddings. Following the paper, we set the hyper-parameters INLINEFORM0 and INLINEFORM1 to 0.1 and 0.7 respectively, and the symmetric document Dirichlet prior to INLINEFORM2 .', 'PTM, Pseudo document based Topic Model BIBREF18 : a model for short text analysis. The original implementation was used. Following the paper, we set the number of pseudo documents to 1000 and INLINEFORM0 to 0.1.']\n",
            "['We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window. Those probabilities were computed on an external large corpus, i.e., a 5.48GB Wikipedia dump in our experiments. The NPMI score of each topic in the experiments is calculated with top 10 words ( INLINEFORM8 ) by the Palmetto package. Again, we report the average scores and the standard deviations over 5 random runs.']\n",
            "[\"It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Table TABREF65 shows the NPMI scores on the three short text datasets. Higher scores indicate better topic coherence. All the models were trained with 100 topics. Besides the NPMI scores averaged over all the 100 topics, we also show the scores averaged over top 20 topics with highest NPMI, where “rubbish” topics are eliminated, following BIBREF22 . It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information. We would like to point out that on the TMN dataset, even though the average score of MetaLDA is still the best, the score of MetaLDA has overlapping with the others' in the standard deviation, which indicates the difference is not statistically significant.\"]\n",
            "['To sample INLINEFORM0 , we first marginalise out INLINEFORM1 in the right part of Eq. ( SECREF4 ) with the Dirichlet multinomial conjugacy: +rCl+x* d=1D (d,)(d, + md,)Gamma ratio 1 k=1K (d,k + md,k)(d,k)Gamma ratio 2 where INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 is the gamma function. Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 . Given a set of INLINEFORM8 for all the documents, Gamma ratio 1 can be approximated by the product of INLINEFORM9 , i.e., INLINEFORM10 .', 'Gamma ratio 2 in Eq. ( SECREF17 ) is the Pochhammer symbol for a rising factorial, which can be augmented with an auxiliary variable INLINEFORM0 BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 as follows: +rCl+x* (d,k + md,k)(d,k)Gamma ratio 2 = td,k=0md,k Smd,ktd,k d,ktd,k where INLINEFORM1 indicates an unsigned Stirling number of the first kind. Gamma ratio 2 is a normalising constant for the probability of the number of tables in the Chinese Restaurant Process (CRP) BIBREF28 , INLINEFORM2 can be sampled by a CRP with INLINEFORM3 as the concentration and INLINEFORM4 as the number of customers: +rCl+x* td,k = i=1md,k Bern(d,kd,k+i) where INLINEFORM5 samples from the Bernoulli distribution. The complexity of sampling INLINEFORM6 by Eq. ( SECREF17 ) is INLINEFORM7 . For large INLINEFORM8 , as the standard deviation of INLINEFORM9 is INLINEFORM10 BIBREF28 , one can sample INLINEFORM11 in a small window around the current value in complexity INLINEFORM12 .', \"Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity.\"]\n",
            "[\"Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity.\"]\n",
            "['We presented the Twitter Job/Employment Corpus and our approach for extracting discourse on work from public social media. We developed and improved an effective, humans-in-the-loop active learning framework that uses human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related. We accurately determine whether or not Twitter accounts are personal or business-related, according to their linguistic characteristics and posts history. Our crowdsourced evaluations suggest that these labels are precise and reliable. Our classification framework could be extended to other open-domain problems that similarly lack high-quality labeled ground truth data.']\n",
            "['Figure FIGREF4 shows the workflow of our humans-in-the-loop framework. It has multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics, to extract job-related tweets from personal and business accounts.', 'FLOAT SELECTED: Figure 1: Our humans-in-the-loop framework collects labeled data by alternating between human annotation and automatic prediction models over multiple rounds. Each diamond represents an automatic classifier (C), and each trapezoid represents human annotations (R). Each classifier filters and provides machine-predicted labels to tweets that are published to human annotators in the consecutive round. The human-labeled tweets are then used as training data by the succeeding automatic classifier. We use two types of classifiers: rule-based classifiers (C0 and C4) and support vector machines (C1, C2, C3 and C5). This framework serves to reduce the amount of human efforts needed to acquire large amounts of high-quality labeled data.']\n",
            "['Figure FIGREF4 shows the workflow of our humans-in-the-loop framework. It has multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics, to extract job-related tweets from personal and business accounts.']\n",
            "[\"In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'.\"]\n",
            "[\"In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'.\"]\n",
            "['None of the above work has focused on understanding the role of political trolls. The only closely relevant work is that of BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches. They characterize trolls using the digital traces they leave behind, which is modeled using a time-sensitive semantic edit distance.']\n",
            "['None of the above work has focused on understanding the role of political trolls. The only closely relevant work is that of BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches. They characterize trolls using the digital traces they leave behind, which is modeled using a time-sensitive semantic edit distance.']\n",
            "['Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.']\n",
            "['Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.']\n",
            "['We consider two possible scenarios. The first, prototypical ML scenario is supervised learning, where we want to learn a function from users to categories {left, right, news feed}, and the ground truth labels for the troll users are available. This scenario has been considered previously in the literature by BIBREF2. Unfortunately, a solution for such a scenario is not directly applicable to a real-world use case. Suppose a new troll farm trying to sway the upcoming European or US elections has just been discovered. While the identities of the accounts might be available, the labels to learn from would not be present. Thus, any supervised machine learning approach would fall short of being a fully automated solution to our initial problem.', 'A more realistic scenario assumes that labels for troll accounts are not available. In this case, we need to use some external information in order to learn a labeling function. Indeed, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.']\n",
            "['A more realistic scenario assumes that labels for troll accounts are not available. In this case, we need to use some external information in order to learn a labeling function. Indeed, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.']\n",
            "['A more realistic scenario assumes that labels for troll accounts are not available. In this case, we need to use some external information in order to learn a labeling function. Indeed, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.']\n",
            "['The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. “Producer X will not press charges against Jeremy Clarkson, his lawyer says.”).', \"The third dataset, the Children's Book Test (CBT) BIBREF3 , is built from books that are freely available thanks to Project Gutenberg. Each context document is formed by 20 consecutive sentences taken from a children's book story. Due to the lack of summary, the cloze-style question is then constructed from the subsequent (21st) sentence.\"]\n",
            "['The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. “Producer X will not press charges against Jeremy Clarkson, his lawyer says.”).', \"The third dataset, the Children's Book Test (CBT) BIBREF3 , is built from books that are freely available thanks to Project Gutenberg. Each context document is formed by 20 consecutive sentences taken from a children's book story. Due to the lack of summary, the cloze-style question is then constructed from the subsequent (21st) sentence.\", 'What concerns ensembles, we used simple averaging of the answer probabilities predicted by ensemble members. For ensembling we used 14, 16, 84 and 53 models for CNN, Daily Mail and CBT CN and NE respectively. The ensemble models were chosen either as the top 70% of all trained models, we call this avg ensemble. Alternatively we use the following algorithm: We started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble.']\n",
            "['On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. The average performance of the top 20% models according to validation accuracy is 69.9% which is even 0.5% better than the single best-validation model. This shows that there were many models that performed better on test set than the best-validation model. Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets..', 'CBT. In named entity prediction our best single model with accuracy of 68.6% performs 2% absolute better than the MenNN with self supervision, the averaging ensemble performs 4% absolute better than the best previous result. In common noun prediction our single models is 0.4% absolute better than MenNN however the ensemble improves the performance to 69% which is 6% absolute better than MenNN.']\n",
            "['Several recent deep neural network architectures BIBREF1 , BIBREF3 , BIBREF7 , BIBREF12 were applied to the task of text comprehension. The last two architectures were developed independently at the same time as our work. All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.', 'Attentive and Impatient Readers were proposed in BIBREF1 . The simpler Attentive Reader is very similar to our architecture. It also uses bidirectional document and query encoders to compute an attention in a similar way we do. The more complex Impatient Reader computes attention over the document after reading every word of the query. However, empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets.', 'Chen et al. 2016', 'A model presented in BIBREF7 is inspired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that is INLINEFORM0 . The document embedding INLINEFORM1 is computed using a weighted sum as in the Attentive Reader, INLINEFORM2 . In the end INLINEFORM3 , where INLINEFORM4 is a new embedding function.', 'Memory Networks', 'MenNN BIBREF13 were applied to the task of text comprehension in BIBREF3 .', 'Dynamic Entity Representation', 'The Dynamic Entity Representation model BIBREF12 has a complex architecture also based on the weighted attention mechanism and max-pooling over contextual embeddings of vectors for each named entity.', 'One can also see how the task complexity varies with the type of the omitted word (named entity, common noun, verb, preposition). BIBREF3 have shown that while standard LSTM language models have human level performance on predicting verbs and prepositions, they lack behind on named entities and common nouns. In this article we therefore focus only on predicting the first two word types.']\n",
            "['The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. “Producer X will not press charges against Jeremy Clarkson, his lawyer says.”).', 'What concerns ensembles, we used simple averaging of the answer probabilities predicted by ensemble members. For ensembling we used 14, 16, 84 and 53 models for CNN, Daily Mail and CBT CN and NE respectively. The ensemble models were chosen either as the top 70% of all trained models, we call this avg ensemble. Alternatively we use the following algorithm: We started with the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been previously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble.']\n",
            "[\"In this section we evaluate our model on the CNN, Daily Mail and CBT datasets. We show that despite the model's simplicity its ensembles achieve state-of-the-art performance on each of these datasets.\"]\n",
            "['The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. “Producer X will not press charges against Jeremy Clarkson, his lawyer says.”).', \"The third dataset, the Children's Book Test (CBT) BIBREF3 , is built from books that are freely available thanks to Project Gutenberg. Each context document is formed by 20 consecutive sentences taken from a children's book story. Due to the lack of summary, the cloze-style question is then constructed from the subsequent (21st) sentence.\"]\n",
            "[\"Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact. For humans, the wider spread in results can be attributed partially to the fact that 55 random permutations out of the 144 permutations were annotated for each template. Therefore, it might have been easier to distinguish grammatical sentences from ungrammatical ones for some templates than others.\"]\n",
            "['How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.']\n",
            "['How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.']\n",
            "['How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.']\n",
            "['How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.']\n",
            "['As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.', 'We use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we will assign a segment embedding $E_A$ or $E_B$ conditioned on $i$ is odd or even. For example, for $[sent_1, sent_2, sent_3, sent_4, sent_5]$ we will assign $[E_A, E_B, E_A,E_B, E_A]$ .']\n",
            "['FLOAT SELECTED: Figure 1: The overview architecture of the BERTSUM model.', 'As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.', 'We use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we will assign a segment embedding $E_A$ or $E_B$ conditioned on $i$ is odd or even. For example, for $[sent_1, sent_2, sent_3, sent_4, sent_5]$ we will assign $[E_A, E_B, E_A,E_B, E_A]$ .', 'The vector $T_i$ which is the vector of the $i$ -th [CLS] symbol from the top BERT layer will be used as the representation for $sent_i$ .']\n",
            "['The experimental results on CNN/Dailymail datasets are shown in Table 1. For comparison, we implement a non-pretrained Transformer baseline which uses the same architecture as BERT, but with smaller parameters. It is randomly initialized and only trained on the summarization task. The Transformer baseline has 6 layers, the hidden size is 512 and the feed-forward filter size is 2048. The model is trained with same settings following BIBREF1 . We also compare our model with several previously proposed systems.']\n",
            "['We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function.']\n",
            "['We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function.']\n",
            "['We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function.']\n",
            "['We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.']\n",
            "['We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset.']\n",
            "['We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset.', 'AliMe For the AliMe service in E-commerce, we collect 8,004 query-bag pairs to form our dataset. Negative sampling is also an important part of the matching model. For each query, we use the Lucene to retrieval the top-20 most similar questions from the whole question candidates. Then we filter the questions which are in the corresponding right bag. After that, we randomly sample one in the retrieved candidate and use the bag that the retrieved candidate belongs to as the negative case. In the bag construction stage, the annotators have already merged all the questions of the same meaning, so we can ensure that the after filtering retrieved cases are negative in our setting. We also restrict the number of questions in a bag not more than 5 and discard the redundant questions. Finally, we get 12,008 training cases, 2,000 valid cases, and 10,000 test cases. Please notice, for the testing, we sampled 9 negative bags instead of 1, and thus formed 10 candidates for ranking.', 'Quora The Quora dataset is originally released for the duplicated question detection task. The dataset contains 400,000 question pairs and each pair is marked whether they are asking the same question. Due to the huge amount of duplicated question pairs, we group the questions as question bag via the union-find algorithm from the duplicated questions. We get 60,400 bags, and all the questions in a bag are asking the same question. We filter the bags that contain questions less than 3 to make the bag not too small. The new bag dataset will help similar questions recommendation on the Quora website. We then extract one question in the bag as query and the other questions make up the bag in our task. Considering the negative samples, we follow the same strategy as AliMe dataset. Finally, we get 20,354 training set, 2,000 validation set, and 10,000 test set. To facilitate the research in this area, the composed Quora dataset are released.']\n",
            "['We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.']\n",
            "['We use the PMB v.2.1.0 for the experiments. The dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. We divide the English sentences into 3072 training sentences, 663 development and 670 testing sentences. We consider all the sentences in other languages as test set.']\n",
            "['Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.']\n",
            "['Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.']\n",
            "['We perform experiments on 3-class polarity classification in tweets, and report results on four different languages: French, Italian, Spanish and German. Existing English sentiment lexicons are translated to the target languages through the proposed approach, given gs trained on the respective Wikipedia of each language. Then, a SVM-based classifier is fed with lexicon features, comparing machine translation with embedding transfer.']\n",
            "['Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e. they do look like conversations from a messenger app and could be condensed in a reasonable way. In a similar validation task, conducted for the existing dialogue-type datasets (described in the Initial approach section), the annotators agreed that only 28% of the dialogues resembled conversations from a messenger app.']\n",
            "['ROUGE is a standard way of evaluating the quality of machine generated summaries by comparing them with reference ones. The metric based on n-gram overlapping, however, may not be very informative for abstractive summarization, where paraphrasing is a keypoint in producing high-quality sentences. To quantify this conjecture, we manually evaluated summaries generated by the models for 150 news and 100 dialogues. We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 – it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.']\n",
            "['ROUGE is a standard way of evaluating the quality of machine generated summaries by comparing them with reference ones. The metric based on n-gram overlapping, however, may not be very informative for abstractive summarization, where paraphrasing is a keypoint in producing high-quality sentences. To quantify this conjecture, we manually evaluated summaries generated by the models for 150 news and 100 dialogues. We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 – it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.']\n",
            "['We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):', 'Pointer generator network BIBREF4. In the case of Pointer Generator, we use a default configuration, changing only the minimum length of the generated summary from 35 (used in news) to 15 (used in dialogues).', 'Transformer BIBREF16. The model is trained using OpenNMT library. We use the same parameters for training both on news and on dialogues, changing only the minimum length of the generated summary – 35 for news and 15 for dialogues.', 'Fast Abs RL BIBREF5. It is trained using its default parameters. For dialogues, we change the convolutional word-level sentence encoder (used in extractor part) to only use kernel with size equal 3 instead of 3-5 range. It is caused by the fact that some of utterances are very short and the default setting is unable to handle that.', \"Fast Abs RL Enhanced. The additional variant of the Fast Abs RL model with slightly changed utterances i.e. to each utterance, at the end, after artificial separator, we add names of all other interlocutors. The reason for that is that Fast Abs RL requires text to be split into sentences (as it selects sentences and then paraphrase each of them). For dialogues, we divide text into utterances (which is a natural unit in conversations), so sometimes, a single utterance may contain more than one sentence. Taking into account how this model works, it may happen that it selects an utterance of a single person (each utterance starts with the name of the author of the utterance) and has no information about other interlocutors (if names of other interlocutors do not appear in selected utterances), so it may have no chance to use the right people's names in generated summaries.\", 'LightConv and DynamicConv BIBREF17. The implementation is available in fairseq BIBREF18. We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation BIBREF19; (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small BIBREF20.']\n",
            "['In the present paper, we further investigate the problem of abstractive dialogue summarization. With the growing popularity of online conversations via applications like Messenger, WhatsApp and WeChat, summarization of chats between a few participants is a new interesting direction of summarization research. For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries. The dataset is freely available for the research community.', 'Description. The created dataset is made of 16369 conversations distributed uniformly into 4 groups based on the number of utterances in conversations: 3-6, 7-12, 13-18 and 19-30. Each utterance contains the name of the speaker. Most conversations consist of dialogues between two interlocutors (about 75% of all conversations), the rest is between three or more people. Table TABREF3 presents the size of the dataset split used in our experiments. The example of a dialogue from this corpus is shown in Table TABREF4.']\n",
            "['In the present paper, we further investigate the problem of abstractive dialogue summarization. With the growing popularity of online conversations via applications like Messenger, WhatsApp and WeChat, summarization of chats between a few participants is a new interesting direction of summarization research. For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries. The dataset is freely available for the research community.']\n",
            "['The performance of these methods are measured by the percentage fall in accuracy of these models on the generated adversarial texts. Higher the percentage dip in the accuracy of the target classifier, more effective is our model.', 'We analyze the effectiveness of our approach by comparing the results from using two different baselines against character and word-based models trained on different datasets. Table TABREF40 demonstrates the capability of our model. Without the reinforcement learning objective, the No-RL model performs better than the back-translation approach(NMT-BT). The improvement can be attributed to the word and character perturbations introduced by our hybrid encoder-decoder model as opposed to only paraphrases in the former model. Our complete AEG model outperforms all the other models with significant drop in accuracy. For the CNN-Word, DeepWordBug decreases the accuracy from 89.95% to 28.13% while AEG model further reduces it to 18.5%.']\n",
            "[\"News categorization: We perform our experiments on AG's news corpus with a character-based convolutional model (CNN-Char) BIBREF12. The news corpus contains titles and descriptions of various news articles along with their respective categories. There are four categories: World, Sports, Business and Sci/Tech. The trained CNN-Char model achieves a test accuracy of 89.11%.\"]\n",
            "['Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.']\n",
            "['Proposed Attack Strategy', 'Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\\\prime }$ such that $T(x^{\\\\prime }) \\\\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\\\prime }$ are called perturbations. We would like to have $x^{\\\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.', 'We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.', 'Training ::: Training with Reinforcement learning', 'We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.', 'Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)', \"In SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\\\prime }$ sampled from the model's distribution $p(y^{\\\\prime }_j|y^{\\\\prime }_{<j},h)$ and (b) $\\\\hat{y}$ obtained by greedily decoding ($argmax$ predictions) from the distribution $p(\\\\hat{y}_j|\\\\hat{y}_{<j},h)$ Next, rewards $r(y^{\\\\prime }_j),r(\\\\hat{y}_j)$ are computed for both the sequences using a reward function $r(\\\\cdot )$, explained in Section SECREF26. We train the model by minimizing:\", 'Here $r(\\\\hat{y})$ can be viewed as the baseline reward. This approach, therefore, explores different sequences that produce higher reward compared to the current best policy.']\n",
            "['In this task of adversarial example generation, we have black-box access to the target model; the generator is not aware of the target model architecture or parameters and is only capable of querying the target model with supplied inputs and obtaining the output predictions. To enable the model to have capabilities to generate word and character perturbations, we develop a hybrid encoder-decoder model, Adversarial Examples Generator (AEG), that operates at both word and character level to generate adversarial examples. Below, we explain the components of this model which have been improved to handle both word and character information from the text sequence.', \"The encoder maps the input text sequence into a sequence of representations using word and character-level information. Our encoder (Figure FIGREF10) is a slight variant of Chen et al.BIBREF31. This approach providing multiple levels of granularity can be useful in order to handle rare or noisy words in the text. Given character embeddings $E^{(c)}=[e_1^{(c)}, e_2^{(c)},...e_{n^{\\\\prime }}^{(c)}]$ and word embeddings $E^{(w)}=[e_1^{(w)}, e_2^{(w)},...e_{n}^{(w)}]$ of the input, starting ($p_t$) and ending ($q_t$) character positions at time step $t$, we define inside character embeddings as: $E_I^{(c)}=[e_{p_t}^{(c)},...., e_{q_t}^{(c)}]$ and outside embeddings as: $E_O^{(c)}=[e_{1}^{(c)},....,e_{p_t-1}^{(c)}; e_{q_t+1}^{(c)},...,e_{n^{\\\\prime }}^{(c)}]$. First, we obtain the character-enhanced word representation $\\\\overleftrightarrow{h_t}$ by combining the word information from $E^{(w)}$ with the character context vectors. Character context vectors are obtained by attending over inside and outside character embeddings. Next, we compute a summary vector $S$ over the hidden states $\\\\overleftrightarrow{h_t}$ using an attention layer expressed as $Attn(\\\\overleftrightarrow{H})$. To generate adversarial examples, it is important to identify the most relevant text units that contribute towards the target model's prediction and then use this information during the decoding step to introduce perturbation on those units. Hence, the summary vector is optimized using target model predictions without back propagating through the entire encoder. This acts as a substitute network that learns to mimic the predictions of the target classifier.\", 'Our AEG should be able to generate both character and word level perturbations as necessary. We achieve this by modifying the standard decoder BIBREF29, BIBREF30 to have two-level decoder GRUs: word-GRU and character-GRU (see Figure FIGREF14). Such hybrid approaches have been studied to achieve open vocabulary NMT in some of the previous work like Wu et al. BIBREF32 and Luong et al. BIBREF25. Given the challenge that all different word misspellings cannot fit in a fixed vocabulary, we leverage the power of both words and characters in our generation procedure. The word-GRU uses word context vector $c_j^{(w)}$ by attending over the encoder hidden states $\\\\overleftrightarrow{h_t}$. Once the word context vector $c_j^{(w)}$ is computed, we introduce a perturbation vector $v_{p}$ to impart information about the need for any word or character perturbations at this decoding step. We construct this vector using the word-GRU decoder state $s_j^{(w)}$, context vector $c_j^{(w)}$ and summary vector $S$ from the encoder as:']\n",
            "[\"M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.\", \"Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.\", \"However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.\"]\n",
            "[\"Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.\", \"However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.\"]\n",
            "['We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.']\n",
            "[\"Figure FIGREF9 plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert. We can see that performance using En-Bert depends directly on word piece overlap: the ability to transfer deteriorates as word piece overlap diminishes, and F1 scores are near zero for languages written in different scripts. M-Bert's performance, on the other hand, is flat for a wide range of overlaps, and even for language pairs with almost no lexical overlap, scores vary between INLINEFORM0 and INLINEFORM1 , showing that M-Bert's pretraining on multiple languages has enabled a representational capacity deeper than simple vocabulary memorization.\", 'Following BIBREF10 , we compare languages on a subset of the WALS features BIBREF11 relevant to grammatical ordering. Figure FIGREF17 plots pos zero-shot accuracy against the number of common WALS features. As expected, performance improves with similarity, showing that it is easier for M-Bert to map linguistic structures when they are more similar, although it still does a decent job for low similarity languages when compared to En-Bert.']\n",
            "['We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.', \"Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.\", 'Following BIBREF10 , we compare languages on a subset of the WALS features BIBREF11 relevant to grammatical ordering. Figure FIGREF17 plots pos zero-shot accuracy against the number of common WALS features. As expected, performance improves with similarity, showing that it is easier for M-Bert to map linguistic structures when they are more similar, although it still does a decent job for low similarity languages when compared to En-Bert.', 'We sample 5000 pairs of sentences from WMT16 BIBREF14 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer INLINEFORM0 , INLINEFORM1 . For each pair of sentences, e.g. INLINEFORM2 , we compute the vector pointing from one to the other and average it over all pairs: INLINEFORM3 , where INLINEFORM4 is the number of pairs. Finally, we translate each sentence, INLINEFORM5 , by INLINEFORM6 , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the “nearest neighbor accuracy”.']\n",
            "['We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.', 'We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.', 'We sample 5000 pairs of sentences from WMT16 BIBREF14 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer INLINEFORM0 , INLINEFORM1 . For each pair of sentences, e.g. INLINEFORM2 , we compute the vector pointing from one to the other and average it over all pairs: INLINEFORM3 , where INLINEFORM4 is the number of pairs. Finally, we translate each sentence, INLINEFORM5 , by INLINEFORM6 , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the “nearest neighbor accuracy”.']\n",
            "['We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.', 'We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.']\n",
            "['Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).']\n",
            "['The third study built a system for real-time sentiment analysis on 2012 U.S. Presidential Election to show public opinion about each candidate on Twitter BIBREF1 . They collected tweets for each candidates using Gnip Power Track since October 12, 2012 and tokenized them. The tweets were labeled by around 800 turkers on Amazon Mechanical Turk (AMT). They trained a Naive Bayes Classifier using 17,000 tweets which consists of 4 classes: (1) positive; (2) negative; (3) neutral; and (4) unsure. It achieved 59% accuracy, which is the best performance achieved in the three recent studies. They visualized the sentiment on a dashboard and calculated the trending words using TF-IDF.', 'As far as we know, there is not any research about prediction on 2016 U.S. Presidential Election yet. Previous researches either set the sentiment of a tweet directly based on a subjectivity lexicon BIBREF3 or preprocessed the tweet using a complex preprocessing method BIBREF1 , BIBREF2 . BIBREF2 not only removed URLs, mentions, retweets, hashtags, numbers and stop words; but also tokenized the tweets and added not_ on negative words. BIBREF1 tokenized the tweets and separated URLs, emoticons, phone numbers, HTML tags, mentions, hashtags, fraction or decimals, and symbol or Unicode character repetition. This research analyzes sentiment on tweets about 2016 U.S. Presidential candidates. We will build a Naive Bayesian predictive model for each candidate and compare the prediction with RealClearPolitics.com. We expect to have a correct prediction on the leading candidates for Democratic and Republican Party. We prove that using a simpler preprocessing method can still have comparable performance to the best performing recent study BIBREF1 .']\n",
            "['We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates\\' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet. We only save tweets which have passed the two requirements such as in Table 1. The first example shows no change in the tweet\\'s content, since there isn\\'t any URLs or pictures, and it contains a candidate\\'s name: Bernie Sanders. The second example shows a removed tweet, which doesn\\'t contain any candidates\\' name. The preprocessing stage changes the third tweet\\'s contents. It removes the URLs and still keeps the tweet because it contains \"Hillary Clinton\" and \"Donald Trump\". The preprocessing stage removes 41% of the data (Figure 2).']\n",
            "['The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top- INLINEFORM0 results. In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.', 'Differently from BIBREF2 , the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by INLINEFORM0 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, INLINEFORM1 returns at most the top 30 relevant facts for INLINEFORM2 . Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary INLINEFORM3 .']\n",
            "['The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 . From now on, we denote the contextual representation for the word INLINEFORM5 by INLINEFORM6 and the contextual representation for the word INLINEFORM7 in the document INLINEFORM8 by INLINEFORM9 . Differently from BIBREF3 , we build a unique representation for the whole set of documents INLINEFORM10 related to the query INLINEFORM11 by stacking each contextual representation INLINEFORM12 obtaining a matrix INLINEFORM13 , where INLINEFORM14 .', 'This phase uncovers a possible inference chain which models meaningful relationships between the query and the set of related documents. The inference chain is obtained by performing, for each inference step INLINEFORM0 , the attention mechanisms given by the query attentive read and the document attentive read keeping a state of the inference process given by an additional recurrent neural network with GRU units. In this way, the network is able to progressively refine the attention weights focusing on the most relevant tokens of the query and the documents which are exploited by the prediction neural network to select the correct answers among the candidate ones.']\n",
            "['The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 . From now on, we denote the contextual representation for the word INLINEFORM5 by INLINEFORM6 and the contextual representation for the word INLINEFORM7 in the document INLINEFORM8 by INLINEFORM9 . Differently from BIBREF3 , we build a unique representation for the whole set of documents INLINEFORM10 related to the query INLINEFORM11 by stacking each contextual representation INLINEFORM12 obtaining a matrix INLINEFORM13 , where INLINEFORM14 .', 'This phase uncovers a possible inference chain which models meaningful relationships between the query and the set of related documents. The inference chain is obtained by performing, for each inference step INLINEFORM0 , the attention mechanisms given by the query attentive read and the document attentive read keeping a state of the inference process given by an additional recurrent neural network with GRU units. In this way, the network is able to progressively refine the attention weights focusing on the most relevant tokens of the query and the documents which are exploited by the prediction neural network to select the correct answers among the candidate ones.']\n",
            "['The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for “SEM-HMM Approx.” is close to the exact version at each batch level, while only taking half the time on average.']\n",
            "['We now present our experimental results on SEM-HMM and SEM-HMM-Approx. The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The “Frequency” baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The “Conditional” baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as “BMM,” is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as “BMM + EM.”']\n",
            "['The Open Minds Indoor Common Sense (OMICS) corpus was developed by the Honda Research Institute and is based upon the Open Mind Common Sense project BIBREF17 . It describes 175 common household tasks with each task having 14 to 122 narratives describing, in short sentences, the necessary steps to complete it. Each narrative consists of temporally ordered, simple sentences from a single author that describe a plan to accomplish a task. Examples from the “Answer the Doorbell” task can be found in Table 2. The OMICS corpus has 9044 individual narratives and its short and relatively consistent language lends itself to relatively easy event extraction.']\n",
            "['The Open Minds Indoor Common Sense (OMICS) corpus was developed by the Honda Research Institute and is based upon the Open Mind Common Sense project BIBREF17 . It describes 175 common household tasks with each task having 14 to 122 narratives describing, in short sentences, the necessary steps to complete it. Each narrative consists of temporally ordered, simple sentences from a single author that describe a plan to accomplish a task. Examples from the “Answer the Doorbell” task can be found in Table 2. The OMICS corpus has 9044 individual narratives and its short and relatively consistent language lends itself to relatively easy event extraction.']\n",
            "[\"Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.\"]\n",
            "['In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).']\n",
            "['Baseline Methods: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-of-the-art method BIBREF0 which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification. (3) BoWV: Bag of Words Vector approach uses the average of the word (GloVe) embeddings to represent a sentence. We experiment with multiple classifiers for both the TF-IDF and the BoWV approaches.']\n",
            "['In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).']\n",
            "[\"Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.\"]\n",
            "['In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).']\n",
            "['We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores.']\n",
            "['Several of these words were undergoing widespread growth in popularity around the time period spanned by our data set. For example, the frequencies of ard, asl, hella, and tfti more than tripled between 2012 and 2013. Our main research question is whether and how these words spread through Twitter. For example, lexical words are mainly transmitted through speech. We would expect their spread to be only weakly correlated with the Twitter social network. In contrast, abbreviations are fundamentally textual in nature, so we would expect their spread to correlate much more closely with the Twitter social network.']\n",
            "['Figure FIGREF55 shows exhaustive hierarchical topic trees extracted from a small text sample with topics from four domains: INLINEFORM0 , INLINEFORM1 INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 . hLDA tends to mix words from different domains into one topic. For instance, words on the first level of the topic tree come from all four domains. This is because the topic path drawing method in existing hLDA-based models takes words in the most important topic of every document and labels them as the main topic of the corpus. In contrast, hrLDA is able to create four big branches for the four domains from the root. Hence, it generates clean topic hierarchies from the corpus.']\n",
            "['We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents “The Acadian flycatcher is a small insect-eating bird.\" and “The Pacific loon is a medium-sized member of the loon.\" to infer that the Acadian flycatcher and the Pacific loon are both related to topic bird, the pre-knowledge that “the loon is a species of bird\" is required for hrLDA. This example explains why the accuracy of extracting ontologies from this kind of corpus is low.']\n",
            "['We utilized the Apache poi library to parse texts from pdfs, word documents and presentation files; the MALLET toolbox BIBREF29 for the implementations of LDA, optimized_LDA BIBREF30 and hLDA; the Apache Jena library to add relations, properties and members to hierarchical topic trees; and Stanford Protege for illustrating extracted ontologies. We make our code and data available . We used the same empirical hyper-parameter setting (i.e., INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 ) across all our experiments. We then demonstrate the evaluation results from two aspects: topic hierarchy and ontology rule.', 'We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents “The Acadian flycatcher is a small insect-eating bird.\" and “The Pacific loon is a medium-sized member of the loon.\" to infer that the Acadian flycatcher and the Pacific loon are both related to topic bird, the pre-knowledge that “the loon is a species of bird\" is required for hrLDA. This example explains why the accuracy of extracting ontologies from this kind of corpus is low.']\n",
            "['To achieve the first objective, we extract noun phrases and then propose a sampling method to estimate the number of topics. For the second objective, we use language parsing and relation extraction to learn relations for the noun phrases. Regarding the third objective, we adapt and improve the hierarchical latent Dirichlet allocation (hLDA) model BIBREF19 , BIBREF20 . hLDA is not ideal for ontology learning because it builds topics from unigrams (which are not descriptive enough to serve as entities in ontologies) and the topics may contain words from multiple domains when input data have documents from many domains (see Section SECREF2 and Figure FIGREF55 ). Our model, hrLDA, overcomes these deficiencies. In particular, hrLDA represents topics with noun phrases, uses syntax and document structures such as paragraph indentations and item lists, assigns multiple topic paths for every document, and allows topic trees to grow vertically and horizontally.', 'Documents are typically composed of chunks of texts, which may be referred to as sections in Word documents, paragraphs in PDF documents, slides in presentation documents, etc. Each chunk is composed of multiple sentences that are either atomic or complex in structure, which means a document is also a collection of atomic and/or complex sentences. An atomic sentence (see module INLINEFORM0 in Figure FIGREF10 ) is a sentence that contains only one subject ( INLINEFORM1 ), one object ( INLINEFORM2 ) and one verb ( INLINEFORM3 ) between the subject and the object. For every atomic sentence whose object is also a noun phrase, there are at least two relation triplets (e.g., “The tiger that gave the excellent speech is handsome\" has relation triplets: (tiger, give, speech), (speech, be given by, tiger), and (tiger, be, handsome)). By contrast, a complex sentence can be subdivided into multiple atomic sentences. Given that the syntactic verb in a relation triplet is determined by the subject and the object, a document INLINEFORM4 in a corpus INLINEFORM5 can be ultimately reduced to INLINEFORM6 subject phrases (we convert objects to subjects using passive voice) associated with INLINEFORM7 relation triplets INLINEFORM8 . Number INLINEFORM9 is usually larger than the actual number of noun phrases in document INLINEFORM10 . By replacing the unigrams in LDA with relation triplets, we retain definitive information and assign salient noun phrases high weights.', 'Extracting relation triplets is the essential step of hrLDA, and it is also the key process for converting a hierarchical topic tree to an ontology structure. The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 . Generally, there are two types of relation triplets:', 'Subject-predicate-object-based relations,', 'e.g., New York is the largest city in the United States INLINEFORM0 (New York, be the largest city in, the United States);']\n",
            "['The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics.']\n",
            "['The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics.']\n",
            "['We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\\\pm 0.01$.']\n",
            "['In order to compare the benefit of a domain with similarity metrics between the source and target domains, we describe a set of symmetric and asymmetric similarity metrics. These also include two novel metrics to evaluate domain adaptability: namely as LM3 (Chameleon Words Similarity) and LM4 (Entropy Change). These metrics perform at par with the metrics that use previously proposed methods. We observe that, amongst word embedding-based metrics, ULM6 (ELMo) performs the best, and amongst sentence embedding-based metrics, ULM7 (Universal Sentence Encoder) is the clear winner. We discuss various metrics, their results and provide a set of recommendations to the problem of source domain selection for CDSA.']\n",
            "['When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain:', 'Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap', \"All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\\\chi ^2$ value greater than or equal to 1. The $\\\\chi ^2$ value is calculated as follows:\", 'Where ${c_p}^w$ and ${c_n}^w$ are the observed counts of word $w$ in positive and negative reviews, respectively. $\\\\mu ^w$ is the expected count, which is kept as half of the total number of occurrences of $w$ in the corpus. We hypothesize that, if a domain-pair $(D_1,D_2)$ shares a larger number of significant words than the pair $(D_1,D_3)$, then $D_1$ is closer to $D_2$ as compared to $D_3$, since they use relatively higher number of similar words for sentiment expression. For every target domain, we compute the intersection of significant words with all other domains and rank them on the basis of intersection count. The utility of this metric is that it can also be used in a scenario where target domain data is unlabelled, but source domain data is labelled. It is due to the fact that once we obtain significant words in the source domain, we just need to search for them in the target domain to find out common significant words.', 'Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)', \"KL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,\", 'where $P$ is the probability of a word appearing in a review which is labelled positive and $N$ is the probability of a word appearing in a review which is labelled negative.', 'SKLD of a polar word for domain-pair $(D_1,D_2)$ is calculated as:', 'where $P_i$ and $N_i$ are probabilities of a word appearing under positively labelled and negatively labelled reviews, respectively, in domain $i$. We then take an average of all common polar words.', 'We observe that, on its own, this metric performs rather poorly. Upon careful analysis of results, we concluded that the imbalance in the number of polar words being shared across domain-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair $(D_1,D_2)$ using the Jaccard Similarity Coefficient which is calculated as follows:', 'where $C$ is the number of common polar words and $W_1$ and $W_2$ are number of polar words in $D_1$ and $D_2$ respectively. The intuition behind this being that the domain-pairs having higher percentage of polar words overlap should be ranked higher compared to those having relatively higher number of polar words. For example, we prefer $(C:40,W_1 :50,W_2 :50)$ over $(C:200,W_1 :500,W_2 :500)$ even though 200 is greater than 40. To compute the final similarity value, we add the reciprocal of $J$ to the SKLD value since a larger value of $J$ will add a smaller fraction to SLKD value. For a smaller SKLD value, the domains would be relatively more similar. This is computed as follows:', 'Domain pairs are ranked in increasing order of this similarity value. After the introduction of the confidence term, a significant improvement in the results is observed.', 'Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity', \"This metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.\", 'For every common polar word between two domains, $L_1 \\\\ Distance$ between two vectors $[P_1,N_1]$ and $[P_2,N_2]$ is calculated as;', 'The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs.', 'Domain pairs are ranked in increasing order of final value.', 'Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change', 'Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:', 'Here, $X$ is the set of n-grams that contain at least one polar word, $Y$ is the set of n-grams which do not contain any polar word, and $w$ is the weight. For our experiments, we keep the value of $w$ as 1 for unigrams and 5 for bi, tri, and quadrigrams.', 'We then say that a source domain $D_2$ is more suitable for target domain $D_1$ as compared to source domain $D_3$ if;', 'where $D_2+D_1$ indicates combined data obtained by mixing $D_1$ in $D_2$ and $\\\\Delta E$ indicates percentage change in entropy before and after mixing of source and target domains.', 'Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data.', 'For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec BIBREF12, GloVe BIBREF13, FastText BIBREF14, and ELMo BIBREF15. We also exploit sentence vectors from models trained using Doc2Vec BIBREF16, FastText, and Universal Sentence Encoder BIBREF17. In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet BIBREF18, where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec', 'We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity BIBREF17. It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity distinguishes nearly parallel vectors much better, we use it instead of Cosine Similarity. We obtain a similarity value by averaging over all common adjectives. For the final similarity value of this metric, we use Jaccard Similarity Coefficient here as well:', 'For a target domain, source domains are ranked in decreasing order of final similarity value.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec', 'Doc2Vec represents each sentence by a dense vector which is trained to predict words in the sentence, given the model. It tries to overcome the weaknesses of the bag-of-words model. Similar to Word2Vec, we train Doc2Vec models on each domain to extract sentence vectors. We train the models over 100 epochs for 100 dimensions, where the learning rate is 0.025. Since we can no longer leverage adjectives for sentiment, we use SentiWordnet for assigning sentiment scores (ranging from -1 to +1 where -1 denotes a negative sentiment, and +1 denotes a positive sentiment) to reviews (as detailed above) and select reviews which have a score above a certain threshold. We have empirically arrived at $\\\\pm 0.01$ as the threshold value. Any review with a score outside this window is selected. We also restrict the length of reviews to a maximum of 100 words to reduce sparsity.', 'After filtering out reviews with sentiment score less than the threshold value, we are left with a minimum of 8000 reviews per domain. We train on 7500 reviews form each domain and test on 500 reviews. To compare a domain-pair $(D_1,D_2)$ where $D_1$ is the source domain and $D_2$ is the target domain, we compute Angular Similarity between two vectors $V_1$ and $V_2$. $V_1$ is obtained by taking an average over 500 test vectors (from $D_1$) inferred from the model trained on $D_1$. $V_2$ is obtained in a similar manner, except that the test data is from $D_2$. Figure FIGREF30 shows the experimental setup for this metric.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe', 'Both Word2Vec and GloVe learn vector representations of words from their co-occurrence information. However, GloVe is different in the sense that it is a count-based model. In this metric, we use GloVe embeddings for adjectives shared by domain-pairs. We train GloVe models for each domain over 50 epochs, for 50 dimensions with a learning rate of 0.05. For computing similarity of a domain-pair, we follow the same procedure as described under the Word2Vec metric. The final similarity value is obtained using equation (DISPLAY_FORM29).', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText', 'We train monolingual word embeddings-based models for each domain using the FastText library. We train these models with 100 dimensions and 0.1 as the learning rate. The size of the context window is limited to 5 since FastText also uses sub-word information. Our model takes into account character n-grams from 3 to 6 characters, and we train our model over 5 epochs. We use the default loss function (softmax) for training.', 'We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\\\pm 0.01$.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo', 'We use the pre-trained deep contextualized word representation model provided by the ELMo library. Unlike Word2Vec, GloVe, and FastText, ELMo gives multiple embeddings for a word based on different contexts it appears in the corpus.', 'In ELMo, higher-level LSTM states capture the context-dependent aspects of word meaning. Therefore, we use only the topmost layer for word embeddings with 1024 dimensions. Multiple contextual embeddings of a word are averaged to obtain a single vector. We again use average Angular Similarity of word embeddings for common adjectives to compare domain-pairs along with Jaccard Similarity Coefficient. The final similarity value is obtained using equation (DISPLAY_FORM29).', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM7: Universal Sentence Encoder', 'One of the most recent contributions to the area of sentence embeddings is the Universal Sentence Encoder. Its transformer-based sentence encoding model constructs sentence embeddings using the encoding sub-graph of the transformer architecture BIBREF19. We leverage these embeddings and devise a metric for our work.', 'We extract sentence vectors of reviews in each domain using tensorflow-hub model toolkit. The dimensions of each vector are 512. To find out the similarity between a domain-pair, we extract top 500 reviews from both domains based on the sentiment score acquired using SentiWordnet (as detailed above) and average over them to get two vectors with 512 dimensions each. After that, we find out the Angular Similarity between these vectors to rank all source domains for a particular target domain in decreasing order of similarity.']\n",
            "['In table TABREF6, we present the n-gram percent match among the domain data used in our experiments. We observe that the n-gram match from among this corpora is relatively low and simple corpus similarity measures which use orthographic techniques cannot be used to obtain domain similarity. Hence, we propose the use of the metrics detailed below to perform our experiments.', 'We use a total of 11 metrics over two scenarios: the first that uses labelled data, while the second that uses unlabelled data.', 'We explain all our metrics in detail later in this section. These 11 metrics can also be classified into two categories:', 'Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings.', 'Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.', 'Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap', \"All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\\\chi ^2$ value greater than or equal to 1. The $\\\\chi ^2$ value is calculated as follows:\", 'Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)', \"KL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,\", 'Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity', \"This metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.\", 'Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change', 'Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec', 'We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity BIBREF17. It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity distinguishes nearly parallel vectors much better, we use it instead of Cosine Similarity. We obtain a similarity value by averaging over all common adjectives. For the final similarity value of this metric, we use Jaccard Similarity Coefficient here as well:', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec', 'Doc2Vec represents each sentence by a dense vector which is trained to predict words in the sentence, given the model. It tries to overcome the weaknesses of the bag-of-words model. Similar to Word2Vec, we train Doc2Vec models on each domain to extract sentence vectors. We train the models over 100 epochs for 100 dimensions, where the learning rate is 0.025. Since we can no longer leverage adjectives for sentiment, we use SentiWordnet for assigning sentiment scores (ranging from -1 to +1 where -1 denotes a negative sentiment, and +1 denotes a positive sentiment) to reviews (as detailed above) and select reviews which have a score above a certain threshold. We have empirically arrived at $\\\\pm 0.01$ as the threshold value. Any review with a score outside this window is selected. We also restrict the length of reviews to a maximum of 100 words to reduce sparsity.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe', 'Both Word2Vec and GloVe learn vector representations of words from their co-occurrence information. However, GloVe is different in the sense that it is a count-based model. In this metric, we use GloVe embeddings for adjectives shared by domain-pairs. We train GloVe models for each domain over 50 epochs, for 50 dimensions with a learning rate of 0.05. For computing similarity of a domain-pair, we follow the same procedure as described under the Word2Vec metric. The final similarity value is obtained using equation (DISPLAY_FORM29).', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText', 'We train monolingual word embeddings-based models for each domain using the FastText library. We train these models with 100 dimensions and 0.1 as the learning rate. The size of the context window is limited to 5 since FastText also uses sub-word information. Our model takes into account character n-grams from 3 to 6 characters, and we train our model over 5 epochs. We use the default loss function (softmax) for training.', 'We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\\\pm 0.01$.', 'Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo', 'We use the pre-trained deep contextualized word representation model provided by the ELMo library. Unlike Word2Vec, GloVe, and FastText, ELMo gives multiple embeddings for a word based on different contexts it appears in the corpus.']\n",
            "['We experimented with a number of other existing manually created or automatically generated sentiment and emotion lexicons, such as the NRC Emotion Lexicon BIBREF21 and the NRC Hashtag Emotion Lexicon BIBREF22 (http://saifmohammad.com/ WebPages/lexicons.html), but did not observe any improvement in the cross-validation experiments. None of the sentiment lexicon features were effective in the cross-validation experiments on Task 1; therefore, we did not include them in the final feature set for this task.']\n",
            "['Two labeled datasets were provided to the participants: a training set containing 10,822 tweets and a development set containing 4,845 tweets. These datasets were distributed as lists of tweet IDs, and the participants needed to download the tweets using the provided Python script. However, only about 60–70% of the tweets were accessible at the time of download (May 2017). The training set contained several hundreds of duplicate or near-duplicate messages, which we decided to remove. Near-duplicates were defined as tweets containing mostly the same text but differing in user mentions, punctuation, or other non-essential context. A separate test set of 9,961 tweets was provided without labels at the evaluation period. This set was distributed to the participants, in full, by email. Table TABREF1 shows the number of instances we used for training and testing our model.', 'Two labeled datasets were provided to the participants: a training set containing 8,000 tweets and a development set containing 2,260 tweets. As for Task 1, the training and development sets were distributed through tweet IDs and a download script. Around 95% of the tweets were accessible through download. Again, we removed duplicate and near-duplicate messages. A separate test set of 7,513 tweets was provided without labels at the evaluation period. This set was distributed to the participants, in full, by email. Table TABREF7 shows the number of instances we used for training and testing our model.']\n",
            "['The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1']\n",
            "['The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0', 'The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1']\n",
            "['Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);', 'From these resources, the following domain-specific features were generated:', 'Pronoun Lexicon features: the number of tokens from the Pronoun lexicon matched in the tweet;', 'domain word embeddings: the sum of the domain word embeddings for all tokens in the tweet;', 'domain word clusters: presence of tokens from the domain word clusters.']\n",
            "['From these resources, the following domain-specific features were generated:', 'Pronoun Lexicon features: the number of tokens from the Pronoun lexicon matched in the tweet;', 'domain word embeddings: the sum of the domain word embeddings for all tokens in the tweet;', 'domain word clusters: presence of tokens from the domain word clusters.']\n",
            "['We generated features using the sentiment scores provided in the following lexicons: Hu and Liu Lexicon BIBREF17 , Norms of Valence, Arousal, and Dominance BIBREF18 , labMT BIBREF19 , and NRC Emoticon Lexicon BIBREF20 . The first three lexicons were created through manual annotation while the last one, NRC Emoticon Lexicon, was generated automatically from a large collection of tweets with emoticons. The following set of features were calculated separately for each tweet and each lexicon:', 'the number of tokens with INLINEFORM0 ;', 'the total score = INLINEFORM0 ;', 'the maximal score = INLINEFORM0 ;', 'the score of the last token in the tweet.']\n",
            "['We generated features using the sentiment scores provided in the following lexicons: Hu and Liu Lexicon BIBREF17 , Norms of Valence, Arousal, and Dominance BIBREF18 , labMT BIBREF19 , and NRC Emoticon Lexicon BIBREF20 . The first three lexicons were created through manual annotation while the last one, NRC Emoticon Lexicon, was generated automatically from a large collection of tweets with emoticons. The following set of features were calculated separately for each tweet and each lexicon:', 'the number of tokens with INLINEFORM0 ;', 'the total score = INLINEFORM0 ;', 'the maximal score = INLINEFORM0 ;', 'the score of the last token in the tweet.']\n",
            "['The following surface-form features were used:', 'INLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;', 'General-domain word embeddings:', 'dense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,', 'word embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;', 'General-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11', 'Negation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features—a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;', 'Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);', 'Punctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark.']\n",
            "['The following surface-form features were used:', 'INLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;', 'General-domain word embeddings:', 'dense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,', 'word embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;', 'General-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11', 'Negation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features—a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;', 'Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);', 'Punctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark.']\n",
            "['In this paper, we develop a BERT-based model for query-focused extractive summarization. The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.']\n",
            "[\"In order to advance query-focused summarization with limited data, we improve the summarization model with data augmentation. Specifically, we transform Wikipedia into a large-scale query-focused summarization dataset (named as WikiRef). To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. Given that Wikipedia is the largest online encyclopedia, we can automatically construct massive query-focused summarization examples.\"]\n",
            "['We used the self-critical model of BIBREF13 proposed for image captioning. In self-critical sequence training, the REINFORCE algorithm BIBREF20 is used by modifying its baseline as the greedy output of the current model. At each time-step $t$, the model predicts two words: $\\\\hat{y}_{t}$ sampled from $p(\\\\hat{y}_{t} | \\\\hat{y}_{1}, \\\\hat{y}_{2}, ..., \\\\hat{y}_{t-1}, x)$, the baseline output that is greedily generated by considering the most probable word from the vocabulary and $\\\\tilde{y}_{t}$ sampled from the $p(\\\\tilde{y}_{t} | \\\\tilde{y}_{1}, \\\\tilde{y}_{2}, ..., \\\\tilde{y}_{t-1}, x)$. This model is trained using the following loss function:', 'Using the above training objective, the model learns to generate samples with high probability and thereby increasing $r(\\\\tilde{y})$ above $r(\\\\hat{y})$. Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization.', 'Where, $p(\\\\tilde{y}_{t})=p(\\\\tilde{y}_{t} | \\\\tilde{y}_{1}, \\\\tilde{y}_{2}, ..., \\\\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\\\alpha $ corresponds to more exploration, and a lower $\\\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.']\n",
            "[\"Our results in Table TABREF26 are in agreement with BIBREF3, BIBREF14, BIBREF30, which establish the fact that providing the answer tagging features as input leads to considerable improvement in the QG system's performance. Our SharedEncoder-QG model, which is a variant of our proposed MultiHop-QG model outperforms all the baselines state-of-the-art models except Semantic-Reinforced. The proposed MultiHop-QG model achieves the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric.\"]\n",
            "['Human Evaluation: For human evaluation, we directly compare the performance of the proposed approach with NQG model. We randomly sample 100 document-question-answer triplets from the test set and ask four professional English speakers to evaluate them. We consider three modalities: naturalness, which indicates the grammar and fluency; difficulty, which measures the document-question syntactic divergence and the reasoning needed to answer the question, and SF coverage similar to the metric discussed in Section SECREF4 except we replace the supporting facts prediction network with a human evaluator and we measure the relative supporting facts coverage compared to the ground-truth supporting facts. measure the relative coverage of supporting facts in the questions with respect to the ground-truth supporting facts. SF coverage provides a measure of the extent of supporting facts used for question generation. For the first two modalities, evaluators are asked to rate the performance of the question generator on a 1–5 scale (5 for the best). To estimate the SF coverage metric, the evaluators are asked to highlight the supporting facts from the documents based on the generated question.']\n",
            "['We use the HotPotQA BIBREF11 dataset to evaluate our methods. This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer. While there exists other multi-hop datasets BIBREF9, BIBREF10, only HotPotQA dataset provides the sentence-level ground-truth labels to locate the supporting facts in the list of documents. We combine the training set ($90,564$) and development set ($7,405$) and randomly split the resulting data, with 80% for training, 10% for development, 10% for testing.']\n",
            "['We use the HotPotQA BIBREF11 dataset to evaluate our methods. This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer. While there exists other multi-hop datasets BIBREF9, BIBREF10, only HotPotQA dataset provides the sentence-level ground-truth labels to locate the supporting facts in the list of documents. We combine the training set ($90,564$) and development set ($7,405$) and randomly split the resulting data, with 80% for training, 10% for development, 10% for testing.']\n",
            "[\"To learn across representations we cast the problem as multi-task learning. mtl enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation BIBREF12 , BIBREF22 . In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks BIBREF23 , BIBREF24 , BIBREF25 , where tasks are learned together with the main task in the mtl setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to better generalization of the model. For instance, BIBREF26 have shown that semantic parsing benefits from that approach.\"]\n",
            "['For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .', 'We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.']\n",
            "['For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .', 'We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.']\n",
            "['We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).']\n",
            "['We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).']\n",
            "['We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).']\n",
            "['We start by a brief description of our NMT model in Section \"NMT Background\" . Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" .', 'The transfer learning approach we use is simple and effective. We first train an NMT model on a dataset where there is a large amount of bilingual data (e.g., French-English), which we call the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a dataset with very little bilingual data (e.g., Uzbek-English), which we call the child model. This means that the low-data NMT model will not start with random weights, but with the weights from the parent model.']\n",
            "['We start by a brief description of our NMT model in Section \"NMT Background\" . Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" .']\n",
            "['A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.']\n",
            "[\"In this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 Bleu on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of `traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.\", 'A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.']\n",
            "[\"In this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 Bleu on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of `traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.\", 'A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.']\n",
            "['The average F1-scores for the end-to-end query generation task are reported in Table TABREF35 . All these results are based on the gold standard entity/relation linking result as input. Our approach SubQG outperformed all the comparative approaches on both datasets. Furthermore, as the results shown in Table TABREF36 , it gained a more significant improvement on complex questions compared with CompQA.', 'Table TABREF40 shows the accuracy of some alternative networks for query substructure prediction (Section SECREF13 ). By removing the attention mechanism (replaced by unweighted average), the accuracy declined approximately 3%. Adding additional part of speech tag sequence of the input question gained no significant improvement. We also tried to replace the attention based BiLSTM with the network in BIBREF14 , which encodes questions with a convolutional layer followed by a max pooling layer. This approach did not perform well since it cannot capture long-term dependencies.']\n",
            "['The average F1-scores for the end-to-end query generation task are reported in Table TABREF35 . All these results are based on the gold standard entity/relation linking result as input. Our approach SubQG outperformed all the comparative approaches on both datasets. Furthermore, as the results shown in Table TABREF36 , it gained a more significant improvement on complex questions compared with CompQA.']\n",
            "['We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple. Also, we did not employ the ComplexQuestions BIBREF0 and ComplexWebQuestions BIBREF12 dataset, since the existing works on these datasets have not reported the formal query generation result, and it is difficult to separate the formal query generation component from the end-to-end KBQA systems in these works.']\n",
            "['Knowledge-based question answering (KBQA) aims to answer natural language questions over knowledge bases (KBs) such as DBpedia and Freebase. Formal query generation is an important component in many KBQA systems BIBREF0 , BIBREF1 , BIBREF2 , especially for answering complex questions. Given entity and relation linking results, formal query generation aims to generate correct executable queries, e.g., SPARQL queries, for the input natural language questions. An example question and its formal query are shown in Figure FIGREF1 . Generally speaking, formal query generation is expected to include but not be limited to have the capabilities of (i) recognizing and paraphrasing different kinds of constraints, including triple-level constraints (e.g., “movies\" corresponds to a typing constraint for the target variable) and higher level constraints (e.g., subgraphs). For instance, “the same ... as\" represents a complex structure shown in the middle of Figure FIGREF1 ; (ii) recognizing and paraphrasing aggregations (e.g., “how many\" corresponds to Count); and (iii) organizing all the above to generate an executable query BIBREF3 , BIBREF4 .', 'We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple. Also, we did not employ the ComplexQuestions BIBREF0 and ComplexWebQuestions BIBREF12 dataset, since the existing works on these datasets have not reported the formal query generation result, and it is difficult to separate the formal query generation component from the end-to-end KBQA systems in these works.']\n",
            "['We simulated the real KBQA environment by considering noisy entity/relation linking results. We firstly mixed the correct linking result for each mention with the top-5 candidates generated from EARL BIBREF6 , which is a joint entity/relation linking system with state-of-the-art performance on LC-QuAD. The result is shown in the second row of Table TABREF42 . Although the precision for first output declined 11.4%, in 85% cases we still can generate correct answer in top-5. This is because SubQG ranked query structures first and considered linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check.']\n",
            "['We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple. Also, we did not employ the ComplexQuestions BIBREF0 and ComplexWebQuestions BIBREF12 dataset, since the existing works on these datasets have not reported the formal query generation result, and it is difficult to separate the formal query generation component from the end-to-end KBQA systems in these works.']\n",
            "['We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple. Also, we did not employ the ComplexQuestions BIBREF0 and ComplexWebQuestions BIBREF12 dataset, since the existing works on these datasets have not reported the formal query generation result, and it is difficult to separate the formal query generation component from the end-to-end KBQA systems in these works.']\n",
            "['The dataset we propose contains 446 sets of parallel text. We show the level of abstraction through the number of novel words in the reference summaries, which is significantly higher than the abstractive single-document summaries created for the shared tasks of the Document Understanding Conference (DUC) in 2002 BIBREF11 , a standard dataset used for single document news summarization. Additionally, we utilize several common readability metrics to show that there is an average of a 6 year reading level difference between the original documents and the reference summaries in our legal dataset.']\n",
            "['The dataset we propose contains 446 sets of parallel text. We show the level of abstraction through the number of novel words in the reference summaries, which is significantly higher than the abstractive single-document summaries created for the shared tasks of the Document Understanding Conference (DUC) in 2002 BIBREF11 , a standard dataset used for single document news summarization. Additionally, we utilize several common readability metrics to show that there is an average of a 6 year reading level difference between the original documents and the reference summaries in our legal dataset.']\n",
            "[\"We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .\"]\n",
            "[\"We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .\"]\n",
            "['The AUD model described in BIBREF0 , BIBREF1 is a non-parametric Bayesian Hidden Markov Model (HMM). This model is topologically equivalent to a phone-loop model with two major differences:', \"In this work, we have used two variants of this original model. The first one (called HMM model in the remainder of this paper), following the analysis led in BIBREF8 , approximates the Dirichlet Process prior by a mere symmetric Dirichlet prior. This approximation, while retaining the sparsity constraint, avoids the complication of dealing with the variational treatment of the stick breaking process frequent in Bayesian non-parametric models. The second variant, which we shall denote Structured Variational AutoEncoder (SVAE) AUD, is based upon the work of BIBREF4 and embeds the HMM model into the Variational AutoEncoder framework BIBREF9 . A very similar version of the SVAE for AUD was developed independently and presented in BIBREF5 . The main noteworthy difference between BIBREF5 and our model is that we consider a fully Bayesian version of the HMM embedded in the VAE; and the posterior distribution and the VAE parameters are trained jointly using the Stochastic Variational Bayes BIBREF4 , BIBREF10 . For both variants, the prior over the HMM parameters were set to the conjugate of the likelihood density: Normal-Gamma prior for the mean and variance of the Gaussian components, symmetric Dirichlet prior over the HMM's state mixture's weights and symmetric Dirichlet prior over the acoustic units' weights. For the case of the uninformative prior, the prior was set to be vague prior with one pseudo-observation BIBREF11 .\"]\n",
            "['The AUD model described in BIBREF0 , BIBREF1 is a non-parametric Bayesian Hidden Markov Model (HMM). This model is topologically equivalent to a phone-loop model with two major differences:']\n",
            "[\"We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .\"]\n",
            "[\"We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .\"]\n",
            "[\"To demonstrate indirect gender bias we adapt a pair of methods proposed by BIBREF4. First, we test whether the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do this, we define a new subspace, $\\\\vec{b}_\\\\text{test}$, using the 23 word pairs used in the Google Analogy family test subset BIBREF14 following BIBREF1's (BIBREF1) method, and determine the 1000 most biased words in each corpus (the 500 words most similar to $\\\\vec{b}_\\\\text{test}$ and $-\\\\vec{b}_\\\\text{test}$) in the unmitigated embedding. For each debiased embedding we then project these words into 2D space with tSNE BIBREF15, compute clusters with k-means, and calculate the clusters' V-measure BIBREF16. Low values of cluster purity indicate that biased words are less clustered following bias mitigation.\"]\n",
            "[\"To demonstrate indirect gender bias we adapt a pair of methods proposed by BIBREF4. First, we test whether the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do this, we define a new subspace, $\\\\vec{b}_\\\\text{test}$, using the 23 word pairs used in the Google Analogy family test subset BIBREF14 following BIBREF1's (BIBREF1) method, and determine the 1000 most biased words in each corpus (the 500 words most similar to $\\\\vec{b}_\\\\text{test}$ and $-\\\\vec{b}_\\\\text{test}$) in the unmitigated embedding. For each debiased embedding we then project these words into 2D space with tSNE BIBREF15, compute clusters with k-means, and calculate the clusters' V-measure BIBREF16. Low values of cluster purity indicate that biased words are less clustered following bias mitigation.\"]\n",
            "['We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword. In our empirical comparison, we found that although both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed to maintain a robust representation of gender (the best variants had an error rate of 23% average when drawing non-biased analogies, suggesting that too much gender information was removed). A new variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect gender bias: following its application, previously biased words are significantly less clustered according to gender, with an average of 49% reduction in cluster purity when clustering the most biased words. We also proposed Counterfactual Data Substitution, which generally performed better than the CDA equivalents, was notably quicker to compute (as Word2Vec is linear in corpus size), and in theory allows for multiple intervention layers without a corpus becoming exponentially large.']\n",
            "['We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword. In our empirical comparison, we found that although both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed to maintain a robust representation of gender (the best variants had an error rate of 23% average when drawing non-biased analogies, suggesting that too much gender information was removed). A new variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect gender bias: following its application, previously biased words are significantly less clustered according to gender, with an average of 49% reduction in cluster purity when clustering the most biased words. We also proposed Counterfactual Data Substitution, which generally performed better than the CDA equivalents, was notably quicker to compute (as Word2Vec is linear in corpus size), and in theory allows for multiple intervention layers without a corpus becoming exponentially large.']\n",
            "[\"We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor) could be paired with a very gender-specific name (e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which would also result in incorrect pronouns, if present). If, on the other hand, only the degree of gender-specificity were considered, we would see frequent names (like James) being paired with far less frequent names (like Sybil), which would distort the overall frequency distribution of names. This might also result in the retention of a gender signal: for instance, swapping a highly frequent male name with a rare female name might simply make the rare female name behave as a new link between masculine contexts (instead of the original male name), as it rarely appears in female contexts.\"]\n",
            "['To improve CDA we make two proposals. The first, Counterfactual Data Substitution (CDS), is designed to avoid text duplication in favour of substitution. The second, the Names Intervention, is a method which can be applied to either CDA or CDS, and treats bias inherent in first names. It does so using a novel name pairing strategy that accounts for both name frequency and gender-specificity. Using our improvements, the clusters of the most biased words exhibit a reduction of cluster purity by an average of 49% across both corpora following treatment, thereby offering a partial solution to the problem of indirect bias as formalised by BIBREF4. [author=simone,color=blue!40,size=,fancyline,caption=,]first part of reaction to reviewer 4Additionally, although one could expect that the debiased embeddings might suffer performance losses in computational linguistic tasks, our embeddings remain useful for at least two such tasks, word similarity and sentiment classification BIBREF6.']\n",
            "['In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. We also introduce one further, novel task, which is designed to quantify how well the embedding spaces capture an understanding of gender using non-biased analogies. Our evaluation matrix and methodology is expanded below.']\n",
            "['In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. We also introduce one further, novel task, which is designed to quantify how well the embedding spaces capture an understanding of gender using non-biased analogies. Our evaluation matrix and methodology is expanded below.']\n",
            "['In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. We also introduce one further, novel task, which is designed to quantify how well the embedding spaces capture an understanding of gender using non-biased analogies. Our evaluation matrix and methodology is expanded below.', 'Experimental Setup ::: Direct bias', \"BIBREF0 introduce the Word Embedding Association Test (WEAT), which provides results analogous to earlier psychological work by BIBREF12 by measuring the difference in relative similarity between two sets of target words $X$ and $Y$ and two sets of attribute words $A$ and $B$. We compute Cohen's $d$ (a measure of the difference in relative similarity of the word sets within each embedding; higher is more biased), and a one-sided $p$-value which indicates whether the bias detected by WEAT within each embedding is significant (the best outcome being that no such bias is detectable). We do this for three tests proposed by BIBREF13 which measure the strength of various gender stereotypes: art–maths, arts–sciences, and careers–family.\", 'Experimental Setup ::: Indirect bias', \"To demonstrate indirect gender bias we adapt a pair of methods proposed by BIBREF4. First, we test whether the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do this, we define a new subspace, $\\\\vec{b}_\\\\text{test}$, using the 23 word pairs used in the Google Analogy family test subset BIBREF14 following BIBREF1's (BIBREF1) method, and determine the 1000 most biased words in each corpus (the 500 words most similar to $\\\\vec{b}_\\\\text{test}$ and $-\\\\vec{b}_\\\\text{test}$) in the unmitigated embedding. For each debiased embedding we then project these words into 2D space with tSNE BIBREF15, compute clusters with k-means, and calculate the clusters' V-measure BIBREF16. Low values of cluster purity indicate that biased words are less clustered following bias mitigation.\", 'Experimental Setup ::: Word similarity', 'The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers. Similarity scores in an embedding are computed as the cosine angle between word-vector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at $\\\\alpha = 0.01$.', 'Experimental Setup ::: Sentiment classification', 'Following BIBREF6, we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input BIBREF18 to Doc2Vec on the Stanford Large Movie Review dataset. The classification is performed by an SVM classifier using the document embeddings as features, trained on 40,000 labelled reviews and tested on the remaining 10,000 documents, reported as error percentage.', 'Experimental Setup ::: Non-biased gender analogies', \"When proposing WED, BIBREF1 use human raters to class gender-analogies as either biased (woman:housewife :: man:shopkeeper) or appropriate (woman:grandmother :: man::grandfather), and postulate that whilst biased analogies are undesirable, appropriate ones should remain. Our new analogy test uses the 506 analogies in the family analogy subset of the Google Analogy Test set BIBREF14 to define many such appropriate analogies that should hold even in a debiased environment, such as boy:girl :: nephew:niece. We use a proportional pair-based analogy test, which measures each embedding's performance when drawing a fourth word to complete each analogy, and report error percentage.\"]\n",
            "['While an external language model may be used to mitigate the weakness of the language modeling power of an attention-based E2E model, by either re-scoring the hypothesis or through shallow or deep fusion BIBREF2, the improvements are usually limited, and it incurs additional computational cost. Inspired by SpecAgument BIBREF3 and BERT BIBREF4, we propose a semantic mask approach to improve the strength of the language modeling power in the attention-based E2E model, which, at the same time, improves the generalization capacity of the model as well. Like SpecAugment, this approach masks out partial of the acoustic features during model training. However, instead of using a random mask as in SpecAugment, our approach masks out the whole patch of the features corresponding to an output token during training, e.g., a word or a word-piece. The motivation is to encourage the model to fill in the missing token (or correct the semantic error) based on the contextual information with less acoustic evidence, and consequently, the model may have a stronger language modeling power and is more robust to acoustic distortions.']\n",
            "['The experiment results are listed in Table TABREF21, showing a similar trend as the results in Librispeech dataset. Semantic mask is complementary to specagumentation, which enables better S2S language modeling training in an E2E model, resulting in a relative 4.5$\\\\%$ gain. The experiment proves the effectiveness of semantic mask on a different and smaller dataset.', 'As far as we know, our model is the best E2E ASR system on the Librispeech testset, which achieves a comparable result with wav2letter Transformer on test-clean dataset and a better result on test-other dataset, even though our model (75M parameters) is much smaller than the wav2letter Transformer (210M parameters). The reason might be that our semantic masking is more suitable on a noisy setting, because the input features are not reliable and the model has to predict the next token relying on previous ones and the whole context of the input. Our model is built upon the code base of ESPnet, and achieves relative $10\\\\%$ gains due to the better architecture and masking strategy. Comparing with hybrid methods, our model obtains a similar performance on the test-clean set, but is still worse than the best hybrid model on the test-other dataset.']\n",
            "['We use the Zoph_RNN toolkit to implement all our described methods. In all experiments, the encoder and decoder include two stacked LSTM layers. The word embedding dimension and the size of hidden layers are both set to 1,000. The minibatch size is set to 128. We limit the vocabulary to 30K most frequent words for both the source and target languages. Other words are replaced by a special symbol “UNK”. At test time, we employ beam search and beam size is set to 12. We use case-insensitive 4-gram BLEU score as the automatic metric BIBREF21 for translation quality evaluation.']\n",
            "['2) Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods. For Chinese-English translation, we can obtain an average improvement of 2.23 BLEU points. For English-Japanese translation, the improvement can reach 1.96 BLEU points. We further find that the phrase table is much more beneficial than bilingual lexicons to NMT.']\n",
            "['2) Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods. For Chinese-English translation, we can obtain an average improvement of 2.23 BLEU points. For English-Japanese translation, the improvement can reach 1.96 BLEU points. We further find that the phrase table is much more beneficial than bilingual lexicons to NMT.']\n",
            "['2) Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods. For Chinese-English translation, we can obtain an average improvement of 2.23 BLEU points. For English-Japanese translation, the improvement can reach 1.96 BLEU points. We further find that the phrase table is much more beneficial than bilingual lexicons to NMT.']\n",
            "['We test the proposed methods on Chinese-to-English (CH-EN) translation and English-to-Japanese (EN-JA) translation. In CH-EN translation, we test the proposed methods with two data sets: 1) small data set, which includes 0.63M sentence pairs; 2) large-scale data set, which contains about 2.1M sentence pairs. NIST 2003 (MT03) dataset is used for validation. NIST2004-2006 (MT04-06) and NIST 2008 (MT08) datasets are used for testing. In EN-JA translation, we use KFTT dataset, which includes 0.44M sentence pairs for training, 1166 sentence pairs for validation and 1160 sentence pairs for testing.']\n",
            "['We test the proposed methods on Chinese-to-English (CH-EN) translation and English-to-Japanese (EN-JA) translation. In CH-EN translation, we test the proposed methods with two data sets: 1) small data set, which includes 0.63M sentence pairs; 2) large-scale data set, which contains about 2.1M sentence pairs. NIST 2003 (MT03) dataset is used for validation. NIST2004-2006 (MT04-06) and NIST 2008 (MT08) datasets are used for testing. In EN-JA translation, we use KFTT dataset, which includes 0.44M sentence pairs for training, 1166 sentence pairs for validation and 1160 sentence pairs for testing.']\n",
            "['2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .']\n",
            "['We compare our method with other relevant methods as follows:', '1) Moses: It is a widely used phrasal SMT system BIBREF22 .', '2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .']\n",
            "['Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings. We only use word unigrams or character trigrams in our model, as it is not immediately clear how to extend the bag-of-tokens approach to methods that incorporate ordering. We compare the performance of using the 3-part L2 hinge loss to the original loss presented for each model. Across all baselines, matching performance of the model improves using the 3-part L2 hinge loss. ARC-II and Match Pyramid ranking performance is similar or lower when using the 3-part loss. Ranking performance improves for DSSM, possibly because the original approach uses only random negatives to approximate the softmax normalization. More complex models, like Match Pyramid and ARC-II, had significantly lower matching and ranking performance while taking significantly longer to train and evaluate. These models are also much harder to tune and tend to overfit.']\n",
            "['Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings. We only use word unigrams or character trigrams in our model, as it is not immediately clear how to extend the bag-of-tokens approach to methods that incorporate ordering. We compare the performance of using the 3-part L2 hinge loss to the original loss presented for each model. Across all baselines, matching performance of the model improves using the 3-part L2 hinge loss. ARC-II and Match Pyramid ranking performance is similar or lower when using the 3-part loss. Ranking performance improves for DSSM, possibly because the original approach uses only random negatives to approximate the softmax normalization. More complex models, like Match Pyramid and ARC-II, had significantly lower matching and ranking performance while taking significantly longer to train and evaluate. These models are also much harder to tune and tend to overfit.']\n",
            "['We use 11 months of search logs as training data and 1 month as evaluation. We sample 54 billion query-product training pairs. We preprocess these sampled pairs to 650 million rows by grouping the training data by query-product pairs over the entire time period and using the aggregated counts as weights for the pairs. We also decrease the training time by 3X by preprocessing the training data into tokens and using mmap to store the tokens. More details on our best practices for reducing training time can be found in Section SECREF6 .']\n",
            "['Significant efforts in toxicology research are being devoted towards developing new in vitro methods for testing chemicals due to the large number of untested chemicals in use ( INLINEFORM0 75,000-80,000 BIBREF8 , BIBREF1 ) and the cost and time required by existing in vivo methods (2-3 years and millions of dollars per chemical BIBREF8 ). To facilitate the development of novel in vitro methods and assess the adherence to existing study guidelines, a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications has recently been developed and published BIBREF1 .']\n",
            "['The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels – one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example “species” label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.']\n",
            "['To avoid selecting the same sentences across the three models we removed documents which contained less than INLINEFORM0 sentences (Table TABREF17 , row Number of documents shows how many documents satisfied this condition). In all of the experiments presented in this section, the publication full text was tokenized, lower-cased, stemmed, and stop words were removed. All models used a Bernoulli Naïve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed. The complete results obtained from leave-one-out cross validation are shown in Table TABREF17 . In all cases we report classification accuracy. In the case of the random-k sentences model the accuracy was averaged over 10 runs of the model.']\n",
            "['To avoid selecting the same sentences across the three models we removed documents which contained less than INLINEFORM0 sentences (Table TABREF17 , row Number of documents shows how many documents satisfied this condition). In all of the experiments presented in this section, the publication full text was tokenized, lower-cased, stemmed, and stop words were removed. All models used a Bernoulli Naïve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed. The complete results obtained from leave-one-out cross validation are shown in Table TABREF17 . In all cases we report classification accuracy. In the case of the random-k sentences model the accuracy was averaged over 10 runs of the model.']\n",
            "['The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels – one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example “species” label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.']\n",
            "['The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels – one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example “species” label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.']\n",
            "['The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as “nshrink\"); 2) with shrink at a rate of 0.5 (“yshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5.']\n",
            "['Our adopted learning rate decay policy is interesting because experiments in BIBREF17 showed that using a decay rate is detrimental to the resultant accuracy. Our designed experiments in Section SECREF4 reveal how CLR performs with the chosen decay policy.', 'The purpose of this section is to demonstrate the effects of applying CLR and various batch sizes to train NMT models. The experiments are performed on two translation directions (DE $\\\\rightarrow $ EN and FR $\\\\rightarrow $ EN) for IWSLT2014 and IWSLT2017 BIBREF25.', 'The data are pre-processed using functions from Moses BIBREF26. The punctuation is normalized into a standard format. After tokenization, byte pair encoding (BPE) BIBREF27 is applied to the data to mitigate the adverse effects of out-of-vocabulary (OOV) rare words. The sentences with a source-target sentence length ratio greater than 1.5 are removed to reduce potential errors from sentence misalignment. Long sentences with a length greater than 250 are also removed as a common practice. The split of the datasets produces the training, validation (valid.) and test sets presented in Table TABREF9.', 'The transformer architecture BIBREF14 from fairseq BIBREF24 is used for all the experiments. The hyperparameters are presented in Table TABREF11. We compared training under CLR with an inverse square for two popular optimizers used in machine translation tasks, Adam and SGD. All models are trained using one NVIDIA V100 GPU.', 'The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as “nshrink\"); 2) with shrink at a rate of 0.5 (“yshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5.']\n",
            "['Applying CLR has positive impacts on NMT training for both Adam and SGD. When applied to SGD, CLR exempts the needs for a big initial learning rate as it enables the optimizer to explore the local minima better. Shrinking on CLR for SGD is not desirable as a higher learning rate is required (Figure FIGREF16). It is noted that applying CLR to Adam produces consistent improvements regardless of shrink options (Figure FIGREF15). Furthermore, it can be observed that the effects of applying CLR to Adam are more significant than those of SGD, as shown in Figure FIGREF17. Similar results are obtained from our experiments on “IWSLT2017-de-en\" and “IWSLT2014-fr-en\" corpora (Figures FIGREF30 and FIGREF31 in Appendix SECREF7). The corresponding BLEU scores are presented in Table TABREF18, in which the above-mentioned effects of CLR on Adam can also be established. The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18).']\n",
            "['Applying CLR has positive impacts on NMT training for both Adam and SGD. When applied to SGD, CLR exempts the needs for a big initial learning rate as it enables the optimizer to explore the local minima better. Shrinking on CLR for SGD is not desirable as a higher learning rate is required (Figure FIGREF16). It is noted that applying CLR to Adam produces consistent improvements regardless of shrink options (Figure FIGREF15). Furthermore, it can be observed that the effects of applying CLR to Adam are more significant than those of SGD, as shown in Figure FIGREF17. Similar results are obtained from our experiments on “IWSLT2017-de-en\" and “IWSLT2014-fr-en\" corpora (Figures FIGREF30 and FIGREF31 in Appendix SECREF7). The corresponding BLEU scores are presented in Table TABREF18, in which the above-mentioned effects of CLR on Adam can also be established. The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18).', 'Batch size is regarded as a significant factor influencing deep learning models from the various CV studies detailed in Section SECREF1. It is well known to CV researchers that a large batch size is often associated with a poor test accuracy. However, the trend is reversed when the CLR policy is introduced by BIBREF22. The critical question is: does this trend of using larger batch size with CLR hold for training transformers in NMT? Furthermore, what range of batch size does the associated regularization becomes significant? This will have implications because if CLR allows using a larger batch size without compromising the generalization capability, then it will allow training speed up by using a larger batch size. From Figure FIGREF20, we see that the trend of CLR with a larger batch size for NMT training does indeed lead to better performance. Thus the phenomenon we observe in BIBREF22 for CV tasks can be carried across to NMT. In fact, using a small batch size of 256 (the green curve in Figure FIGREF20) leads to divergence, as shown by the validation loss spiraling out of control. This is in line with the need to prevent over regularization when using CLR; in this case, the small batch size of 256 adds a strong regularization effect and thus need to be avoided. This larger batch size effect afforded by CLR is certainly good news because NMT typically deals with large networks and huge datasets. The benefit of a larger batch size afforded by CLR means that training time can be cut down considerably.']\n",
            "['We collect the set of audio samples $\\\\mathbb {X}_{\\\\text{init}}=\\\\lbrace \\\\mathbf {x}_{\\\\text{init}}^{i}\\\\rbrace _{i=1}^{N_{\\\\text{init}}}$, with $N_{\\\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\\\mathbf {x}_{\\\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\\\text{ s}\\\\le d({\\\\mathbf {x}_{\\\\text{init}}^{i}})\\\\le 300$ s (where $d(\\\\mathbf {x})$ is the duration of $\\\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g. tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like “speech”, “speak”, and “woman”. We normalize $\\\\mathbf {x}^{i}_{\\\\text{init}}$ to the range $[-1, 1]$, trim the silence (60 dB below the maximum amplitude) from the beginning and end, and resample to 44.1 kHz. Finally, we keep samples that are longer than 15 s as a result of the processing. This results in $\\\\mathbb {X}^{\\\\prime }_{\\\\text{init}}=\\\\lbrace \\\\mathbf {x}_{\\\\text{init}}^{j}\\\\rbrace _{j=1}^{N^{\\\\prime }_{\\\\text{init}}},\\\\,N^{\\\\prime }_{\\\\text{init}}=9000$.']\n",
            "['We collect the set of audio samples $\\\\mathbb {X}_{\\\\text{init}}=\\\\lbrace \\\\mathbf {x}_{\\\\text{init}}^{i}\\\\rbrace _{i=1}^{N_{\\\\text{init}}}$, with $N_{\\\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\\\mathbf {x}_{\\\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\\\text{ s}\\\\le d({\\\\mathbf {x}_{\\\\text{init}}^{i}})\\\\le 300$ s (where $d(\\\\mathbf {x})$ is the duration of $\\\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g. tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like “speech”, “speak”, and “woman”. We normalize $\\\\mathbf {x}^{i}_{\\\\text{init}}$ to the range $[-1, 1]$, trim the silence (60 dB below the maximum amplitude) from the beginning and end, and resample to 44.1 kHz. Finally, we keep samples that are longer than 15 s as a result of the processing. This results in $\\\\mathbb {X}^{\\\\prime }_{\\\\text{init}}=\\\\lbrace \\\\mathbf {x}_{\\\\text{init}}^{j}\\\\rbrace _{j=1}^{N^{\\\\prime }_{\\\\text{init}}},\\\\,N^{\\\\prime }_{\\\\text{init}}=9000$.']\n",
            "[\"We use AMT and a novel three-step based framework BIBREF0 for crowdsourcing the annotation of $\\\\mathbb {X}_{\\\\text{sam}}$, acquiring the set of captions $\\\\mathbb {C}_{\\\\text{sam}}^{z}=\\\\lbrace c_{\\\\text{sam}}^{z,u}\\\\rbrace _{u=1}^{N_{\\\\text{cp}}}$ for each $\\\\mathbf {x}_{\\\\text{sam}}^{z}$, where $c_{\\\\text{sam}}^{z,u}$ is an eight to 20 words long caption for $\\\\mathbf {x}_{\\\\text{sam}}^{z}$. In a nutshell, each audio sample $\\\\mathbf {x}_{\\\\text{sam}}^{z}$ gets annotated by $N_{\\\\text{cp}}$ different annotators in the first step of the framework. The annotators have access only to $\\\\mathbf {x}_{\\\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\\\times N_{\\\\text{cp}}$ captions per $\\\\mathbf {x}_{\\\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\\\mathbf {x}_{\\\\text{sam}}^{z}$ and its $2\\\\times N_{\\\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\\\mathbf {x}_{\\\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\\\text{cp}}$ and the bottom $N_{\\\\text{cp}}$ captions. The top $N_{\\\\text{cp}}$ captions are selected as $\\\\mathbb {C}_{\\\\text{sam}}^{z}$. We manually sanitize further $\\\\mathbb {C}_{\\\\text{sam}}^{z}$, e.g. by replacing “it's” with “it is” or “its”, making consistent hyphenation and compound words (e.g. “nonstop”, “non-stop”, and “non stop”), removing words or rephrasing captions pertaining to the content of speech (e.g. “French”, “foreign”), and removing/replacing named entities (e.g. “Windex”).\"]\n",
            "['In order to provide an example of how to employ Clotho and some initial (baseline) results, we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention. The method accepts as an input a length-$T$ sequence of 64 log mel-band energies $\\\\mathbf {X}\\\\in \\\\mathbb {R}^{T\\\\times 64}$, which is used as an input to a DNN which outputs a probability distribution of words. The generated caption is constructed from the output of the DNN, as in BIBREF3. We optimize the parameters of the method using the development split of Clotho and we evaluate it using the evaluation and the testing splits, separately.']\n",
            "['In order to provide an example of how to employ Clotho and some initial (baseline) results, we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention. The method accepts as an input a length-$T$ sequence of 64 log mel-band energies $\\\\mathbf {X}\\\\in \\\\mathbb {R}^{T\\\\times 64}$, which is used as an input to a DNN which outputs a probability distribution of words. The generated caption is constructed from the output of the DNN, as in BIBREF3. We optimize the parameters of the method using the development split of Clotho and we evaluate it using the evaluation and the testing splits, separately.', 'We first extract 64 log mel-band energies, using a Hamming window of 46 ms, with 50% overlap. We tokenize the captions of the development split, using a one-hot encoding of the words. Since all the words in in the development split appear in the other two splits as well, there are no unknown tokens/words. We also employ the start- and end-of-sequence tokens ($\\\\left<\\\\text{SOS}\\\\right>$ and $\\\\left<\\\\text{EOS}\\\\right>$ respectively), in order to signify the start and end of a caption.', 'The encoder is a series of bi-directional gated recurrent units (bi-GRUs) BIBREF10, similarly to BIBREF3. The output dimensionality for the GRU layers (forward and backward GRUs have same dimensionality) is $\\\\lbrace 256, 256, 256\\\\rbrace $. The output of the encoder is processed by an attention mechanism and its output is given as an input to the decoder. The attention mechanism is a feed-forward neural network (FNN) and the decoder a GRU. Then, the output of the decoder is given as an input to another FNN with a softmax non-linearity, which acts as a classifier and outputs the probability distribution of words for the $i$-th time-step. To optimize the parameters of the employed method, we use five times each audio sample, using its five different captions as targeted outputs each time. We optimize jointly the parameters of the encoder, attention mechanism, decoder, and the classifier, using 150 epochs, the cross entropy loss, and Adam optimizer BIBREF11 with proposed hyper-parameters. Also, in each batch we pad the captions of the batch to the longest in the same batch, using the end-of-sequence token, and the input audio features to the longest ones, by prepending zeros.']\n",
            "['As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:']\n",
            "['As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:', 'Subset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 . PASCAL VOC-2008 dataset consists of 1,000 images and has been used by a number of image description systems. In total, we obtained 374 pairs (out of 750 in the original file).', 'Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).']\n",
            "['Subset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 . PASCAL VOC-2008 dataset consists of 1,000 images and has been used by a number of image description systems. In total, we obtained 374 pairs (out of 750 in the original file).', 'Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.']\n",
            "['The Stanford Natural Language Inference BIBREF7 dataset contains 570k human annotated hypothesis/premise pairs. This is the most widely used entailment dataset for natural language inference.']\n",
            "['This section briefly describes three ABSA datasets and SNLI dataset. Statistics of these datasets are shown in Table TABREF15.']\n",
            "['Intuitively, attention operation can learn the contribution of each $h_{\\\\tiny \\\\textsc {CLS}}^i$. We use a dot-product attention module to dynamically combine all intermediates:', 'where $W_h^T$ and $\\\\mathbf {q}$ are learnable weights.']\n",
            "['As shown in Table TABREF26, the results were consistent with those on ABSA. From the results, BERT-Attention and BERT-LSTM perform better than vanilla BERT$_{\\\\tiny \\\\textsc {BASE}}$. Furthermore, MT-DNN-Attention and MT-DNN-LSTM outperform vanilla MT-DNN on Dev set, and are slightly inferior to vanilla MT-DNN on Test set. As a whole, our pooling strategies generally improve the vanilla BERT-based model, which draws the same conclusion as on ABSA.']\n",
            "['Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.']\n",
            "['Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.']\n",
            "['This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers.']\n",
            "['This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers.']\n",
            "['Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.']\n",
            "['Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.']\n",
            "['Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.']\n",
            "['We have described a generic method for improving the decoding speed and BLEU score of single system NMT. Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons. Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a INLINEFORM0 CPU decoding speed up with only a minor drop in BLEU.']\n",
            "[\"The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\"]\n",
            "[\"The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\"]\n",
            "[\"The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\"]\n",
            "[\"The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\"]\n",
            "[\"The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\"]\n",
            "['To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.']\n",
            "['In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task. Training a decision tree learning algorithm involves the generation of a series of rules, split points or nodes ordered in a tree-like structure enabling the prediction of a target output value based on the values of the input features. More specifically, XGBoost, as an ensemble technique, is trained by sequentially adding a high number of individually weak but complementary classifiers to produce a robust estimator: each new model is built to be maximally correlated with the negative gradient of the loss function associated with the model assembly BIBREF44 . To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.']\n",
            "['In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task. Training a decision tree learning algorithm involves the generation of a series of rules, split points or nodes ordered in a tree-like structure enabling the prediction of a target output value based on the values of the input features. More specifically, XGBoost, as an ensemble technique, is trained by sequentially adding a high number of individually weak but complementary classifiers to produce a robust estimator: each new model is built to be maximally correlated with the negative gradient of the loss function associated with the model assembly BIBREF44 . To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.']\n",
            "['Our central dataset was collected from Twitter, an online news and social networking service. Through Twitter, users can post and interact by “tweeting\" messages with restricted length. Tweets may come with several types of metadata including information about the author\\'s profile, the detected language as well as where and when the tweet was posted. Specifically, we recorded 90,369,215 tweets written in French, posted by 1.3 Million users in the timezones GMT and GMT+1 over one year (between August 2014 to July 2015) BIBREF23 . These tweets were obtained via the Twitter Powertrack API provided by Datasift with an access rate of INLINEFORM0 . Using this dataset we built several other corpora:']\n",
            "['In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task. Training a decision tree learning algorithm involves the generation of a series of rules, split points or nodes ordered in a tree-like structure enabling the prediction of a target output value based on the values of the input features. More specifically, XGBoost, as an ensemble technique, is trained by sequentially adding a high number of individually weak but complementary classifiers to produce a robust estimator: each new model is built to be maximally correlated with the negative gradient of the loss function associated with the model assembly BIBREF44 . To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.']\n",
            "['In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task. Training a decision tree learning algorithm involves the generation of a series of rules, split points or nodes ordered in a tree-like structure enabling the prediction of a target output value based on the values of the input features. More specifically, XGBoost, as an ensemble technique, is trained by sequentially adding a high number of individually weak but complementary classifiers to produce a robust estimator: each new model is built to be maximally correlated with the negative gradient of the loss function associated with the model assembly BIBREF44 . To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.']\n",
            "[\"As a second method to estimate SES, we took a sample of Twitter users who mentioned their LinkedIn BIBREF29 profile url in their tweets or Twitter profile. Using these pointers we collected professional profile descriptions from LinkedIn by relying on an automatic crawler mainly used in Search Engine Optimization (SEO) tasks BIBREF30 . We obtained INLINEFORM0 Twitter/LinkedIn users all associated with their job title, professional skills and profile description. Apart from the advantage of working with structured data, professional information extracted from LinkedIn is significantly more reliable than Twitter's due to the high degree of social scrutiny to which each profile is exposed BIBREF31 .\"]\n",
            "['BIBREF0 however only used a single target langauge (French), and had no control over the quality of the labels extracted from back-translated connectives. In this paper, we therefore systematically compare the contribution of three target translation languages from different language families: French (a Romance language), German (from the Germanic language family) and Czech (a Slavic language). As all three of these languages are part of the EuroParl corpus, this also allows us to directly test whether higher quality can be achieved by using those instances that were consistently explicitated in several languages.', 'Europarl Corpora The parallel corpora used here are from Europarl BIBREF13 , it contains about 2.05M English-French, 1.96M English-German and 0.65M English-Czech pairs. After preprocessing, we got about 0.53M parallel sentence pairs in all these four languages.']\n",
            "['We propose to train the neural network by referencing candidates extracted by a high-recall candidate-generating parser against a potentially noisy reference source (see Figure FIGREF12 , left panel). In our application, this reference was a database containing historical time series data, which enabled us to check how well the extracted numerical data fit into time series in the database. Concretely, we compute a consistency score INLINEFORM0 that measures the degree of consistency with the database. Depending on the application, the score may for instance be a squared relative error, an absolute error, or a more complex error function. In many applications, the score INLINEFORM1 will be noisy (see below for further discussion). We threshold INLINEFORM2 to obtain binary correctness labels INLINEFORM3 . We then use the binary correctness labels INLINEFORM4 for supervised neural network training, with binary cross-entropy loss as the loss function. This allows us to train a network that can compute a pseudo-likelihood INLINEFORM5 of a given extraction candidate to agree with the database. Thus, INLINEFORM6 estimates how likely the extraction candidate is correct.']\n",
            "['We propose to train the neural network by referencing candidates extracted by a high-recall candidate-generating parser against a potentially noisy reference source (see Figure FIGREF12 , left panel). In our application, this reference was a database containing historical time series data, which enabled us to check how well the extracted numerical data fit into time series in the database. Concretely, we compute a consistency score INLINEFORM0 that measures the degree of consistency with the database. Depending on the application, the score may for instance be a squared relative error, an absolute error, or a more complex error function. In many applications, the score INLINEFORM1 will be noisy (see below for further discussion). We threshold INLINEFORM2 to obtain binary correctness labels INLINEFORM3 . We then use the binary correctness labels INLINEFORM4 for supervised neural network training, with binary cross-entropy loss as the loss function. This allows us to train a network that can compute a pseudo-likelihood INLINEFORM5 of a given extraction candidate to agree with the database. Thus, INLINEFORM6 estimates how likely the extraction candidate is correct.']\n",
            "[\"We present an information extraction architecture that augments a candidate-generating parser with a deep neural network. The candidate-generating parser may leverage constraints. At the same time, the architecture gains the neural networks's ability to leverage large amounts of data to learn complex features that are tuned for the application at hand. Our method assumes the existence of a potentially noisy source of supervision INLINEFORM0 , e.g. via consistency checks of extracted data against existing databases, or via human interaction. This supervision is used to train the neural network.\"]\n",
            "['We identified the following three linguistic markers to study across users from different socioeconomic backgrounds: Correlation with SES has been evidenced for all of them. The optional deletion of negation is typical of spoken French, whereas the omission of the mute letters marking the plural in the nominal phrase is a variable cue of French writing. The third linguistic variable is a global measure of the lexical diversity of the Twitter users. We present them here in greater detail.']\n",
            "[\"To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location. Further we limited our users to ones located throughout the French territory thus not considering others tweeting from places outside the country. This selection method provided us with $110,369$ geolocated users who are either detected as French speakers or assigned to be such by Twitter and all associated to specific 'home' GPS coordinates in France. To verify the spatial distribution of the selected population, we further assessed the correlations between the true population distributions (obtained from census data BIBREF28 ) at different administrative level and the geolocated user distribution aggregated correspondingly. More precisely, we computed the $R^2$ coefficient of variation between the inferred and official population distributions (a) at the level of 22 regions. Correlations at this level induced a high coefficient of $R^2\\\\simeq 0.89$ ( $p<10^{-2}$ ); (b) At the arrondissement level with 322 administrative units and coefficient $R^2\\\\simeq 0.87$ ( $p<10^{-2}$ ); and (c) at the canton level with 4055 units with a coefficient $R\\\\simeq 0.16$ ( $p<10^{-2}$ ). Note that the relatively small coefficient at this level is due to the interplay of the sparsity of the inferred data and the fine grained spatial resolution of cantons. All in all, we can conclude that our sample is highly representative in terms of spatial population distribution, which at the same time validate our selection method despite the potential inherent biases induced by the method taking the most frequented GPS coordinates as the user's home location.\", 'The second dataset we used was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This data corpus BIBREF29 contains a set of sociodemographic aggregated indicators, estimated from the 2010 tax return in France, for each 4 hectare ( $200m \\\\times 200m$ ) square patch across the whole French territory. Using these indicators, one can estimate the distribution of the average socioeconomic status (SES) of people with high spatial resolution. In this study, we concentrated on three indicators for each patch $i$ , which we took to be good proxies of the socioeconomic status of the people living within them. These were the $S^i_\\\\mathrm {inc}$ average yearly income per capita (in euros), the $S^i_{\\\\mathrm {own}}$ fraction of owners (not renters) of real estate, and the $S^i_\\\\mathrm {den}$ density of population defined respectively as', 'To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.']\n",
            "['Here, we consider how this global structure in the learned embeddings is related to a linearity in the training objective. In particular, linear functions have the property that INLINEFORM0 , imposing a systematic relation between the predictions we make for INLINEFORM1 , INLINEFORM2 and INLINEFORM3 . In fact, we could think of this as a form of translational symmetry where adding INLINEFORM4 to the input has the same effect on the output throughout the space.']\n",
            "['We train subword level phrase-based SMT models between related languages. Along with BPE level, we also train PBSMT models at morpheme and OS levels for comparison.']\n",
            "['We train subword level phrase-based SMT models between related languages. Along with BPE level, we also train PBSMT models at morpheme and OS levels for comparison.']\n",
            "['Table TABREF14 shows train, test and tune splits of the parallel corpora used. The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25 . Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection BIBREF26 . Language models for word-level systems were trained on the target side of training corpora plus additional monolingual corpora from various sources (See Table TABREF14 for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs.']\n",
            "['Table TABREF14 shows train, test and tune splits of the parallel corpora used. The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25 . Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection BIBREF26 . Language models for word-level systems were trained on the target side of training corpora plus additional monolingual corpora from various sources (See Table TABREF14 for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs.']\n",
            "['We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.']\n",
            "['We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.']\n",
            "['The above mentioned results for BPE units do not explore optimal values of the number of merge operations. This is the only hyper-parameter that has to be selected for BPE. We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table TABREF25 . Selecting the optimal value of merge operations lead to a modest, average increase of 1.6% and maximum increase of 3.5% in the translation accuracy over B INLINEFORM0 across different language pairs .']\n",
            "['We addressed the challenge of identifying nuances between fake news and satire. Inspired by the humor and social message aspects of satire articles, we tested two classification approaches based on a state-of-the-art contextual language model, and linguistic features of textual coherence. Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire. In particular, both methods achieved a significantly better performance than the baseline language-based method. Lastly, we studied the feature importance of our linguistic-based method to help shed light on the nuances between fake news and satire. For instance, we observed that satire articles are more sophisticated, or less easy to read, than fake news articles.']\n",
            "[\"Observing the significant features, in bold in Table TABREF3, we see a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences. Semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on Fuzzy-trace theory BIBREF13, a well-known theory of decision making under risk, gist representation of content drives individual's decision to spread misinformation online. Also among the significant features, we observe the causal connectives, that are proven to be important in text comprehension, and two indices related to the text easability and readability, both suggesting that satire articles are more sophisticated, or less easy to read, than fake news articles.\"]\n",
            "['With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner.']\n",
            "['In their work, Golbeck et al. studied whether there are differences in the language of fake news and satirical articles on the same topic that could be utilized with a word-based classification approach. A model using the Naive Bayes Multinomial algorithm is proposed in their paper which serves as the baseline in our experiments.', 'We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.']\n",
            "['In their work, Golbeck et al. studied whether there are differences in the language of fake news and satirical articles on the same topic that could be utilized with a word-based classification approach. A model using the Naive Bayes Multinomial algorithm is proposed in their paper which serves as the baseline in our experiments.']\n",
            "['To investigate the predictive power of the linguistic cues, we use those Coh-Metrix indices that were significant in both the logistic and step-wise backward elimination regression models, and train a classifier on fake news and satire articles. We tested a few classification models, including Naive Bayes, Support Vector Machine (SVM), logistic regression, and gradient boosting - among which the SVM classifier gave the best results.']\n",
            "[\"To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT's neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles.\"]\n",
            "[\"To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT's neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles.\"]\n",
            "['Previous works show that introducing extra data for distant supervised learning usually boost the model performance. For this task, we collect a large-scale Baidu Baike corpus (about 6 million sentences) for NER pre-training. As shown in figure FIGREF12, each sample contains the content and its title. These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data. Experimental results show that it improves performance.']\n",
            "['Model Description ::: Named Entity Recognition ::: Extra Corpus for NER Pretraining', 'Previous works show that introducing extra data for distant supervised learning usually boost the model performance. For this task, we collect a large-scale Baidu Baike corpus (about 6 million sentences) for NER pre-training. As shown in figure FIGREF12, each sample contains the content and its title. These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data. Experimental results show that it improves performance.']\n",
            "['(1) BERT BIBREF14 is introduced as a feature extraction layer in place of BiLSTM. We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.', 'Original google BERT is pre-trained using two unsupervised tasks, masked language model (MLM) and next sentence prediction (NSP). MLM task enables the model to capture the discriminative contextual feature. NSP task makes it possible to understand the relationship between sentence pairs, which is not directly captured by language modeling. We further design a semantic-enhanced task to enhance the performance of BERT. It incorporate previous sentence prediction and document level prediction. We pre-train BERT by combining MLM, NSP and the semantic-enhanced task together.']\n",
            "['NER (Named Entity Recognition) is the first task in the joint multi-head selection model. It is usually formulated as a sequence labeling problem using the BIO (Beginning, Inside, Outside) encoding scheme. Since there are different entity types, the tags are extended to B-type, I-type and O. Linear-chain CRF BIBREF15 is widely used for sequence labeling in deep models. In our method, CRF is built on the top of BERT. Supposed $y\\\\in {\\\\left\\\\lbrace B-type,I-type,O \\\\right\\\\rbrace }$ is the label, score function $ s(X,i)_{y_{i}} $ is the output of BERT at $ i_{th}$ character and $ b_{y_{i-1}y_{i}} $ is trainable parameters, the probability of a possible label sequence is formalized as:', 'We formulated the relation classification task as a multi-head selection problem, since each token in the sentence has multiple heads, i.e., multiple relations with other tokens. Soft label embedding of the $ i_{th}$ token $ h_{i}$ is feed into two separate fully connected layers to get the subject representation $ h_{i}^{s}$ and object representation $ h_{i}^{o}$. Given the $ i_{th}$ token ($ h_{i}^{s}$, $ h_{i}^{o}$) and the $ j_{th}$ token ($ h_{j}^{s}$, $ h_{j}^{o}$) , our task is to predict their relation:', 'Relation classification is of entity pairs level in the original multi-head selection framework. We introduce an auxiliary sentence-level relation classification prediction task to guide the feature learning process. As shown in figure FIGREF6, the final hidden state of the first token $[CLS]$ is taken to obtain a fixed-dimensional pooled representation of the input sequence. The hidden state is then feed into a multi-sigmoid layer for classification. In conclusion, our model is trained using the combined loss:']\n",
            "['Figure FIGREF6 summarizes the proposed model architecture. The model takes character sequence as input and captures contextual features using BERT. A CRF layer is applied to extract entities from the sentence. To effectively transmit information between entity recognition and relation extraction, soft label embedding is built on the top of CRF logits. To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied. We find that adding an auxiliary global relation prediction task also improve the performance.', 'Miwa et al. (2016) BIBREF16 and Bekoulis et al. (2018) BIBREF13 use the entity tags as input to relation classification layer by learning label embeddings. As reported in their experiments, an improvement of 1$\\\\sim $2% F1 is achieved with the use of label embeddings. Their mechanism is hard label embedding because they use the CRF decoding results, which have two disadvantages. On one hand, the entity recognition results are not absolutely correct since they are predicted by the model during inference. The error from the entity tags may propagate to the relation classification branch and hurt the performance. On the other hand, CRF decoding process is based on the Viterbi Algorithm, which contains an argmax operation which is not differentiable. To solve this problem, we proposed soft label embedding, which takes the logits as input to preserve probability of each entity type. Suppose $N$ is the logits dimension, i.e., the number of entity type, M is the label embedding matrix, then soft label embedding for $ i_{th}$ character can be formalized as Eq DISPLAY_FORM15:']\n",
            "['Miwa et al. (2016) BIBREF16 and Bekoulis et al. (2018) BIBREF13 use the entity tags as input to relation classification layer by learning label embeddings. As reported in their experiments, an improvement of 1$\\\\sim $2% F1 is achieved with the use of label embeddings. Their mechanism is hard label embedding because they use the CRF decoding results, which have two disadvantages. On one hand, the entity recognition results are not absolutely correct since they are predicted by the model during inference. The error from the entity tags may propagate to the relation classification branch and hurt the performance. On the other hand, CRF decoding process is based on the Viterbi Algorithm, which contains an argmax operation which is not differentiable. To solve this problem, we proposed soft label embedding, which takes the logits as input to preserve probability of each entity type. Suppose $N$ is the logits dimension, i.e., the number of entity type, M is the label embedding matrix, then soft label embedding for $ i_{th}$ character can be formalized as Eq DISPLAY_FORM15:']\n",
            "['Although there is a noticeable level of variation among languages and categories, the null hypothesis that male pronouns are not significantly more frequent than female ones was consistently rejected for all languages and all categories examined. The same is true for the null hypothesis that male pronouns are not significantly more frequent than gender neutral pronouns, with the one exception of the Basque language (which exhibits a rather strong tendency towards neutral pronouns). The null hypothesis that neutral pronouns are not significantly more frequent than female ones is accepted with much more frequency, namely for the languages Malay, Estonian, Finnish, Hungarian, Armenian and for the categories Farming & Fishing & Forestry, Healthcare, Legal, Arts & Entertainment, Education. In all three cases, the null hypothesis corresponding to the aggregate for all languages and categories is rejected. We can learn from this, in summary, that Google Translate translates male pronouns more frequently than both female and gender neutral ones, either in general for Language-Category pairs or consistently among languages and among categories (with the notable exception of the Basque idiom).']\n",
            "['SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric.']\n",
            "['SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric.']\n",
            "['We compare our method with the following state-of-the-art baselines:', 'LR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.', \"GBDT Here, we use NYU's method BIBREF7, which is the best method among all tree ensemble methods. It uses an ensemble of GBDTs with existing features of dataset and manually constructed features based on psychological theories.\", \"RNN Here, we use singsound's method BIBREF30, which is the best method among all sequence modeling methods. It uses an RNN architecture which has four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format.\", 'ours-MTL It is our encoder-decoder model without multi-task learning. Thus, we will separately train a model for each language-learning dataset.', 'In the experiments, the embedding size is set to 150 and the hidden size is also set to 150. Dropout BIBREF31 regularization is applied, where the dropout rate is set to 0.5. We use the Adam optimization algorithm with a learning rate of 0.001.']\n",
            "['We compare our method with the following state-of-the-art baselines:', 'LR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.', \"GBDT Here, we use NYU's method BIBREF7, which is the best method among all tree ensemble methods. It uses an ensemble of GBDTs with existing features of dataset and manually constructed features based on psychological theories.\", \"RNN Here, we use singsound's method BIBREF30, which is the best method among all sequence modeling methods. It uses an RNN architecture which has four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format.\"]\n",
            "['We conduct experiments on Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29. Table TABREF19 shows basic statistics of each dataset.']\n",
            "['We conduct experiments on Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29. Table TABREF19 shows basic statistics of each dataset.']\n",
            "['Explanations for a given question here take the form of a list of sentences, where each sentence is a reference to a specific table row in the table store. To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation. We call this lexically-connected set of sentences an explanation graph.']\n",
            "['Each explanation sentence is represented as a single row from a semi-structured table defined around a particular relation. Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun). The initial selection of table relations was drawn from a list of 21 common relations required for science explanations identified by Jansen et al. jansen2016:COLING on a smaller corpus, and expanded as new knowledge types were identified. Subsets of example tables are included in Figure 2 . Each explanation in this corpus contains an average of 6.3 rows.']\n",
            "['Each explanation sentence is represented as a single row from a semi-structured table defined around a particular relation. Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun). The initial selection of table relations was drawn from a list of 21 common relations required for science explanations identified by Jansen et al. jansen2016:COLING on a smaller corpus, and expanded as new knowledge types were identified. Subsets of example tables are included in Figure 2 . Each explanation in this corpus contains an average of 6.3 rows.']\n",
            "['The idea is that we use the generic test responses to determine how each model would score the types of responses the engine would typically see. While the number of alerts in any set can vary wildly, it is assumed that the set includes both normal and alert responses in the proportions we expect in production. Our baseline model is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions. This baseline model performs poorly at lower thresholds and fairly well at higher thresholds.']\n",
            "['Our training sample has vastly over-sampled alerts compared with a typical responses in order to make it easier to train an engine. This also means that a typical test train split would not necessarily be useful in determining the efficacy of our models. The metric we use to evaluate the efficacy of our model is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts.']\n",
            "['We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\\\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\\\in \\\\lbrace 0.5, 0.7\\\\rbrace $, which results in $\\\\sim $72K and $\\\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.']\n",
            "['We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\\\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\\\in \\\\lbrace 0.5, 0.7\\\\rbrace $, which results in $\\\\sim $72K and $\\\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.']\n",
            "['In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively.']\n",
            "['Experiments ::: Datasets ::: Image captioning dataset', 'In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively.']\n",
            "['We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.']\n",
            "['To evaluate our models, we run human evaluation studies on the T2 test dataset used in the CVPR 2019 Conceptual Captions Challenge. The dataset contains 1K images sampled from the Open Images Dataset BIBREF29. Note that the images in the Caption-Quality dataset are also sampled from the Open Images Dataset, but using a disjoint split. So there is no overlap between the caption ratings dataset $\\\\mathcal {D}_\\\\mathrm {CR}$ we use for training, and the T2 test set we use for evaluations.']\n",
            "['In our experiments, we use the Caption-Quality dataset BIBREF13, recently introduced for the purpose of training quality-estimation models for image captions. We re-purpose this data as our caption ratings dataset $\\\\mathcal {D}_\\\\mathrm {CR}$. The dataset is divided into training, validation and test splits containing approximately 130K, 7K and 7K rated captions, respectively. Each image has an average of 4.5 captions (generated by different models that underwent evaluation evaluation). The captions are individually rated by asking raters the question “Is this a good caption for the image?”, with the answers “NO” or “YES” mapped to a 0 or 1 score, respectively. Each image/caption pair is evaluated by 10 different human raters, and an average rating score per-caption is obtained by quantizing the resulting averages into a total of nine bins $\\\\lbrace 0, \\\\frac{1}{8} \\\\dots \\\\frac{7}{8}, 1\\\\rbrace $.']\n",
            "['Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time. Developers needed three hours, three hours, four hours, and one hour to develop the narrator chain, temporal entity, genealogy, and number normalization case studies using MERF, respectively. However, the developers of ANGE, ATEEMA, GENTREE, and NUMNORM needed two months, one and a half months, three weeks, and one week, respectively. MERF needed eight MBFs and four MREs for narrator chain, three MBFs and two MREs for temporal entity, three MBFs and three MREs for genealogy, and three MBFs, one MRE, and 57 lines of code actions for the number normalization tasks. However, ANGE, ATEEMA, GENTREE, and NUMNORM required 3,000+, 1,000+, 3,000+, and 500 lines of code, respectively.', 'FLOAT SELECTED: Table 3. MERF compared to task specific applications.']\n",
            "['We survey three case studies from the literature: (1) narrator chain, (2) temporal entity, and (3) genealogy entity extraction tasks, and we use the reported development time for the task specific techniques proposed in ANGE BIBREF43 , ATEEMA BIBREF44 , and GENTREE BIBREF31 , respectively. We also compare a MERF number normalization task to a task specific implementation.']\n",
            "[\"SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.\", 'Preprocessing ::: SESAME ::: Tokens', 'SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.']\n",
            "[\"SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.\"]\n",
            "['We define the quality evaluation problem as a standard regression task since the scores we aim to predict are continuous values. Hence we use Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to estimate the performance of diverse learning algorithms. MAE and RMSE are used to evaluate the fit quality of the learning algorithms, if they are close to zero, it means the learning algorithm fits the dataset well. DISPLAYFORM0 DISPLAYFORM1']\n",
            "['In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ), achieving INLINEFORM0 F1. tab:performance details the performance measures per class. tab:relatedwork presents current state-of-the-art results for the same dataset. The best model achieves INLINEFORM1 F1-measure, but uses encoded rules. Models which are not rule-based, achieve INLINEFORM2 and INLINEFORM3 . We argue that in combination with existing techniques (such as linguistic patterns), we can potentially achieve even better results.']\n",
            "['Text Analytics (TA): Text Classification - Function Description (D.2): analogously to (D.1), we perform clustering to group texts together that are “distributively” similar. Thus, for each retrieved web page (title and excerpt of its content), we perform the classification based on the main NER classes. We extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF. In experiments, we did not find a significant performance gain using HashingVectorizer) - Training (D.2): with this objective in mind, we trained classifiers that rely on a bag-of-words technique. We collected data using DBpedia instances to create our training dataset ( INLINEFORM0 ) and annotated each instance with the respective MUC classes, i.e. PER, ORG and LOC. Listing shows an example of a query to obtain documents of organizations (ORG class). Thereafter, we used this annotated dataset to train our model.']\n",
            "['Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), “faces” from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection. These datasets produces the training data for our set of supervised classifiers (1 for ORG, 1 for PER and 10 for LOC). We trained our classifiers using Support Vector Machines BIBREF19 once they generalize reasonably enough for the task.']\n",
            "['To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.']\n",
            "['To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.']\n",
            "['We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression.']\n",
            "['To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.']\n",
            "['To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.']\n",
            "['Satire/Parody: This category consists of content that spins true contemporary content with a satirical tone or information that makes it false. One of the four subreddits that make up this label is theonion, with headlines such as “Man Lowers Carbon Footprint By Bringing Reusable Bags Every Time He Buys Gas\". Other satirical subreddits are fakealbumcovers, satire, and waterfordwhispersnews.', 'Misleading Content: This category consists of information that is intentionally manipulated to fool the audience. Our dataset contains three subreddits in this category: propagandaposters, fakefacts, and savedyouaclick.', 'Imposter Content: This category contains the subredditsimulator subreddit, which contains bot-generated content and is trained on a large number of other subreddits. It also includes subsimulatorgpt2.', 'False Connection: Submission images in this category do not accurately support their text descriptions. We have four subreddits with this label, containing posts of images with captions that do not relate to the true meaning of the image. These include misleadingthumbnails, confusing_perspective, pareidolia, and fakehistoryporn.']\n",
            "['We provide three labels for each sample, allowing us to train for 2-way, 3-way, and 5-way classification. Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3. We provide examples from each class for 5-way classification in Figure SECREF3. The 5-way classification labels are explained below:', 'True: True content is accurate in accordance with fact. Eight of the subreddits fall into this category, such as usnews and mildlyinteresting. The former consists of posts from various news sites. The latter encompasses real photos with accurate captions. The other subreddits include photoshopbattles, nottheonion, neutralnews, pic, usanews, and upliftingnews.', 'Satire/Parody: This category consists of content that spins true contemporary content with a satirical tone or information that makes it false. One of the four subreddits that make up this label is theonion, with headlines such as “Man Lowers Carbon Footprint By Bringing Reusable Bags Every Time He Buys Gas\". Other satirical subreddits are fakealbumcovers, satire, and waterfordwhispersnews.', 'Misleading Content: This category consists of information that is intentionally manipulated to fool the audience. Our dataset contains three subreddits in this category: propagandaposters, fakefacts, and savedyouaclick.', 'Imposter Content: This category contains the subredditsimulator subreddit, which contains bot-generated content and is trained on a large number of other subreddits. It also includes subsimulatorgpt2.', 'False Connection: Submission images in this category do not accurately support their text descriptions. We have four subreddits with this label, containing posts of images with captions that do not relate to the true meaning of the image. These include misleadingthumbnails, confusing_perspective, pareidolia, and fakehistoryporn.']\n",
            "['As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches.']\n",
            "['First, following Kusner et al. Kusner2015, documents are represented by normalized bag-of-words (BOW) vectors, i.e. if a word INLINEFORM0 appears INLINEFORM1 times in a document, the weight is DISPLAYFORM0', 'where INLINEFORM0 is number of words in the document. The higher the weight, the more important the word. They assume a word embedding so that each word INLINEFORM1 has an associated vector INLINEFORM2 . The dissimilarity INLINEFORM3 between INLINEFORM4 and INLINEFORM5 is then calculated by DISPLAYFORM0']\n",
            "['Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank. The sample contains twenty thousand tweets with sentiment distribution as following:']\n",
            "['In this paper we described our system of sentiment analysis adapted to participate in SemEval task 4. The highest ranking we reached was third place on the 5-label classification task. Compared with classification with 2 and 3 labels, in which we scored lower, and the fact we used similar workflow for tasks A, B, C, we speculate that the relative success is due to our sentiment treebank ranking on a 5-label basis. This can also explain the relatively superior results in quantification of 5 categories as opposed to quantification of 2 categories.']\n",
            "['The goals of these tasks are to classify tweets sentiment regarding a given entity into five classes—very negative, negative, neutral, positive, very positive—(task C) and estimate sentiment distribution over five classes for each entity (task E). The measured metrics are macro-averaged MAE and earth-movers-distance (EMD), respectively.']\n",
            "['The goals of these tasks are to classify tweets sentiment regarding a given entity into five classes—very negative, negative, neutral, positive, very positive—(task C) and estimate sentiment distribution over five classes for each entity (task E). The measured metrics are macro-averaged MAE and earth-movers-distance (EMD), respectively.']\n",
            "['BIBREF18 used BERT for their sentence-level extractive summarization model. BIBREF19 proposed a new pre-trained model that considers document-level information for sentence-level extractive summarization. Several researchers have published pre-trained encoder-decoder models very recently BIBREF20, BIBREF1, BIBREF2. BIBREF20 pre-trained a Transformer-based pointer-generator model. BIBREF1 pre-trained a standard Transformer-based encoder-decoder model using large unlabeled data and achieved state-of-the-art results. BIBREF8 and BIBREF16 extended the BERT structure to handle seq-to-seq tasks.']\n",
            "['The decoder consists of $M$ layer decoder blocks. The inputs of the decoder are the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\\\lbrace y_1,...,y_{t-1} \\\\rbrace $. The output through the $M$ layer Transformer decoder blocks is defined as', 'In each step $t$, the $h_{dt}^M$ is projected to blackthe vocabulary space and the decoder outputs the highest probability token as the next token. The Transformer decoder block consists of a self-attention module, a context-attention module, and a two-layer feed-forward network.']\n",
            "['The encoder consists of $M$ layer encoder blocks. The input of the encoder is $X = \\\\lbrace x_i, x_2, ... x_L \\\\rbrace $. The output through the $M$ layer encoder blocks is defined as', 'The encoder block consists of a self-attention module and a two-layer feed-forward network.']\n",
            "['A basic saliency model consists of $M$-layer Transformer encoder blocks ($\\\\mathrm {Encoder}_\\\\mathrm {sal}$) and a single-layer feed-forward network. We define the saliency score of the $l$-th token ($1 \\\\le l \\\\le L$) in the source text as', 'In this study, we use two types of saliency model for combination: a shared encoder and an extractor. Each model structure is based on the basic saliency model. We describe them below.']\n",
            "['This paper reviews the area of modeling and machine learning across multiple modalities based on deep learning, particularly the combination of vision and natural language. In particular, we propose to organize the many pieces of work in the language-vision multimodal intelligence field from three aspects, which include multimodal representations, the fusion of multimodal signals, and the applications of multimodal intelligence. In the section of representations, both single modal and multimodal representations are reviewed under the key concept of embedding. The multimodal representation unifies the involved signals of different modalities into the same vector space for general downstream tasks. On multimodal fusion, special architectures, such as attention mechanism and bilinear pooling, are discussed. In the application section, three selected areas of broad interest are presented, which include image caption generation, text-to-image synthesis, and visual question answering. A set of visual reasoning methods for VQA is also discussed. Our review covers task definition, data set specification, development of commonly used methods, as well as issues and trends, and therefore can facilitate future studies in this emerging field of multimodal intelligence for our community.']\n",
            "['This paper reviews the area of modeling and machine learning across multiple modalities based on deep learning, particularly the combination of vision and natural language. In particular, we propose to organize the many pieces of work in the language-vision multimodal intelligence field from three aspects, which include multimodal representations, the fusion of multimodal signals, and the applications of multimodal intelligence. In the section of representations, both single modal and multimodal representations are reviewed under the key concept of embedding. The multimodal representation unifies the involved signals of different modalities into the same vector space for general downstream tasks. On multimodal fusion, special architectures, such as attention mechanism and bilinear pooling, are discussed. In the application section, three selected areas of broad interest are presented, which include image caption generation, text-to-image synthesis, and visual question answering. A set of visual reasoning methods for VQA is also discussed. Our review covers task definition, data set specification, development of commonly used methods, as well as issues and trends, and therefore can facilitate future studies in this emerging field of multimodal intelligence for our community.']\n",
            "['ResNet BIBREF8 solves many problems in training very deep CNNs. The key ResNet innovation is the shortcut connections shown in Figure FIGREF1. Figure FIGREF1 is a typical building block of ResNet. The input to the block, $x$, will go through both the original mapping $F(x)$ (weight layers, RELU activations and batch normalization BIBREF3) and the identity shortcut connection. The output, $y$, will be $F(x)+x$. The authors of BIBREF8 hypothesize that the so-called residual mapping of $y=F(x)+x$ should be easier to optimize than the original mapping of $y=F(x)$. The design of the special building block is motivated by the observation in BIBREF6, BIBREF7 that accuracy degrades when more layers are stacked onto an already very deep CNN model. If the added layers can be constructed as identity mappings, the deeper model should not have worse training error than the original shallower model without these added layers. The degradation actually suggests that the optimizer has difficulties in approximating identity mappings. With the identity shortcut connections in the ResNet building block, the optimizer can simply drive the layer weights toward zero to make the block identity mapping. ResNet-style CNNs have maintained state-of-the-art results and have inspired other model structures BIBREF9, BIBREF14.', 'Besides the shortcut connections shown in Figure FIGREF1, batch normalization (BN) BIBREF3 is also an important feature of ResNet. BN is designed to reduce internal covariate shift, defined as the change in the distribution of network activations due to the change in network parameters, during training. This ensures better and faster convergence of the training process. BN is achieved by whitening the input of each layer, but full whitening of each layer’s inputs is costly and not differentiable everywhere. Instead of whitening the features in layer inputs and outputs jointly, each scalar feature is normalized independently to zero mean and unit variance. For a layer with d-dimensional input $x = (x(1) . . . x(d))$, each dimension will be normalized as:', 'BIBREF0 introduces self-normalizing neural networks (SNNs) in which neuron activations automatically converge towards zero mean and unit variance. The key to inducing the self-normalizing properties in SNNs is the special activation function, the scaled exponential linear unit (SELU), formulated as:', 'with $\\\\alpha \\\\approx 1.6733$ and $\\\\lambda \\\\approx 1.0507$.']\n",
            "['We already achieve significant inference speedup by removing BN and SC from ResNet-50 as discussed in Section SECREF25. Further inference optimization for SNDCNN-50 was investigated, particularly frame-skipping and multi-threaded lazy computation.']\n",
            "['All the data used in this paper comes from Siri internal datasets (en_US and zh_CN). All the models are trained with Blockwise Model-Update Filtering (BMUF) BIBREF18 with 32 GPUs. A 4-gram language model is used in all experiments. 40 dimensional filter bank feature is extracted with 25ms window and 10ms step size. All the models use a context window of 41 frames (20-1-20) as the visible states BIBREF19.']\n",
            "['All the data used in this paper comes from Siri internal datasets (en_US and zh_CN). All the models are trained with Blockwise Model-Update Filtering (BMUF) BIBREF18 with 32 GPUs. A 4-gram language model is used in all experiments. 40 dimensional filter bank feature is extracted with 25ms window and 10ms step size. All the models use a context window of 41 frames (20-1-20) as the visible states BIBREF19.']\n",
            "['For the evaluation, we have extracted 10K random utterances from the user log data and independent annotators labeled the top three predictions of all the evaluated models for each utterance so that we can correctly compute nDCG at rank position 3.']\n",
            "['Previous work BIBREF21, BIBREF22 excludes such negative utterances from the training set. We find that it is more effective to explicitly demote the prediction confidences of the domains resulted in negative responses if they are top ranked. It is formulated as a loss function:', 'where $j$ denotes the index corresponding to the negative ground-truth domain. We demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming.']\n",
            "[\"Since the proposed approach regards the formal semantics and the task domain is not specified, we expect our study to be meaningful for a general AI that talks with human beings without making the users feel isolated. Recalling that for also humans, the reaction towards the directive and the non-directive utterance differs, our two-way approach makes sense. Along with the non-task-oriented dialog, our scheme may be useful for avoiding inadvertent ignorance of the users' will.\", 'Beyond the application to the spoken language understanding (SLU) modules within the smart agents, our approach can be utilized in making up the paraphrase corpus or supporting the semantic web search. Along with the boosted performance of recent text generation and reconstruction algorithms, we expect a large size of the dataset is furthermore constructed and be utilized with the real-life personal agents.']\n",
            "[\"Since the proposed approach regards the formal semantics and the task domain is not specified, we expect our study to be meaningful for a general AI that talks with human beings without making the users feel isolated. Recalling that for also humans, the reaction towards the directive and the non-directive utterance differs, our two-way approach makes sense. Along with the non-task-oriented dialog, our scheme may be useful for avoiding inadvertent ignorance of the users' will.\", 'Beyond the application to the spoken language understanding (SLU) modules within the smart agents, our approach can be utilized in making up the paraphrase corpus or supporting the semantic web search. Along with the boosted performance of recent text generation and reconstruction algorithms, we expect a large size of the dataset is furthermore constructed and be utilized with the real-life personal agents.']\n",
            "['In this paper, we explore these aspects in the context of Korean, a less explored, low-resource language with various non-canonical expressions. From there on, we propose a structured sentence annotation scheme which can help enrich the human-like conversation with artificial intelligence (AI). For the automation, we annotate an existing corpus and then augment the dataset to mitigate class imbalance, demonstrating the flexibility, practicality, and extensibility of the proposed methods. To further prove that the scheme is not limited to Korean, we demonstrate the methodology using English examples and supplement specific cases with Korean. To begin with, in section 2, we present the theoretical background of this study. We then discuss the detailed procedure with examples, along with an explanation of how it fits with modern natural language understanding (NLU) systems and an evaluation framework.']\n",
            "['In the above, we used an existing dataset to annotate intent arguments for questions and command utterances. During our work, we concluded that there was an imbalance in the dataset - specifically not having enough data for some utterance types. Additionally, we concluded that the amount of parallel data was not large enough for wh-question to be useful in real life, also taking into account that the extraction of arguments from wh- questions involves the abstraction of the wh-related concept. To mitigate the issues, we increased the dataset size by obtaining various types of sentences from intent arguments, specifically via human-aided sentence rewriting.']\n",
            "['The Visual Question Answering has recently witnessed a great interest and development by the group of researchers and scientists from all around the world. The recent trends are observed in the area of developing more and more real life looking datasets by incorporating the real world type questions and answers. The recent trends are also seen in the area of development of sophisticated deep learning models by better utilizing the visual cues as well as textual cues by different means. The performance of even best model is still lagging and around 60-70% only. Thus, it is still an open problem to develop better deep learning models as well as more challenging datasets for VQA. Different strategies like object level details, segmentation masks, deeper models, sentiment of the question, etc. can be considered to develop the next generation VQA models.']\n",
            "['The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.', 'Vanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.', 'Stacked Attention Networks BIBREF11: This model introduced the attention using the softmax output of the intermediate question feature. The attention between the features are stacked which helps the model to focus on the important portion of the image.', 'Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.', 'Neural-Symbolic VQA BIBREF23: Specifically made for CLEVR dataset, this model leverages the question formation and image generation strategy of CLEVR. The images are converted to structured features and the question features are converted to their original root question strategy. This feature is used to filter out the required answer.', 'Focal Visual Text Attention (FVTA) BIBREF24: This model combines the sequence of image features generated by the network, text features of the image (or probable answers) and the question. It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images.', 'Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.', 'Differential Networks BIBREF19: This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN BIBREF21. The differential modules BIBREF29 are used to refine the features in both text and images. GRU BIBREF30 is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. FIGREF5.']\n",
            "['Deep Learning Based VQA Methods', 'The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.', 'Vanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.', 'Stacked Attention Networks BIBREF11: This model introduced the attention using the softmax output of the intermediate question feature. The attention between the features are stacked which helps the model to focus on the important portion of the image.', 'Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.', 'Neural-Symbolic VQA BIBREF23: Specifically made for CLEVR dataset, this model leverages the question formation and image generation strategy of CLEVR. The images are converted to structured features and the question features are converted to their original root question strategy. This feature is used to filter out the required answer.', 'Focal Visual Text Attention (FVTA) BIBREF24: This model combines the sequence of image features generated by the network, text features of the image (or probable answers) and the question. It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images.', 'Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.', 'Differential Networks BIBREF19: This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN BIBREF21. The differential modules BIBREF29 are used to refine the features in both text and images. GRU BIBREF30 is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. FIGREF5.']\n",
            "['Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.']\n",
            "['To analyze the contributions and effects of language representation pretraining in our approach, we perform ablation tests. GloVe is the method with pretrained GloVe BIBREF22 word embeddings; w/o pretrain is our method without pre-trained embeddings (random initialization). From the evaluation results in Table TABREF11 , we observe the performance drop significantly without pretraining, which proves the effectiveness of explicit common linguistic features learning. We also notice that our model with GloVe does not achieve good performance even compared with the random initialization, which indicates that the poor generalization capability for few-shot text classification.']\n",
            "['The training procedure of our approach consists of two parts. Language Representation Pretraining. Given all the training samples, we first utilize pretraining strategies to learn task-agnostic contextualized features that capture linguistic properties to benefit downstream few-shot text classification tasks.', 'While the pretraining tasks have been designed with particular downstream tasks in mind BIBREF16 , we focus on those training tasks that seek to induce universal representations suitable for downstream few-shot learning tasks. We utilize BERT BIBREF12 as a recent study BIBREF17 has shown its potential to achieve state-of-the-art performance when fine-tuned in NLP tasks. BERT combines both word and sentence representations (via masked language model and next sentence prediction objectives) in a single very large pretrained transformer BIBREF18 . It is adapted to both word- and sentence-level tasks with task-specific layers. We feed the sentence representation into a softmax layer for text classification based on BIBREF12 .']\n",
            "['We use the multiple tasks with the multi-domain sentiment classification BIBREF19 dataset ARSC. This dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset. All model parameters are updated by backpropagation using Adam with a learning rate of 0.01. We regularize our network using dropout with a rate of 0.3 tuned using the development set.']\n",
            "['For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017.']\n",
            "['For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017.']\n",
            "['Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.']\n",
            "['Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.']\n",
            "['This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score.']\n",
            "['According to Census of India, there are 22 scheduled languages and more than 100 non scheduled languages in India. There are 462 million internet users in India and most people know more than one language. They express their feelings or emotions using more than one languages, thus generating a new code-mixed/code-switched language. The problem of code-mixing and code-switching are well studied in the field of NLP BIBREF0 , BIBREF1 . Information extraction from Indian internet user-generated texts become more difficult due to this multilingual nature. Much research has been conducted in this field such as language identification BIBREF2 , BIBREF3 , part-of-speech tagging BIBREF4 . Joshi et al. JoshiPSV16 have performed sentiment analysis in Hindi-English (HI-EN) code-mixed data and almost no work exists on sentiment analysis of Bengali-English (BN-EN) code-mixed texts. The Sentiment Analysis of Indian Language (Code-Mixed) (SAIL _Code-Mixed) is a shared task at ICON-2017. Two most popular code-mixed languages namely Hindi and Bengali mixed with English were considered for the sentiment identification task. A total of 40 participants registered for the shared task and only nine teams have submitted their predicted outputs. Out of nine unique submitted systems for evaluation, eight teams submitted fourteen runs for HI-EN dataset whereas seven teams submitted nine runs for BN-EN dataset. The training and test dataset were provided after annotating the languages and sentiment (positive, negative, and neutral) tags. The language tags were automatically annotated with the help of different dictionaries whereas the sentiment tags were manually annotated. The submitted systems are ranked using the macro average f-score.']\n",
            "['The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .']\n",
            "['The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally. Two different types of evaluation are considered and these are described below.']\n",
            "['The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally. Two different types of evaluation are considered and these are described below.', 'Overall: The macro average precision, recall, and f-score are calculated for all submitted runs.', 'Two way: Then, two way classification approach is used where the system will be evaluated on two classes. For positive sentiment calculation, the predicted negative and neutral tags are converted to other for both gold and predicted output by making the task as binary classification. Then, the macro averaged precision, recall, and f-score are calculated. Similar process is also applied for negative and neural metrics calculation.']\n",
            "['Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.']\n",
            "['NLG is the process of automatically generating coherent NL text from non-linguistic data BIBREF0. Recently, the field has seen an increased interest in the development of NLG systems focusing on verbalizing resources from SW data BIBREF1. The SW aims to make information available on the Web easier to process for machines and humans. However, the languages underlying this vision, i.e., RDF, SPARQL and OWL, are rather difficult to understand for non-expert users. For example, while the meaning of the OWL class expression Class: Professor SubClassOf: worksAt SOME University is obvious to every SW expert, this expression (“Every professor works at a university”) is rather difficult to fathom for lay persons.', 'OWL BIBREF15 is the de-facto standard for machine processable and interoperable ontologies on the SW. In its second version, OWL is equivalent to the description logic $\\\\mathcal {SROIQ}(D)$. Such expressiveness has a higher computational cost but allows the development of interesting applications such as automated reasoning BIBREF16. OWL 2 ontologies consist of the following three different syntactic categories:', 'RDF BIBREF18 uses a graph-based data model for representing knowledge. Statements in RDF are expressed as so-called triples of the form (subject, predicate, object). RDF subjects and predicates are IRI and objects are either IRI or literals. RDF literals always have a datatype that defines its possible values. A predicate denotes a property and can also be seen as a binary relation taking subject and object as arguments. For example, the following triple expresses that Albert Einstein was born in Ulm:']\n",
            "['In the collection of the dataset articles, we targeted only Japanese Wikipedia articles, since our annotators were fluent Japanese speakers. The articles were selected from Japanese Wikipedia with the condition of being hyperlinked at least 100 times from other articles in Wikipedia. We also considered the Goodness scoring measures mentioned in BIBREF9 to remove some of the unuseful articles. The collected dataset contained 120,333 Japanese Wikipedia articles in different areas, covering 141 out of 200 ENE labels.']\n",
            "['Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy BIBREF8, containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set.']\n",
            "['Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy BIBREF8, containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set.']\n",
            "['From the learned patterns, we derive some characteristic syntactic forms associated with the fact and feel that we use to discriminate between the classes. We observe distinctions between the way that different arguments are expressed, with respect to the technical and more opinionated terminologies used, which we analyze on the basis of grammatical forms and more direct syntactic patterns, such as the use of different prepositional phrases. Overall, we demonstrate how the learned patterns can be used to more precisely gather similarly-styled argument responses from a pool of unannotated responses, carrying the characteristics of factual and emotional argumentation style.']\n",
            "['Since the IAC data set contains a large number of unannotated debate forum posts, we embedd AutoSlog-TS in a bootstrapping framework to learn additional patterns. The flow diagram for the bootstrapping system is shown in Figure FIGREF10 .', 'FLOAT SELECTED: Figure 4: Flow Diagram for Bootstrapping Process']\n",
            "[\"Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .\"]\n",
            "['The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.']\n",
            "[\"Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.\"]\n",
            "['Section SECREF9 describes the quantitative performance of the models by comparing BLEU scores, while a qualitative analysis is performed in Section SECREF10 by analysing translated sentences as well as attention maps. Section SECREF25 provides the results for an ablation study done regarding the effects of BPE.']\n",
            "['Section SECREF9 describes the quantitative performance of the models by comparing BLEU scores, while a qualitative analysis is performed in Section SECREF10 by analysing translated sentences as well as attention maps. Section SECREF25 provides the results for an ablation study done regarding the effects of BPE.']\n",
            "['We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work.']\n",
            "['We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work.']\n",
            "['In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models. The results also show that the translations using BPE tokenisation outperformed translations using standard word-based tokenisation. The relative performance of Transformer to ConvS2S models agrees with what has been seen in existing NMT literature BIBREF20 . This is also the case when using BPE tokenisation as compared to standard word-based tokenisation techniques BIBREF21 .']\n",
            "['In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models. The results also show that the translations using BPE tokenisation outperformed translations using standard word-based tokenisation. The relative performance of Transformer to ConvS2S models agrees with what has been seen in existing NMT literature BIBREF20 . This is also the case when using BPE tokenisation as compared to standard word-based tokenisation techniques BIBREF21 .']\n",
            "['The publicly-available Autshumato parallel corpora are aligned corpora of South African governmental data which were created for use in machine translation systems BIBREF15 . The datasets are available for download at the South African Centre for Digital Language Resources website. The datasets were created as part of the Autshumato project which aims to provide access to data to aid in the development of open-source translation systems in South Africa.']\n",
            "['We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc. The Wine Ontology is one of the most commonly used examples of owl ontologies and involves a wide variety of owl constructs; hence, it is a good test case for systems that produce texts from owl. The Consumer Electronics and Disease Ontologies were constructed by biomedical and e-commerce experts to address real-life information needs; hence, they constitute good real-world test cases from different domains.']\n",
            "['We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc. The Wine Ontology is one of the most commonly used examples of owl ontologies and involves a wide variety of owl constructs; hence, it is a good test case for systems that produce texts from owl. The Consumer Electronics and Disease Ontologies were constructed by biomedical and e-commerce experts to address real-life information needs; hence, they constitute good real-world test cases from different domains.']\n",
            "[\"The dataset for this task was collected via crowdsourcing. Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set. Note that these numeric ratings were requested only when collecting the initial training data, not during deployment, where only natural dialogue is used.\"]\n",
            "[\"The dataset for this task was collected via crowdsourcing. Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set. Note that these numeric ratings were requested only when collecting the initial training data, not during deployment, where only natural dialogue is used.\"]\n",
            "['FLOAT SELECTED: Table 6: The accuracy of various models and baselines on the original PERSONACHAT test set.', 'Our main result, reported in Table TABREF16 , is that utilizing the deployment examples improves accuracy on the Dialogue task regardless of the number of available supervised (HH) Dialogue examples. The boost in quality is naturally most pronounced when the HH Dialogue training set is small (i.e., where the learning curve is steepest), yielding an increase of up to 9.4 accuracy points, a 31% improvement. However, even when the entire PersonaChat dataset of 131k examples is used—a much larger dataset than what is available for most dialogue tasks—adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve. It is interesting to note that the two types of deployment examples appear to provide complementary signal, with models performing best when they use both example types, despite them coming from the same conversations. We also calculated hit rates with 10,000 candidates (instead of 20), a setup more similar to the interactive setting where there may be many candidates that could be valid responses. In that setting, models trained with the deployment examples continue to outperform their HH-only counterparts by significant margins (see Appendix SECREF8 ).']\n",
            "['As shown by Table TABREF22 , even with only 1k training examples (the amount we used for the experiments in Section SECREF18 ), the trained classifier significantly outperforms both the uncertainty-based methods and our original regular expression, by as much as 0.28 and 0.42 F1 points, respectively.']\n",
            "['For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .', 'The remaining two datasets are two sub-datasets about movie reviews.', 'IMDB The movie reviews with labels of subjective or objective BIBREF28 .', 'MR The movie reviews with two classes BIBREF29 .', 'For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 .']\n",
            "['For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .', 'The remaining two datasets are two sub-datasets about movie reviews.', 'IMDB The movie reviews with labels of subjective or objective BIBREF28 .', 'MR The movie reviews with two classes BIBREF29 .', 'For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 .']\n",
            "['To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. The parameters of transferred Meta-LSTM, $\\\\theta ^{(s)}_m$ in Eq.( 33 ), are fixed and cannot be updated on the new task.', 'We demonstrate the effectiveness of our architectures on two kinds of NLP tasks: text classification and sequence tagging. Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.', 'Table 5 shows the accuracies or F1 scores on the sequence tagging datasets of our models, compared to some state-of-the-art results. As shown, our proposed Meta-LSTM performs better than our competitor models whether it is single or multi-task learning.']\n",
            "['To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. The parameters of transferred Meta-LSTM, $\\\\theta ^{(s)}_m$ in Eq.( 33 ), are fixed and cannot be updated on the new task.']\n",
            "['In this paper, inspired by recent work on dynamic parameter generation BIBREF15 , BIBREF16 , BIBREF17 , we propose a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks. The task-specific semantic composition function is generated by the meta-network. Then the task-specific composition function is used to obtain the task-specific representation of a text sequence. The difference between two sharing schemes is shown in Figure 1 . Specifically, we use two LSTMs as meta and basic (task-specific) network respectively. The meta LSTM is shared for all the tasks. The parameters of the basic LSTM are generated based on the current context by the meta LSTM, therefore the composition function is not only task-specific but also position-specific. The whole network is differentiable with respect to the model parameters and can be trained end-to-end.']\n",
            "['Experimental Setting', 'We first present the data sets used for the evaluation of the proposed approach, followed by the experimental scenario. The results are presented in Section SECREF5.', 'Experimental Setting ::: Hate Speech Data Sets', 'We use three data sets related to the hate speech.', 'Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval', 'data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).', 'Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic', 'data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.', 'Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets', 'data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.']\n",
            "['We use three data sets related to the hate speech.', 'Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval', 'data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).', 'Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic', 'data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.', 'Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets', 'data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.']\n",
            "['We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.']\n",
            "['We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.']\n",
            "['We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.']\n",
            "['We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.']\n",
            "['Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .', 'Multi-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.', 'Multi-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .']\n",
            "['Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .', 'Multi-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.', 'Multi-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .']\n",
            "['As Table TABREF35 shows, we select five benchmark datasets for text classification and design three experiment scenarios to evaluate the performances of our model.', 'Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .', 'Multi-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.', 'Multi-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .']\n",
            "['Based on the LSTM implementation of BIBREF13 , we propose a generalized multi-task learning architecture for text classification with four types of recurrent neural layers to convey information inside and among tasks. Figure FIGREF21 illustrates the structure design and information flows of our model, where three tasks are jointly learned in parallel.']\n",
            "['For extractive models, we include LEAD-$k$ which is a strong baseline for single document summarization tasks and takes the first $k$ sentences in the document as summary BIBREF5. TextRank BIBREF6 and LexRank BIBREF7 are two graph-based methods, where nodes are text units and edges are defined by a similarity measure. SumBasic BIBREF8 is a frequency-based sentence selection method, which uses a component to re-weigh the word probabilities in order to minimize redundancy. The last extractive baselines are the near state-of-the-art models C_SKIP from rossiello2017centroid and SemSenSum from antognini2019. The former exploits the capability of word embeddings to leverage semantics, whereas the latter aggregates two types of sentence embeddings using a sentence semantic relation graph, followed by a graph convolution.']\n",
            "['For extractive models, we include LEAD-$k$ which is a strong baseline for single document summarization tasks and takes the first $k$ sentences in the document as summary BIBREF5. TextRank BIBREF6 and LexRank BIBREF7 are two graph-based methods, where nodes are text units and edges are defined by a similarity measure. SumBasic BIBREF8 is a frequency-based sentence selection method, which uses a component to re-weigh the word probabilities in order to minimize redundancy. The last extractive baselines are the near state-of-the-art models C_SKIP from rossiello2017centroid and SemSenSum from antognini2019. The former exploits the capability of word embeddings to leverage semantics, whereas the latter aggregates two types of sentence embeddings using a sentence semantic relation graph, followed by a graph convolution.']\n",
            "['We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3. We use implementations from fairseq and tensor2tensor. As the corpus size is too large to train extractive and abstractive models in an end-to-end manner due to hardware constraints, we use Tf-Idf to coarsely select sentences before training similarly to wiki2018. We limit the input size to 2K tokens so that all models can be trained on a Titan Xp GPU (12GB GPU RAM). We run all models with their best reported parameters.']\n",
            "['We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3. We use implementations from fairseq and tensor2tensor. As the corpus size is too large to train extractive and abstractive models in an end-to-end manner due to hardware constraints, we use Tf-Idf to coarsely select sentences before training similarly to wiki2018. We limit the input size to 2K tokens so that all models can be trained on a Titan Xp GPU (12GB GPU RAM). We run all models with their best reported parameters.']\n",
            "['We crawl approximately $265\\\\,000$ professional reviews for around $72\\\\,000$ games and $26\\\\,000$ Wikipedia gameplay sections. Since there is no automatic mapping between a game to its Wikipedia page, we design some heuristics. The heuristics are the followings and applied in this order:']\n",
            "['We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context. Furthermore, it is extensible, thus other components such as neural approaches for question-answering can be easily integrated. It is also possible to collect a dialogue corpus from its iterations.']\n",
            "['We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context. Furthermore, it is extensible, thus other components such as neural approaches for question-answering can be easily integrated. It is also possible to collect a dialogue corpus from its iterations.']\n",
            "[\"Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\\\%$, $64.33\\\\%$ and $63.46\\\\%$ respectively.\", 'We also evaluated the coreference resolution model on the test-set of CALOR (Table TABREF11), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English BIBREF6. Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data.']\n",
            "['The evaluation of the individual components of the proposed system was performed outside the scope of this work. We evaluated out-of-context questions, as well as the coreference resolution module.', \"Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\\\%$, $64.33\\\\%$ and $63.46\\\\%$ respectively.\", 'We also evaluated the coreference resolution model on the test-set of CALOR (Table TABREF11), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English BIBREF6. Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data.']\n",
            "['The decoder handles an easier task than the encoder. 1) We find that adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. 2) We also compare the training time of the encoder and decoder by fixing the parameters of a well-trained decoder (encoder), and just update the parameters of the encoder (decoder). We found that the decoder converges faster than the encoder. These two results suggest that the decoder handles an easier task than the encoder in NMT.']\n",
            "['The decoder handles an easier task than the encoder. 1) We find that adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. 2) We also compare the training time of the encoder and decoder by fixing the parameters of a well-trained decoder (encoder), and just update the parameters of the encoder (decoder). We found that the decoder converges faster than the encoder. These two results suggest that the decoder handles an easier task than the encoder in NMT.']\n",
            "['We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. We find that the preceding tokens in the decoder provide strong conditional information, which partially explain the previous two observations on the decoder.']\n",
            "['The decoder in NMT model typically acts as a conditional language model, which generates tokens highly depending on the previous tokens, like the standard language model BIBREF25. We guess the conditional information (especially the tokens right before the predicted token) is too strong for the decoder. Therefore, we study the impact of the previous tokens as follows. For each predicted token $w_{t}$, where $t$ is the position in the target sentence, we drop its previous token $w_{t-n}$ from the decoder input and watch the performance changes, where $n\\\\in [1, t]$ is the distance between the dropping token and the current predicted token. Note that the experiments are conducted in the inference phase and evaluated with teacher forcing. As shown in Figure FIGREF14, when dropping the token close to the predicted token, the accuracy declines more heavily than dropping the token far away, which indicates the decoder depends more on the nearby tokens.']\n",
            "['In this section, we compare the characteristics between the encoder and decoder by analyzing their robustness according to the input noise in the inference phase. We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens. The decoder in NMT model typically generates the current token one-by-one conditioned on the previous generated tokens, which suffers from error propagation BIBREF22: if a token is incorrectly predicted by the decoder, it will affect the prediction of the following tokens. Adding input noise to the decoder will further enhance the effect of error propagation, and thus influence our analysis. To eliminate the influence of error propagation, we apply teacher forcing BIBREF23 in the inference phase by feeding the previous ground-truth target tokens instead of the previously generated target tokens, following BIBREF24. We evaluate our model on IWSLT14 De$\\\\leftrightarrow $En, IWSLT14 Ro$\\\\leftrightarrow $En and WMT17 Chinese$\\\\leftrightarrow $English (Zh$\\\\leftrightarrow $En) translation tasks. More details on experimental configurations are described in supplementary materials (Section 1.2). The results are demonstrated in Figure FIGREF10. It can be seen that as the perturbation rate increases, adding different types of noise to the decoder input consistently achieves lower translation quality than adding noise to the encoder input.']\n",
            "['In this section, we compare the characteristics between the encoder and decoder by analyzing their robustness according to the input noise in the inference phase. We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens. The decoder in NMT model typically generates the current token one-by-one conditioned on the previous generated tokens, which suffers from error propagation BIBREF22: if a token is incorrectly predicted by the decoder, it will affect the prediction of the following tokens. Adding input noise to the decoder will further enhance the effect of error propagation, and thus influence our analysis. To eliminate the influence of error propagation, we apply teacher forcing BIBREF23 in the inference phase by feeding the previous ground-truth target tokens instead of the previously generated target tokens, following BIBREF24. We evaluate our model on IWSLT14 De$\\\\leftrightarrow $En, IWSLT14 Ro$\\\\leftrightarrow $En and WMT17 Chinese$\\\\leftrightarrow $English (Zh$\\\\leftrightarrow $En) translation tasks. More details on experimental configurations are described in supplementary materials (Section 1.2). The results are demonstrated in Figure FIGREF10. It can be seen that as the perturbation rate increases, adding different types of noise to the decoder input consistently achieves lower translation quality than adding noise to the encoder input.']\n",
            "['That said, the notion of abuse has proven elusive and difficult to formalize. Different norms across (online) communities can affect what is considered abusive BIBREF1 . In the context of natural language, abuse is a term that encompasses many different types of fine-grained negative expressions. For example, Nobata et al. nobata use it to collectively refer to hate speech, derogatory language and profanity, while Mishra et al. mishra use it to discuss racism and sexism. The definitions for different types of abuse tend to be overlapping and ambiguous. However, regardless of the specific type, we define abuse as any expression that is meant to denigrate or offend a particular person or group. Taking a course-grained view, Waseem et al. W17-3012 classify abuse into broad categories based on explicitness and directness. Explicit abuse comes in the form of expletives, derogatory words or threats, while implicit abuse has a more subtle appearance characterized by the presence of ambiguous terms and figures of speech such as metaphor or sarcasm. Directed abuse targets a particular individual as opposed to generalized abuse, which is aimed at a larger group such as a particular gender or ethnicity. This categorization exposes some of the intricacies that lie within the task of automated abuse detection. While directed and explicit abuse is relatively straightforward to detect for humans and machines alike, the same is not true for implicit or generalized abuse. This is illustrated in the works of Dadvar et al. davdar and Waseem and Hovy waseemhovy: Dadvar et al. observed an inter-annotator agreement of INLINEFORM0 on their cyber-bullying dataset. Cyber-bullying is a classic example of directed and explicit abuse since there is typically a single target who is harassed with personal attacks. On the other hand, Waseem and Hovy noted that INLINEFORM1 of all the disagreements in annotation of their dataset occurred on the sexism class. Sexism is typically both generalized and implicit.']\n",
            "['That said, the notion of abuse has proven elusive and difficult to formalize. Different norms across (online) communities can affect what is considered abusive BIBREF1 . In the context of natural language, abuse is a term that encompasses many different types of fine-grained negative expressions. For example, Nobata et al. nobata use it to collectively refer to hate speech, derogatory language and profanity, while Mishra et al. mishra use it to discuss racism and sexism. The definitions for different types of abuse tend to be overlapping and ambiguous. However, regardless of the specific type, we define abuse as any expression that is meant to denigrate or offend a particular person or group. Taking a course-grained view, Waseem et al. W17-3012 classify abuse into broad categories based on explicitness and directness. Explicit abuse comes in the form of expletives, derogatory words or threats, while implicit abuse has a more subtle appearance characterized by the presence of ambiguous terms and figures of speech such as metaphor or sarcasm. Directed abuse targets a particular individual as opposed to generalized abuse, which is aimed at a larger group such as a particular gender or ethnicity. This categorization exposes some of the intricacies that lie within the task of automated abuse detection. While directed and explicit abuse is relatively straightforward to detect for humans and machines alike, the same is not true for implicit or generalized abuse. This is illustrated in the works of Dadvar et al. davdar and Waseem and Hovy waseemhovy: Dadvar et al. observed an inter-annotator agreement of INLINEFORM0 on their cyber-bullying dataset. Cyber-bullying is a classic example of directed and explicit abuse since there is typically a single target who is harassed with personal attacks. On the other hand, Waseem and Hovy noted that INLINEFORM1 of all the disagreements in annotation of their dataset occurred on the sexism class. Sexism is typically both generalized and implicit.']\n",
            "['The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table TABREF28 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website). This was then changed after the evaluation deadline to equation EQREF25 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in BIBREF18 (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).', \"As you can see from the results table TABREF28 , the difference between the metrics is quite substantial. This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e. company) within them. Within those top 50 errors we found that the BLSTM systems do not know which parts of the sentence are associated to the company the sentiment is with respect to. Also they do not know the strength/existence of certain sentiment words.\"]\n",
            "['The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table TABREF28 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website). This was then changed after the evaluation deadline to equation EQREF25 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in BIBREF18 (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).', \"As you can see from the results table TABREF28 , the difference between the metrics is quite substantial. This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e. company) within them. Within those top 50 errors we found that the BLSTM systems do not know which parts of the sentence are associated to the company the sentiment is with respect to. Also they do not know the strength/existence of certain sentiment words.\"]\n",
            "['We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language.']\n",
            "['We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language.']\n",
            "['We further show that regularised adaptation with EWC can reduce bias while limiting degradation in general translation quality. We also present a lattice rescoring procedure in which initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored by the adapted model. We believe this approach, rescoring with models targeted to remove bias, is novel in NMT. The rescoring procedure improves WinoMT accuracy by up to 30% with no decrease in BLEU on the general test set.']\n",
            "['WinoMT provides an evaluation framework for translation from English to eight diverse languages. We select three pairs for experiments: English to German (en-de), English to Spanish (en-es) and English to Hebrew (en-he). Our selection covers three language groups with varying linguistic properties: Germanic, Romance and Semitic. Training data available for each language pair also varies in quantity and quality. We filter training data based on parallel sentence lengths and length ratios.']\n",
            "['WinoMT provides an evaluation framework for translation from English to eight diverse languages. We select three pairs for experiments: English to German (en-de), English to Spanish (en-es) and English to Hebrew (en-he). Our selection covers three language groups with varying linguistic properties: Germanic, Romance and Semitic. Training data available for each language pair also varies in quantity and quality. We filter training data based on parallel sentence lengths and length ratios.']\n",
            "['WinoMT evaluation extracts the grammatical gender of the primary entity from each translation hypothesis by automatic word alignment followed by morphological analysis. WinoMT then compares the translated primary entity with the gold gender, with the objective being a correctly gendered translation. The authors emphasise the following metrics over the challenge set:', 'Accuracy – percentage of hypotheses with the correctly gendered primary entity.', '$\\\\mathbf {\\\\Delta G}$ – difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities.', \"$\\\\mathbf {\\\\Delta S}$ – difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities, as determined by BIBREF5 using US labour statistics. For example, the `pro' set contains male doctors and female nurses, while `anti' contains female doctors and male nurses.\", 'We note that $\\\\Delta S$ can be significantly skewed by very biased systems. A model that generates male forms for almost all test sentences, stereotypical roles or not, will have an extremely low $\\\\Delta S$, since its pro- and anti-stereotypical class accuracy will both be about 50%. Consequently we also report:', 'M:F – ratio of hypotheses with male predictions to those with female predictions.', 'Finally, we wish to reduce gender bias without reducing translation performance. We report BLEU BIBREF22 on separate, general test sets for each language pair. WinoMT is designed to work without target language references, and so it is not possible to measure translation performance on this set by measures such as BLEU.']\n",
            "['WinoMT evaluation extracts the grammatical gender of the primary entity from each translation hypothesis by automatic word alignment followed by morphological analysis. WinoMT then compares the translated primary entity with the gold gender, with the objective being a correctly gendered translation. The authors emphasise the following metrics over the challenge set:', '$\\\\mathbf {\\\\Delta G}$ – difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities.', \"$\\\\mathbf {\\\\Delta S}$ – difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities, as determined by BIBREF5 using US labour statistics. For example, the `pro' set contains male doctors and female nurses, while `anti' contains female doctors and male nurses.\"]\n",
            "['Regarding data, we suggest that a small, trusted gender-balanced set could allow more efficient and effective gender debiasing than a larger, noisier set. To explore this we create a tiny, handcrafted profession-based dataset for transfer learning. For contrast, we also consider fine-tuning on a counterfactual subset of the full dataset and propose a straightforward scheme for artificially gender-balancing parallel text for NMT.']\n",
            "['Unless stated above, all models were trained on the Toronto Books Corpus, which has the inter-sentential coherence required for SkipThought and FastSent. The corpus consists of 70m ordered sentences from over 7,000 books.', \"We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset BIBREF36 consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset BIBREF37 consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table TABREF15 . All available pairs are used for testing apart from the 500 SICK `trial' pairs, which are held-out for tuning hyperparameters (representation size of log-linear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations.\"]\n",
            "[\"We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset BIBREF36 consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset BIBREF37 consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table TABREF15 . All available pairs are used for testing apart from the 500 SICK `trial' pairs, which are held-out for tuning hyperparameters (representation size of log-linear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations.\"]\n",
            "['Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) BIBREF30 , movie review sentiment (MR) BIBREF31 , product reviews (CR) BIBREF32 , subjectivity classification (SUBJ) BIBREF33 , opinion polarity (MPQA) BIBREF34 and question type classification (TREC) BIBREF35 . We follow the procedure (and code) of kiros2015skip: a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined.']\n",
            "['Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) BIBREF30 , movie review sentiment (MR) BIBREF31 , product reviews (CR) BIBREF32 , subjectivity classification (SUBJ) BIBREF33 , opinion polarity (MPQA) BIBREF34 and question type classification (TREC) BIBREF35 . We follow the procedure (and code) of kiros2015skip: a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined.']\n",
            "['We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.']\n",
            "['We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine distance.']\n",
            "[\"Advances in deep learning algorithms, software and hardware mean that many architectures and objectives for learning distributed sentence representations from unlabelled data are now available to NLP researchers. We have presented the first (to our knowledge) systematic comparison of these methods. We showed notable variation in the performance of approaches across a range of evaluations. Among other conclusions, we found that the optimal approach depends critically on whether representations will be applied in supervised or unsupervised settings - in the latter case, fast, shallow BOW models can still achieve the best performance. Further, we proposed two new objectives, FastSent and Sequential Denoising Autoencoders, which perform particularly well on specific tasks (MSRP and SICK sentence relatedness respectively). If the application is unknown, however, the best all round choice may be DictRep: learning a mapping of pre-trained word embeddings from the word-phrase signal in dictionary definitions. While we have focused on models using naturally-occurring training data, in future work we will also consider supervised architectures (including convolutional, recursive and character-level models), potentially training them on multiple supervised tasks as an alternative way to induce the 'general knowledge' needed to give language technology the elusive human touch.\"]\n",
            "['In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model. In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.']\n",
            "['In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model. In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.']\n",
            "['In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model. In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.']\n",
            "['The Penn Discourse Treebank is the corpus which is over one million words from the Wall Street Journal BIBREF10 , annotated with discourse relations. The table one shows the discourse relation extracted from PDTB. Arg1 is shown in italicized, Arg2 is shown in bold. The discourse connective is underlined.']\n",
            "['We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset. These settings make it possible to explicitly evaluate models both on the quality of domain-aware text representation and on their adaptation ability to derive reasonable representations in unfamiliar domains.']\n",
            "['We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset. These settings make it possible to explicitly evaluate models both on the quality of domain-aware text representation and on their adaptation ability to derive reasonable representations in unfamiliar domains.']\n",
            "['The recently proposed dataset Newsroom BIBREF16 is used, which was scraped from 38 major news publications. We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22. To obtain the ground truth labels for extractive summarization task, we follow the greedy approach introduced by BIBREF1. Finally, we randomly divide ten domains into two groups, one for training and the other for test. We call this re-purposed subset of Newsroom MULTI-SUM to indicate it is specially designed for multi-domain learning in summarization tasks.', 'From Table TABREF6, we can find that data from those news publications vary in indicators that are closely relevant to summarization. This means that (document, summary) pairs from different publications will have unique summarization formation, and models might need to learn different semantic features for different publications. Furthermore, we follow the simple experiment by BIBREF23 to train a classifier for the top five domains. A simple classification model with GloVe initializing words can also achieve 74.84% accuracy (the chance is 20%), which ensures us that there is a built-in bias in each publication. Therefore, it is reasonable to view one publication as a domain and use our multi-publication MULTI-SUM as a multi-domain dataset.']\n",
            "['In this paper, we focus on the extractive summarization and demonstrate that news publications can cause data distribution differences, which means that they can also be defined as domains. Based on this, we re-purpose a multi-domain summarization dataset MULTI-SUM and further explore the issue of domain shift.']\n",
            "['The recently proposed dataset Newsroom BIBREF16 is used, which was scraped from 38 major news publications. We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22. To obtain the ground truth labels for extractive summarization task, we follow the greedy approach introduced by BIBREF1. Finally, we randomly divide ten domains into two groups, one for training and the other for test. We call this re-purposed subset of Newsroom MULTI-SUM to indicate it is specially designed for multi-domain learning in summarization tasks.', 'From Table TABREF6, we can find that data from those news publications vary in indicators that are closely relevant to summarization. This means that (document, summary) pairs from different publications will have unique summarization formation, and models might need to learn different semantic features for different publications. Furthermore, we follow the simple experiment by BIBREF23 to train a classifier for the top five domains. A simple classification model with GloVe initializing words can also achieve 74.84% accuracy (the chance is 20%), which ensures us that there is a built-in bias in each publication. Therefore, it is reasonable to view one publication as a domain and use our multi-publication MULTI-SUM as a multi-domain dataset.']\n",
            "['Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@', 'This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.', 'We achieve this by pre-training our basic model $Model^{I}_{Base}$ with BERT BIBREF28, which is one of the most successful learning frameworks. Then we investigate if BERT can provide domain information and bring the model good domain adaptability. To avoid introducing new structures, we use the feature-based BERT with its parameters fixed.', 'Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@', 'The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.', 'Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@', 'In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35.']\n",
            "['Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@', 'This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.', 'Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@', 'More recently, unsupervised pre-training has achieved massive success in NLP community BIBREF28, BIBREF29, which usually provides tremendous external knowledge. However, there are few works on building the connection between large-scale pre-trained models and multi-domain learning. In this model, we explore how the external knowledge unsupervised pre-trained models bring can contribute to multi-domain learning and new domain adaption .', 'Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@', 'The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.', 'Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@', 'In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35.']\n",
            "[\"Table TABREF25 summarizes the results for POS tagging. We find that neither M-BERT model improves on the previous state of the art for any of the three resources, with results ranging 0.1-0.8% points below the best previously published results. By contrast, both language-specific models outperform the previous state of the art, with absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points. While these improvements over the already very high reference results are modest in absolute terms, the relative reductions in errors are notable: in particular, the FinBERT cased error rate on FTB is less than half of the best CoNLL'18 result BIBREF22. We also note that the uncased models are surprisingly competitive with their cased equivalents for a task where capitalization has long been an important feature: for example, FinBERT uncased performance is within approx. 0.1% points of FinBERT cased for all corpora.\", 'Table TABREF41 shows LAS results for predicted and gold segmentation. While Udify initialized with M-BERT fails to outperform our strongest baseline BIBREF22, Udify initialized with FinBERT achieves notably higher performance on all three treebanks, establishing new state-of-the-art parsing results for Finnish with a large margin. Depending on the treebank, Udify with cased FinBERT LAS results are 2.3–3.6% points above the previous state of the art, decreasing errors by 24%–31% relatively.', 'Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest.']\n",
            "['The current transfer learning methods have evolved from word embedding techniques, such as word2vec BIBREF5, GLoVe BIBREF6 and fastText BIBREF7, to take into account the textual context of words. Crucially, incorporating the context avoids the obvious limitations stemming from the one-vector-per-unique-word assumption inherent to the previous word embedding methods. The current successful wave of work proposing and applying different contextualized word embeddings was launched with ELMo BIBREF0, a context embedding method based on bidirectional LSTM networks. Another notable example is the ULMFit model BIBREF8, which specifically focuses on techniques for domain adaptation of LSTM-based language models. Following the introduction of the attention-based (as opposed to recurrent) Transformer architecture BIBREF9, BERT was proposed by BIBREF2, demonstrating superior performance on a broad array of tasks. The BERT model has been further refined in a number of follow-up studies BIBREF10, BIBREF11 and, presently, BERT and related models form the de facto standard approach to embedding text segments as well as individual words in context.']\n",
            "['We implement the BERT POS tagger straightforwardly by attaching a time-distributed dense output layer over the top layer of BERT and using the first piece of each wordpiece-tokenized input word to represent the word. The implementation and data processing tools are openly available. We compare POS tagging results to the best-performing methods for each corpus in the CoNLL 2018 shared task, namely that of che2018towards for TDT and FTB and lim2018sex for PUD. We report performance for the UPOS metric as implemented by the official CoNLL 2018 evaluation script.', 'We compare NER results to the rule-based FiNER-tagger BIBREF32 developed together with the FiNER corpus and to the neural network-based model of gungor2018improving targeted specifically toward morphologically rich languages. The former achieved the highest results on the corpus and the latter was the best-performing machine learning-based method in the experiments of ruokolainen2019finnish. Named entity recognition performance is evaluated in terms of exact mention-level precision, recall and F-score as implemented by the standard conlleval script, and F-score is used to compare performance.', 'We compare our results to the best-performing system in the CoNLL 2018 shared task for the LAS metric, HIT-SCIR BIBREF22. In addition to having the highest average score over all treebanks for this metric, the system also achieved the highest LAS among 26 participants for each of the three Finnish treebanks. The dependency parser used in the HIT-SCIR system is the biaffine graph-based parser of BIBREF35 with deep contextualized word embeddings (ELMo) BIBREF36 trained monolingually on web crawl and Wikipedia data provided by BIBREF37. The final HIT-SCIR model is an ensemble over three parser models trained with different parameter initializations, where the final prediction is calculated by averaging the softmaxed output scores.', 'We also compare results to the recent work of BIBREF33, where the merits of two parsing architectures, graph-based BIBREF38 and transition-based BIBREF39, are studied with two different deep contextualized embeddings, ELMo and BERT. We include results for their best-performing combination on the Finnish TDT corpus, the transition-based parser with monolingual ELMo embeddings.']\n",
            "[\"We combine two major sources of Finnish news: the Yle corpus, an archive of news published by Finland's national public broadcasting company in the years 2011-2018, and The STT corpus of newswire articles sent to media outlets by the Finnish News Agency (STT) between 1992 and 2018. The combined resources contain approx. 900 million tokens, with 20% originating from the Yle corpus and 80% from STT.\", 'The Suomi24 corpus (version 2017H2) contains all posts to the Suomi24 online discussion website from 2001 to 2017. Suomi24 is one of the largest social networking forums in Finland and covers a broad range of topics and levels of style and formality in language. The corpus is also roughly five times the size of the available news resources.', 'Two primary sources were used to create pretraining data from unrestricted crawls. First, we compiled documents from the dedicated internet crawl of the Finnish internet of luotolahti2015towards run between 2014 and 2016 using the SpiderLing crawler BIBREF16. Second, we selected texts from the Common Crawl project by running a a map-reduce language detection job on the plain text material from Common Crawl. These sources were supplemented with plain text extracted from the Finnish Wikipedia using the mwlib library. Following initial compilation, this text collection was analyzed for using the Onion deduplication tool. Duplicate documents were removed, and remaining documents grouped by their level of duplication.']\n",
            "['To provide a sufficiently large and varied unannotated corpus for pretraining, we compiled Finnish texts from three primary sources: news, online discussion, and an internet crawl. All of the unannotated texts were split into sentences, tokenized, and parsed using the Turku Neural Parser pipeline BIBREF15. Table TABREF4 summarizes the initial statistics of the three sources prior to cleanup and filtering.']\n",
            "['We first compare the performance of our approach with the baseline results obtained from Rashkin et al. BIBREF3 that uses a full transformer architecture BIBREF26, consisting of an encoder and decoder. Table TABREF9 provides a comparison of our approach with to the baseline approach. In Table TABREF9, we refer our “Our Model Fine-Tuned” as the baseline fine-tuned GPT-2 model trained on the dialogue and “Our-model Emo-prepend” as the GPT-2 model that is fine-tuned on the dialogues but also conditioned on the emotion displayed in the conversation. We find that fine-tuning the GPT-2 language model using a transfer learning approach helps us achieve a lower perplexity and a higher BLEU scores. The results from our approach are consistent with the empirical study conducted by Edunov et al BIBREF27 that demonstrate the effectiveness of the using pre-trained model diminishes when added to the decoder network in an seq2seq approach. We also perform a comparison between our two models on the metrics of length, diversity, readability and coherence. We find that our baseline model produces less diverse responses compared to when the model is conditioned on emotion. We find that the our emo-prepend model also higher a slightly higher readability score that our baseline model.']\n",
            "['Given verb phrases, we seek for the best assignment function INLINEFORM0 that minimizes the code length of phrases. Let INLINEFORM1 be the code length derived by INLINEFORM2 . The problem of verb pattern assignment thus can be formalized as below:']\n",
            "['To evaluate the effectiveness of our pattern summarization approach, we report two metrics: (1) ( INLINEFORM0 ) how much of the verb phrases in natural language our solution can find corresponding patterns (2) ( INLINEFORM1 ) how much of the phrases and their corresponding patterns are correctly matched? We compute the two metrics by: DISPLAYFORM0']\n",
            "['On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States. Moreover, people have different baseline emotional valences from one another. These claims are supported for example by the annual Gallup poll that ranks states from most happy to least happy BIBREF4 , or the work by Csikszentmihalyi and Hunter BIBREF5 that showed reported happiness varies significantly by day of week and time of day. We believe these factors manifest themselves in sentiments expressed in tweets and that by accounting for these factors, we can improve sentiment classification on Twitter.']\n",
            "['On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States. Moreover, people have different baseline emotional valences from one another. These claims are supported for example by the annual Gallup poll that ranks states from most happy to least happy BIBREF4 , or the work by Csikszentmihalyi and Hunter BIBREF5 that showed reported happiness varies significantly by day of week and time of day. We believe these factors manifest themselves in sentiments expressed in tweets and that by accounting for these factors, we can improve sentiment classification on Twitter.']\n",
            "['On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States. Moreover, people have different baseline emotional valences from one another. These claims are supported for example by the annual Gallup poll that ranks states from most happy to least happy BIBREF4 , or the work by Csikszentmihalyi and Hunter BIBREF5 that showed reported happiness varies significantly by day of week and time of day. We believe these factors manifest themselves in sentiments expressed in tweets and that by accounting for these factors, we can improve sentiment classification on Twitter.']\n",
            "['It is interesting to note that even with the noisy dataset, our ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index calculated by Oswald and Wu BIBREF25 in their work on measuring well-being and life satisfaction across America. Their data is from the behavioral risk factor survey score (BRFSS), which is a survey of life satisfaction across the United States from INLINEFORM0 million citizens. Figure FIGREF27 shows this correlation ( INLINEFORM1 , INLINEFORM2 ).']\n",
            "['Bigram frequencies are often calculated using the approximation', '$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)', \"In a much cited paper, Church and Hanks BIBREF0 use ` $=$ ' in place of ` $\\\\approx $ ' because the approximation is so good. Indeed, this approximation will only cause errors for the very few words which occur near the beginning or the end of the text. Take for example the text appearing above - the bigram (doggies, *) does not occur once, but the approximation says it does.\", 'An efficient method for computing the contingency matrix for a bigram (word1, word2) is suggested by the approximation. Store $freq(w1, w2)$ for all bigrams $(w1, w2)$ and the frequencies of all words. Then,', 'The statistical importance of miscalculations due to this method diminishes as our text grows larger and larger. Interest is growing in the analysis of small texts, however, and a means of computing bigrams for this type of corpus must be employed. This approximation is implemented in popular NLP libraries and can be seen in many tutorials across the internet. People who use this code, or write their own software, must know when it is appropriate.']\n",
            "['Bigram frequencies are often calculated using the approximation', '$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)', \"In a much cited paper, Church and Hanks BIBREF0 use ` $=$ ' in place of ` $\\\\approx $ ' because the approximation is so good. Indeed, this approximation will only cause errors for the very few words which occur near the beginning or the end of the text. Take for example the text appearing above - the bigram (doggies, *) does not occur once, but the approximation says it does.\"]\n",
            "[\"For a new text analytics application requiring feature engineering, it starts with estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order. Furthermore, if user's selections are not aligned with system's recommendations, system gradually adapts its recommendation so that eventually it can achieve alignment with user preferences.\"]\n",
            "[\"For a new text analytics application requiring feature engineering, it starts with estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order. Furthermore, if user's selections are not aligned with system's recommendations, system gradually adapts its recommendation so that eventually it can achieve alignment with user preferences.\"]\n",
            "['NLP Feature Specification Language ::: Feature Types ::: Linguistic Features', 'Figure FIGREF8 depicts two levels of taxonomy for features considered as linguistic.', 'NLP Feature Specification Language ::: Feature Types ::: Semantic Similarity and Relatedness based Features', 'Semantic similarity can be estimated between words, between phrases, between sentences, and between documents in a corpus. Estimation could either be based upon corpus text alone by applying approaches like vector space modeling BIBREF16, latent semantic analysis BIBREF17, topic modeling BIBREF18, or neural embeddings (e.g., Word2Vec BIBREF19 or Glove BIBREF20) and their extensions to phrase, sentence, and document levels. Otherwise it can be estimated based upon ontological relationships (e.g., WordNet based BIBREF21) among concept terms appearing in the corpus.', 'NLP Feature Specification Language ::: Feature Types ::: Statistical Features', 'Figure FIGREF13 depicts different types of statistical features which can be extracted for individual documents or corpus of documents together with methods to extract these features at different levels.']\n",
            "['Analysis Unit (AU) specifies level at which features have to be extracted. At Corpus level, features are extracted for all the text documents together. At Document level, features are extracted for each document in corpus separately. At Para (paragraph) level Features are extracted for multiple sentences constituting paragraphs together. At Sentence level features to be extracted for each sentence. Figure FIGREF6 depicts classes of features considered in nlpFSpL and their association with different AUs.', \"Syntactic Unit (SU) specifies unit of linguistic features. It could be a `Word' or a `Phrase', or a `N-gram' or a sequence of words matching specific lexico-syntactic pattern captured as `POS tag pattern' (e.g., Hearst pattern BIBREF15) or a sequence of words matching specific regular expression `Regex' or a combination of these. Option Regex is used for special types of terms, e.g., Dates, Numbers, etc. LOGICAL is a Boolean logical operator including AND, OR and NOT (in conjunction with other operator). For example, Phrase AND POS Regex would specify inclusion of a `Phrase' as SU when its constituents also satisfy 'regex' of `POS tags'. Similarly, POS Regex OR NOT(Regex) specifies inclusion of sequence of words as SU if it satisfies `POS tag Pattern' but does not match pattern specified by character `Regex'. Note that SU can be a feature in itself for document and corpus level analysis.\", 'Normalize Morphosyntactic Variants: If YES, variants of words including stems, lemmas, and fuzzy matches will be identified before analyzing input text for feature exaction and would be treated equivalent.']\n",
            "['Baselines (and Related Work)', 'We compare our approach against recent competing methods that report results on UD datasets.', 'The current state of the art is held by N18-1126, who, as discussed in sec:introduction, provide a direct context-to-lemma approach, avoiding the use of morphological tags. We remark that N18-1126 assume a setting where lemmata are annotated at the token level, but morphological tags are not available; we contend, however, that such a setting is not entirely realistic as almost all corpora annotated with lemmata at the token level include morpho-syntactic annotation, including the vast majority of the UD corpora. Thus, we do not consider it a stretch to assume the annotation of morphological tags to train our joint model.', 'Our next baseline is the UDPipe system of K17-3009. Their system performs lemmatization using an averaged perceptron tagger that predicts a (lemma rule, UPOS) pair. Here, a lemma rule generates a lemma by removing parts of the word prefix/suffix and prepending and appending a new prefix/suffix. A guesser first produces correct lemma rules and the tagger is used to disambiguate from them.', 'The strongest non-neural baseline we consider is the system of D15-1272, who, like us, develop a joint model of morphological tagging lemmatization. In contrast to us, however, their model is globally normalized BIBREF29 . Due to their global normalization, they directly estimate the parameters of their model with MLE without worrying about exposure bias. However, in order to efficiently normalize the model, they heuristically limit the set of possible lemmata through the use of edit trees BIBREF30 , which makes the computation of the partition function tractable.', 'Much like D15-1272, Morfette relies on the concept of edit trees. However, a simple perceptron is used for classification with hand-crafted features. A full description of the model is given in grzegorz2008learning.']\n",
            "['Baselines (and Related Work)', 'We compare our approach against recent competing methods that report results on UD datasets.', 'The current state of the art is held by N18-1126, who, as discussed in sec:introduction, provide a direct context-to-lemma approach, avoiding the use of morphological tags. We remark that N18-1126 assume a setting where lemmata are annotated at the token level, but morphological tags are not available; we contend, however, that such a setting is not entirely realistic as almost all corpora annotated with lemmata at the token level include morpho-syntactic annotation, including the vast majority of the UD corpora. Thus, we do not consider it a stretch to assume the annotation of morphological tags to train our joint model.', 'Our next baseline is the UDPipe system of K17-3009. Their system performs lemmatization using an averaged perceptron tagger that predicts a (lemma rule, UPOS) pair. Here, a lemma rule generates a lemma by removing parts of the word prefix/suffix and prepending and appending a new prefix/suffix. A guesser first produces correct lemma rules and the tagger is used to disambiguate from them.', 'The strongest non-neural baseline we consider is the system of D15-1272, who, like us, develop a joint model of morphological tagging lemmatization. In contrast to us, however, their model is globally normalized BIBREF29 . Due to their global normalization, they directly estimate the parameters of their model with MLE without worrying about exposure bias. However, in order to efficiently normalize the model, they heuristically limit the set of possible lemmata through the use of edit trees BIBREF30 , which makes the computation of the partition function tractable.', 'Much like D15-1272, Morfette relies on the concept of edit trees. However, a simple perceptron is used for classification with hand-crafted features. A full description of the model is given in grzegorz2008learning.']\n",
            "['Our experiments with the European Portuguese datasets support the hypothesis that discriminative speaker embeddings contain information relevant for disease detection. In particular, we found evidence that these embeddings contain information that KB features fail to represent, thus proving the validity of our approach. It was also observed that x-vectors are more suitable than i-vectors for tasks whose domain does not match that of the training data, such as verbal task mismatch and cross-lingual experiments. This indicates that x-vectors embeddings are a strong contender in the replacement of knowledge-based feature sets for PD and OSA detection.']\n",
            "['Our first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions. In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing.']\n",
            "['Our first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions. In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing.']\n",
            "['The TLE currently contains 5,124 sentences (97,681 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 . The sentences were obtained from the FCE corpus BIBREF1 , a collection of upper intermediate English learner essays, containing error annotations with 75 error categories BIBREF7 . Sentence level segmentation was performed using an adaptation of the NLTK sentence tokenizer. Under-segmented sentences were split further manually. Word level tokenization was generated using the Stanford PTB word tokenizer.']\n",
            "['Notice that most of these models, especially the later ones, use the bias-attention method to represent and inject attributes, but also employ a more complex model architecture to enjoy a boost in performance. Results are summarized in Table TABREF33. On all three datasets, our best results outperform all previous models based on accuracy and RMSE. Among our four models, CHIM-embedding performs the best in terms of accuracy, with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively. CHIM-classifier performs the best in terms of RMSE, outperforming all other models on both Yelp 2013 and 2014 datasets. Among our models, CHIM-attention mechanism performs the worst, which shows similar results to our previous experiment (see Figure FIGREF25). We emphasize that our models use a simple BiLSTM as base model, and extensions to the base model (e.g., using multiple hierarchical LSTMs as in BIBREF21), as well as to other aspects (e.g., consideration of cold-start entities as in BIBREF9), are orthogonal to our proposed attribute representation and injection method. Thus, we expect a further increase in performance when these extensions are done.']\n",
            "['The results of our experiments can be summarized in three statements. First, our preliminary experiments show that doing bias-based attribute representation and attention-based injection is not an effective method to incorporate user and product information in sentiment classification models. Second, despite using only a simple BiLSTM with attention classifier, we significantly outperform previous state-of-the-art models that use more complicated architectures (e.g., models that use hierarchical models, external memory networks, etc.). Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation.']\n",
            "['We perform experiments on two tasks. The first task is Sentiment Classification, where we are tasked to classify the sentiment of a review text, given additionally the user and product information as attributes. The second task is Attribute Transfer, where we attempt to transfer the attribute encodings learned from the sentiment classification model to solve two other different tasks: (a) Product Category Classification, where we are tasked to classify the category of the product, and (b) Review Headline Generation, where we are tasked to generate the title of the review, given only both the user and product attribute encodings. Datasets, evaluation metrics, and competing models are different for each task and are described in their corresponding sections.']\n",
            "['Our work here focuses on the zero-shot translation aspect of universal multilingual NMT. First, we attempt to investigate the relationship of encoder representation and ZS performance. By modifying the Transformer architecture of BIBREF10 to afford a fixed-size representation for the encoder output, we found that we can significantly improve zero-shot performance at the cost of a lower performance on the supervised language pairs. To the best of our knowledge, this is the first empirical evidence showing that the multilingual model can capture both language-independent and language-dependent features, and that the former can be prioritized during training.', 'This observation leads us to the most important contribution in this work, which is to propose several techniques to learn a joint semantic space for different languages in multilingual models without any architectural modification. The key idea is to prefer a source language-independent representation in the decoder using an additional loss function. As a result, the NMT architecture remains untouched and the technique is scalable to the number of languages in the training data. The success of this method is shown by significant gains on zero-shot translation quality in the standard IWSLT 2017 multilingual benchmark BIBREF11 . Finally, we introduce a more challenging scenario that involves more than one bridge language between source and target languages. This challenging setup confirms the consistency of our zero-shot techniques while clarifying the disadvantages of pivot-based translation.']\n",
            "['Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.']\n",
            "['Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.']\n",
            "['As the dataset for training the machine translation systems, we used high precision sentences extracted with greedy decoding, by ranking the sentence-pairs on their translation probabilities. Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses). We trained 5-gram language models with Kneser-Ney smoothing using KenLM BIBREF16 . With these parameters, we trained SMT systems for en–ta and en–hi language pairs, with and without the use of extracted parallel sentence pairs.']\n",
            "['For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en–ta and en–hi pairs, as can be seen in Table TABREF23 .']\n",
            "['For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en–ta and en–hi pairs, as can be seen in Table TABREF23 .']\n",
            "['The experiments are shown for English-Tamil and English-Hindi language pairs. Our model achieved a marked percentage increase in the BLEU score for both en–ta and en–hi language pairs. We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en–ta and en–hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.']\n",
            "['The experiments are shown for English-Tamil and English-Hindi language pairs. Our model achieved a marked percentage increase in the BLEU score for both en–ta and en–hi language pairs. We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en–ta and en–hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.']\n",
            "['Nematus implements an attentional encoder–decoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14, but with several implementation differences. The main differences are as follows:']\n",
            "['Nematus is implemented in Python, and based on the Theano framework BIBREF4 . It implements an attentional encoder–decoder architecture similar to DBLP:journals/corr/BahdanauCB14. Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus.']\n",
            "['For our QA system we used the DBpedia repository and the Subtitles corpus. Due to the nature of training a sequence-to-sequence neural model, questions and answers need to be aligned. The statistics on the used data are shown in Table TABREF5 .', 'Since the extracted information form the DBpedia n-triples does not represent a natural language, we added to the extracted n-triples answers with dialogues from the OpenSubtitles dataset BIBREF18 . Since this dataset is stored in an XML structure with time codes, only sentences were extracted, where the first sentence ends with a question mark and the second sentence does not end with a question mark. Additionally, to ensure a better consistency between an question and the answer, the second sentence has to follow the first sentence by less than 20 seconds. From the 14M sentence corpus of question-answer pairs provided by the OpenNMT project, we used 5M dialogue entries to modify the language generation part. Table TABREF10 shows some examples from the OpenSubtitles dataset.']\n",
            "['Since the extracted information form the DBpedia n-triples does not represent a natural language, we added to the extracted n-triples answers with dialogues from the OpenSubtitles dataset BIBREF18 . Since this dataset is stored in an XML structure with time codes, only sentences were extracted, where the first sentence ends with a question mark and the second sentence does not end with a question mark. Additionally, to ensure a better consistency between an question and the answer, the second sentence has to follow the first sentence by less than 20 seconds. From the 14M sentence corpus of question-answer pairs provided by the OpenNMT project, we used 5M dialogue entries to modify the language generation part. Table TABREF10 shows some examples from the OpenSubtitles dataset.']\n",
            "['We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set.']\n",
            "['We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set.']\n",
            "['We do comparisons with three sportball datasets, as follows:', 'Original English (English-sportball): The original MS-COCO English dataset with 3,850 sportball images. This dataset is first evaluated in order to have base results for following comparisons.']\n",
            "['Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.']\n",
            "['Regarding to the Vietnamese language processing, there are quite a number of research works on other tasks such as parsing, part-of-speech, named entity recognition, sentiment analysis, question answering. However, to the extent of our knowledge, there are no research publications on image captioning for Vietnamese. Therefore, we decide to build a new corpus of Vietnamese image captioning for Image Captioning research community and evaluate the state-of-the-art models on our corpus. In particular, we validate and compare the results by BLEU BIBREF11, ROUGE BIBREF12 and CIDEr BIBREF13 metrics between Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey on our corpus as the pioneering results.']\n",
            "['Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.']\n",
            "['Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.', 'After evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree–Structured LSTMs BIBREF13 . These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor.']\n",
            "['Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.', 'After evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree–Structured LSTMs BIBREF13 . These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor.']\n",
            "['Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.']\n",
            "['Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.']\n",
            "['Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.']\n",
            "['Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.']\n",
            "['BIBREF1 show how the LSTM can simulate a simplified variant of the counter machine. Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.', 'Another important formal language task for assessing network memory is string reversal. Reversing requires remembering a INLINEFORM0 prefix of characters, which implies INLINEFORM1 state complexity.', 'We frame reversing as a seq2seq transduction task, and compare the performance of an LSTM encoder-decoder architecture to the same architecture augmented with attention. We also report the results of BIBREF22 for a stack neural network (StackNN), another architecture with INLINEFORM0 state complexity (thm:stackstatecomplexity).', 'Counting', 'The goal of this experiment is to evaluate which architectures have memory beyond finite state. We train a language model on INLINEFORM0 with INLINEFORM1 and test it on longer strings INLINEFORM2 . Predicting the INLINEFORM3 character correctly while maintaining good overall accuracy requires INLINEFORM4 states. The results reported in fig:countingresults demonstrate that all recurrent models, with only two hidden units, find a solution to this task that generalizes at least over this range of string lengths.', 'Counting with Noise', \"In order to abstract away from asymptotically unstable representations, our next experiment investigates how adding noise to an RNN's activations impacts its ability to count. For the SRN and GRU, noise is added to INLINEFORM0 before computing INLINEFORM1 , and for the LSTM, noise is added to INLINEFORM2 . In either case, the noise is sampled from the distribution INLINEFORM3 .\", 'Reversing']\n",
            "['The goal of this experiment is to evaluate which architectures have memory beyond finite state. We train a language model on INLINEFORM0 with INLINEFORM1 and test it on longer strings INLINEFORM2 . Predicting the INLINEFORM3 character correctly while maintaining good overall accuracy requires INLINEFORM4 states. The results reported in fig:countingresults demonstrate that all recurrent models, with only two hidden units, find a solution to this task that generalizes at least over this range of string lengths.', 'BIBREF1 report failures in attempts to train SRNs and GRUs to accept counter languages, unlike what we have found. We conjecture that this stems not from the requisite memory, but instead from the different objective function we used. Our language modeling training objective is a robust and transferable learning target BIBREF19 , whereas sparse acceptance classification might be challenging to learn directly for long strings.']\n",
            "['In future work, the memory module could be applied to other domains such as summary generation. While memory modules are able to capture neural vectors of information, they may not easily capture specific words for later use. A possible future approach might combine memory module architectures with pointer softmax networks BIBREF16 to allow memory models to store information about which words from previous utterances of the conversation to use in future responses.']\n",
            "['After one epoch of training, the perplexity evaluated on the validation set was 68.50 for the proposed memory-augmented NTM-LM architecture. This is a 0.68 perplexity improvement over the vanilla language model without the NTM augmentation.']\n",
            "['The best performing model was the NTM-LM architecture. While the model received the best performance in perplexity, it demonstrated only a one-point improvement over the existing language model architecture. While in state-of-the-art comparisons a one point difference can be significant, it does indicate that the proposed NTM addition to the language model only contributed a small improvement. It is possible that the additional NTM module was too difficult to train, or that the NTM module injected noise into the input of the GRU such that training became difficult. It is still surprising that the NTM was not put to better use, for performance gains. It is possible the model has not been appropriately tuned.']\n",
            "['See Table TABREF3 for details on model and baseline perplexity. To begin, it is worth noting that all of the above architectures were trained in a similar environment, with the exception of HRED, which was trained using an existing Github implementation implementation. Overall, the NTM-LM architecture performed the best of all model architectures, whereas the sequence-to-sequence architecture performed the worst. The proposed NTM-LM outperformed the DNTM-S architecture.']\n",
            "[\"To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples.\"]\n",
            "[\"To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples.\"]\n",
            "['The hyper-parameter settings adopted in baselines and our model are the best practice settings for each training set. All models are tested with various hyper-parameter settings to get their best performance. Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14.']\n",
            "['The hyper-parameter settings adopted in baselines and our model are the best practice settings for each training set. All models are tested with various hyper-parameter settings to get their best performance. Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14.']\n",
            "['As the proposed approach mainly concentrates on the interaction of human-computer, we select and modify two very different style datasets to test the performance of our method. One is a task-oriented dialogue dataset MultiWoz 2.0 and the other is a chitchat dataset DailyDialogue . Both datasets are collected from human-to-human conversations. We evaluate and compare the results with the baseline methods in multiple dimensions. Table TABREF28 shows the statistics of datasets.']\n",
            "['As the proposed approach mainly concentrates on the interaction of human-computer, we select and modify two very different style datasets to test the performance of our method. One is a task-oriented dialogue dataset MultiWoz 2.0 and the other is a chitchat dataset DailyDialogue . Both datasets are collected from human-to-human conversations. We evaluate and compare the results with the baseline methods in multiple dimensions. Table TABREF28 shows the statistics of datasets.']\n",
            "['Advances', \"PETRARCH (Python Engine for Text Resolution And Related Coding Hierarchy) is the new generation of event-data coding software that is the successor to the TABARI software. As noted in the previous sections, the major advance of this next generation of event data coding is the incorporation of a “deep parse” that enables more advanced analysis of the syntactic structure of sentences. In PETRARCH's case, this deep parse is provided by the Stanford NLP group's CoreNLP software BIBREF14 . CoreNLP provides information regarding part-of-speech tags for individual words, noun and verb phrase chunking, and syntactic information regarding the relation of noun and verb phrases. Figure 1 provides an example of what information CoreNLP outputs, while Figure 2 provides an example of the input that PETRARCH accepts.\", 'PETRARCH2 represents a further iteration upon the basic principles seen in PETRARCH, mainly a deep reliance on information from a syntactic parse tree. The exact operational details of PETRARCH2 are beyond the scope of this chapter, with a complete explanation of the algorithm available in BIBREF15 , it should suffice to say that this second version of PETRARCH makes extensive use of the actual structure of the parse tree to determine source-action-target event codings. In other words, PETRARCH still mainly focused on parsing noun and verb phrase chunks without fully integrating syntactic information. In PETRARCH2 the tree structure of sentences is inherent to the coding algorithm. Changing the algorithm to depend more heavily on the tree structure of the sentence allows for a clearer identification of actors and the assignment of role codes to the actors, and a more accurate identification of the who and whom portions of the who-did-what-to-whom equation. The second major change between PETRARCH and PETRARCH2 is the internal category coding logic within PETRARCH2. In short, PETRARCH2 allows for interactions of verbs to create a different category classification than either verb on its own would produce. For PETRARCH, such things would have to be defined explicitly within the dictionaries. In PETRARCH2, however, there is a coding scheme that allows verbs like “intend” and “aid” to interact in order to create a different coding than either verb on its own would create. Additionally, PETRARCH2 brought about a refactoring and speedup of the code base and a reformatting of the underlying verb dictionaries. This reformatting of the dictionaries also included a “cleaning up” of various verb patterns within the dictionaries. This was largely due to changes internal to the coding engine such as the tight coupling to the constituency parse tree and the verb interactions mentioned above. This change in the event coder software further demonstrates the modular and composable nature of the processing pipeline; the rest of the processing architecture is able to remain the same even with a relatively major shift in the event coding software.', \"There are several ways that the scraping of news content from the web can occur. A system can sit on top of an aggregator such as Google News, use a true spidering system that follows links from a seed list, or can pull from a designated list of trusted resources. Each system has its benefits and challenges. The use of an aggregator means that a project is subject to another layer of complexity that is out of the user's control; those making use of Google News have no say over how, and what, content is aggregated. Implementing a full-scale web spider to obtain news content is a labor and maintenance intensive process that calls for a dedicated team of software engineers. This type of undertaking is beyond the scope of the current event data projects. The final option is to use a list of predefined resources, in this case RSS feeds of news websites, and pull content from these resources. For the purposes of the realtime event data discussed herein, I have settled on the final option.\", \"The final additional piece of information necessary for a modern event dataset is the geolocation of the coded events. The geolocation of event data is difficult from both a technological and ontological perspective. First, from an ontological standpoint, deciding which location to pick as the location for an event is often difficult. For example, a sentence such as “Speaking from the Rose Garden, President Obama denounced the Russian actions in Syria” provides several possible locations: the Rose Garden, Syria, and even, possibly, Russia. It is also possible for an event to have no location. This problem relates to the “aboutness” of an article. In the above example, the statement event of President Obama denouncing Russia should likely be coded as not having a location. The second difficulty is the technological issues at play when geolocating place mentions. First, geolocation must sit on top of named entity recognition, which is itself a fragile process. Once these location identities are identified, they must be resolved to their latitude and longitude coordinates. These lookups are difficult since any process must disambiguate between Paris, Texas and Paris, France or between Washington state and Washington D.C. Finally, event data coding currently works at the sentence level, which restricts how much information can be discerned when using the entirety of an article's text.\"]\n",
            "[\"PETRARCH (Python Engine for Text Resolution And Related Coding Hierarchy) is the new generation of event-data coding software that is the successor to the TABARI software. As noted in the previous sections, the major advance of this next generation of event data coding is the incorporation of a “deep parse” that enables more advanced analysis of the syntactic structure of sentences. In PETRARCH's case, this deep parse is provided by the Stanford NLP group's CoreNLP software BIBREF14 . CoreNLP provides information regarding part-of-speech tags for individual words, noun and verb phrase chunking, and syntactic information regarding the relation of noun and verb phrases. Figure 1 provides an example of what information CoreNLP outputs, while Figure 2 provides an example of the input that PETRARCH accepts.\", 'PETRARCH2 represents a further iteration upon the basic principles seen in PETRARCH, mainly a deep reliance on information from a syntactic parse tree. The exact operational details of PETRARCH2 are beyond the scope of this chapter, with a complete explanation of the algorithm available in BIBREF15 , it should suffice to say that this second version of PETRARCH makes extensive use of the actual structure of the parse tree to determine source-action-target event codings. In other words, PETRARCH still mainly focused on parsing noun and verb phrase chunks without fully integrating syntactic information. In PETRARCH2 the tree structure of sentences is inherent to the coding algorithm. Changing the algorithm to depend more heavily on the tree structure of the sentence allows for a clearer identification of actors and the assignment of role codes to the actors, and a more accurate identification of the who and whom portions of the who-did-what-to-whom equation. The second major change between PETRARCH and PETRARCH2 is the internal category coding logic within PETRARCH2. In short, PETRARCH2 allows for interactions of verbs to create a different category classification than either verb on its own would produce. For PETRARCH, such things would have to be defined explicitly within the dictionaries. In PETRARCH2, however, there is a coding scheme that allows verbs like “intend” and “aid” to interact in order to create a different coding than either verb on its own would create. Additionally, PETRARCH2 brought about a refactoring and speedup of the code base and a reformatting of the underlying verb dictionaries. This reformatting of the dictionaries also included a “cleaning up” of various verb patterns within the dictionaries. This was largely due to changes internal to the coding engine such as the tight coupling to the constituency parse tree and the verb interactions mentioned above. This change in the event coder software further demonstrates the modular and composable nature of the processing pipeline; the rest of the processing architecture is able to remain the same even with a relatively major shift in the event coding software.', \"There are several ways that the scraping of news content from the web can occur. A system can sit on top of an aggregator such as Google News, use a true spidering system that follows links from a seed list, or can pull from a designated list of trusted resources. Each system has its benefits and challenges. The use of an aggregator means that a project is subject to another layer of complexity that is out of the user's control; those making use of Google News have no say over how, and what, content is aggregated. Implementing a full-scale web spider to obtain news content is a labor and maintenance intensive process that calls for a dedicated team of software engineers. This type of undertaking is beyond the scope of the current event data projects. The final option is to use a list of predefined resources, in this case RSS feeds of news websites, and pull content from these resources. For the purposes of the realtime event data discussed herein, I have settled on the final option.\", \"The final additional piece of information necessary for a modern event dataset is the geolocation of the coded events. The geolocation of event data is difficult from both a technological and ontological perspective. First, from an ontological standpoint, deciding which location to pick as the location for an event is often difficult. For example, a sentence such as “Speaking from the Rose Garden, President Obama denounced the Russian actions in Syria” provides several possible locations: the Rose Garden, Syria, and even, possibly, Russia. It is also possible for an event to have no location. This problem relates to the “aboutness” of an article. In the above example, the statement event of President Obama denouncing Russia should likely be coded as not having a location. The second difficulty is the technological issues at play when geolocating place mentions. First, geolocation must sit on top of named entity recognition, which is itself a fragile process. Once these location identities are identified, they must be resolved to their latitude and longitude coordinates. These lookups are difficult since any process must disambiguate between Paris, Texas and Paris, France or between Washington state and Washington D.C. Finally, event data coding currently works at the sentence level, which restricts how much information can be discerned when using the entirety of an article's text.\", \"To make all the various pieces communicate, a comprehensive pipeline is necessary in order to successfully coordinate the various tasks. Specifically, there are three main pieces of software/technology that must communicate with each other: PETRARCH, Stanford's CoreNLP software, and the MongoDB instance. For the realtime data component, the web scraper must also fit into this system. The overall flow of this pipeline is demonstrated in the figure below.\"]\n",
            "['The Phoenix dataset is an attempt to take both the new advances in event data described above, along with decades of knowledge regarding best practices, in order to create a new iteration of event data. The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content. New data is generated on a daily basis, coded according to the CAMEO event ontology, with an average of 2,200 events generated per day. The full dataset examined here contains 254,060 total events spread across 102 days of generated data. Based on publicly available information, the project also makes use of the most up-to-date actor dictionaries of any available machine-coded event dataset.']\n",
            "['The Phoenix dataset is an attempt to take both the new advances in event data described above, along with decades of knowledge regarding best practices, in order to create a new iteration of event data. The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content. New data is generated on a daily basis, coded according to the CAMEO event ontology, with an average of 2,200 events generated per day. The full dataset examined here contains 254,060 total events spread across 102 days of generated data. Based on publicly available information, the project also makes use of the most up-to-date actor dictionaries of any available machine-coded event dataset.']\n",
            "['Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .', 'Proof. Consider the simple RNN with INLINEFORM0 as its INLINEFORM1 -language described in the proof of Theorem 1.1 and the simple RNN with INLINEFORM2 as its INLINEFORM3 -language constructed to prove Theorem 1.10. Merge the INLINEFORM4 nodes in the input layer corresponding to the input and merge the single output nodes of both RNNs. Stack the two hidden layers, and add no new edges. There were INLINEFORM5 hidden nodes in the first RNN and INLINEFORM6 in the second, so altogether the new RNN has INLINEFORM7 hidden nodes.', 'The output of the new RNN is equal to the summed output of the two original RNNs, and from the proofs of Theorems 1.1 and 1.10 these outputs are always nonnegative. Thus the output of the new RNN is INLINEFORM0 if and only if the outputs of both old RNNs were INLINEFORM1 , immediately proving the theorem. INLINEFORM2', 'Discussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs.']\n",
            "['Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.']\n",
            "['Experiments were performed using the CHiME-5 data. Distant microphone recordings (U data) during training and/or testing were processed using the speech enhancement methods depicted in Table TABREF6. Speech was either left unprocessed, enhanced using a weighted delay-and-sum beamformer (BFIt) BIBREF21 with or without dereverberation (WPE), or processed using the guided source separation (GSS) approach described in Section SECREF3. In Table TABREF6, the strength of the enhancement increases from top to bottom, i.e., GSS6 signals are much cleaner than the unprocessed ones.', 'An extensive set of experiments was performed to measure the WER impact of enhancement on the CHiME-5 training and test data. We test enhancement methods of varying strengths, as described in Section SECREF5, and the results are depicted in Table TABREF12. In all cases, the (unprocessed) worn dataset was also included for AM training since it was found to improve performance (supporting therefore the argument that data variability helps ASR robustness).', 'In Table TABREF12, in each row the recognition accuracy improves monotonically from left to right, i.e., as the enhancement strategy on the test data becomes stronger. Reading the table in each column from top to bottom, one observes that accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data. Compared with unprocessed training and test data (None-None), GSS6-GSS6 yields roughly 35% (24%) relative WER reduction on the DEV (EVAL) set, and 12% (11%) relative WER reduction when compared with the None-GSS6 scenario. Comparing the amount of training data used to train the acoustic models, we observe that it decreases drastically from no enhancement to the GSS6 enhancement.', 'In this paper we performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data showing that: (i) cleaning up training data can lead to substantial word error rate reduction, and (ii) enhancement in training is advisable as long as enhancement in test is at least as strong as in training. This approach stands in contrast and delivers larger accuracy gains at a fraction of training data than the common data simulation strategy found in the literature. Using a CNN-TDNNF acoustic model topology along with GSS enhancement refined with time annotations from ASR, discriminative training and RNN LM rescoring, we achieved a new single-system state-of-the-art result on CHiME-5, which is 41.6% (43.2%) on the development (evaluation) set, which is a 8% relative improvement of the word error rate over a comparable system reported so far.']\n",
            "['In Table TABREF12, in each row the recognition accuracy improves monotonically from left to right, i.e., as the enhancement strategy on the test data becomes stronger. Reading the table in each column from top to bottom, one observes that accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data. Compared with unprocessed training and test data (None-None), GSS6-GSS6 yields roughly 35% (24%) relative WER reduction on the DEV (EVAL) set, and 12% (11%) relative WER reduction when compared with the None-GSS6 scenario. Comparing the amount of training data used to train the acoustic models, we observe that it decreases drastically from no enhancement to the GSS6 enhancement.']\n",
            "['To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported inBIBREF12.']\n",
            "['In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations. Details on each approach are given in section \"Multilingual word representations\" and \"Multilingual sentence representations\" respectively. In contrast to previous works on cross-lingual document classification with RVC2, we explore training the classifier on all languages and transfer it to all others, ie. we do not limit our study to the transfer between English and a foreign language.']\n",
            "['Several works have been proposed to learn multilingual word embeddings, which are then combined to perform cross-lingual document classifications. These word embeddings are trained on either word alignments or sentence-aligned parallel corpora. To provide reproducible benchmark results, we use MultiCCA word embeddings published by BIBREF3 .', 'A second direction of research is to directly learn multilingual sentence representations. In this paper, we evaluate a recently proposed technique to learn joint multilingual sentence representations BIBREF5 . The underlying idea is to use multiple sequence encoders and decoders and to train them with aligned corpora from the machine translation community. The goal is that all encoders share the same sentence representation, i.e. we map all languages into one common space. A detailed description of this approach can be found in BIBREF5 . We have developed two versions of the system: one trained on the Europarl corpus BIBREF6 to cover the languages English, German, French, Spanish and Italian, and another one trained on the United Nations corpus BIBREF7 which allows to learn a joint sentence embedding for English, French, Spanish, Russian and Chinese. We use a one hidden-layer MLP as classifier. For comparison, we have evaluated its performance on the original subset of RCV2 as used in previous publications on cross-lingual document classification: we are able to outperform the current state-of-the-art in three out of six transfer directions.', 'In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations. Details on each approach are given in section \"Multilingual word representations\" and \"Multilingual sentence representations\" respectively. In contrast to previous works on cross-lingual document classification with RVC2, we explore training the classifier on all languages and transfer it to all others, ie. we do not limit our study to the transfer between English and a foreign language.']\n",
            "['We restricted the data to English tweets only, as tagged by langid.py BIBREF18 . Jaccard similarity was computed between messages to identify and remove duplicates. We removed URLs and preserved only tweets that mention contenders in the text. This automatic post-processing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table TABREF17 gives the data distribution across event categories.']\n",
            "['We restricted the data to English tweets only, as tagged by langid.py BIBREF18 . Jaccard similarity was computed between messages to identify and remove duplicates. We removed URLs and preserved only tweets that mention contenders in the text. This automatic post-processing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table TABREF17 gives the data distribution across event categories.']\n",
            "[\"We model the conditional distribution over a tweet's veridicality toward a candidate INLINEFORM0 winning a contest against a set of opponents, INLINEFORM1 , using a log-linear model: INLINEFORM2\", 'where INLINEFORM0 is the veridicality (positive, negative or neutral).', \"To extract features INLINEFORM0 , we first preprocessed tweets retrieved for a specific event to identify named entities, using BIBREF20 's Twitter NER system. Candidate ( INLINEFORM1 ) and opponent entities were identified in the tweet as follows:\", 'We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword.']\n",
            "['The goal of our system, TwiVer, is to automate the annotation process by predicting how veridical a tweet is toward a candidate winning a contest: is the candidate deemed to be winning, or is the author uncertain? For the purpose of our experiments, we collapsed the five labels for veridicality into three: positive veridicality (“Definitely Yes\" and “Probably Yes\"), neutral (“Uncertain about the outcome\") and negative veridicality (“Definitely No\" and “Probably No\").']\n",
            "[\"We used the publicly available Convote dataset BIBREF3 for our experiments. The dataset provides transcripts of debates in the House of Representatives of the U.S Congress for the year 2005. Each file in the dataset corresponds to a single, uninterrupted utterance by a speaker in a given debate. We combine all the utterances of a speaker in a given debate in a single file to capture different opinions/view points of the speaker about the debate topic. We call this document the view point document (VPD) representing the speaker's opinion about different aspects of the issue being debated. The dataset also provides political affiliations of all the speakers – Republican (R), Democrat (D), and Independent (I). With there being only six documents for the independent class (four in training, two in test), we excluded them from our evaluation. Table TABREF15 summarizes the statistics about the dataset and distribution of different classes. We obtained 50 topics using LDA from Mallet run over the training dataset. The topic-sentiment matrix was obtained using the Stanford CoreNLP sentiment API BIBREF9 which provides probability distributions over a set of five sentiment polarity classes.\", 'FLOAT SELECTED: Table 3: List of most polarizing (top) and least polarizing (bottom) topics as computed using equation 5.']\n",
            "['In order to evaluate our proposed TSM-based methods - viz., nearest class (NC) and logistic regression (LR) - we use the following methods in our empirical evaluation.', 'GloVe-d2v: We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.']\n",
            "['In order to evaluate our proposed TSM-based methods - viz., nearest class (NC) and logistic regression (LR) - we use the following methods in our empirical evaluation.', 'GloVe-d2v: We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.']\n",
            "[\"We used the publicly available Convote dataset BIBREF3 for our experiments. The dataset provides transcripts of debates in the House of Representatives of the U.S Congress for the year 2005. Each file in the dataset corresponds to a single, uninterrupted utterance by a speaker in a given debate. We combine all the utterances of a speaker in a given debate in a single file to capture different opinions/view points of the speaker about the debate topic. We call this document the view point document (VPD) representing the speaker's opinion about different aspects of the issue being debated. The dataset also provides political affiliations of all the speakers – Republican (R), Democrat (D), and Independent (I). With there being only six documents for the independent class (four in training, two in test), we excluded them from our evaluation. Table TABREF15 summarizes the statistics about the dataset and distribution of different classes. We obtained 50 topics using LDA from Mallet run over the training dataset. The topic-sentiment matrix was obtained using the Stanford CoreNLP sentiment API BIBREF9 which provides probability distributions over a set of five sentiment polarity classes.\"]\n",
            "['We develop a simple classification model that uses a topic-specific sentiment summarization for republican and democrat speeches separately. Initial results of experiments conducted using a widely used dataset of US Congress debates BIBREF3 are encouraging and show that this simple model compares well with classification models that employ state-of-the-art distributional text representations (Section SECREF4 ).', \"We used the publicly available Convote dataset BIBREF3 for our experiments. The dataset provides transcripts of debates in the House of Representatives of the U.S Congress for the year 2005. Each file in the dataset corresponds to a single, uninterrupted utterance by a speaker in a given debate. We combine all the utterances of a speaker in a given debate in a single file to capture different opinions/view points of the speaker about the debate topic. We call this document the view point document (VPD) representing the speaker's opinion about different aspects of the issue being debated. The dataset also provides political affiliations of all the speakers – Republican (R), Democrat (D), and Independent (I). With there being only six documents for the independent class (four in training, two in test), we excluded them from our evaluation. Table TABREF15 summarizes the statistics about the dataset and distribution of different classes. We obtained 50 topics using LDA from Mallet run over the training dataset. The topic-sentiment matrix was obtained using the Stanford CoreNLP sentiment API BIBREF9 which provides probability distributions over a set of five sentiment polarity classes.\"]\n",
            "['Semi-supervised Framework', 'Our semi-supervised text classification framewrok is inspired by works BIBREF18 , BIBREF19 . We assume the original classifier classify a sample into one of INLINEFORM0 possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class INLINEFORM1 . And correspondingly the dimension of our classifier output increases from INLINEFORM2 to INLINEFORM3 . The configuration of our generator network G is inspired by the architecture proposed in BIBREF16 . And we modify the architecture to make it suitable to the text classification tasks. Table TABREF13 shows the configuration of each layer in the generator G. Lets assume the training batch size is INLINEFORM4 and the percentage of the generated samples among a batch training samples is INLINEFORM5 . At each iteration of the training process, we first generate INLINEFORM6 samples from the generator G then we draw INLINEFORM7 samples from the real dataset. We then perform gradient descent on the AC-BLSTM and generative net G and finally update the parameters of both nets.']\n",
            "['Word embeddings are used extensively as the first stage of neural networks throughout NLP. Typically, embeddings are initalized based on a vector trained with word2vec or GloVe and then further modified as part of training for the target task. We study two downstream tasks to see whether stability impacts performance.', 'Since we are interested in seeing the impact of word vector stability, we choose tasks that have an intuitive evaluation at the word level: word similarity and POS tagging.']\n",
            "['Since we are interested in seeing the impact of word vector stability, we choose tasks that have an intuitive evaluation at the word level: word similarity and POS tagging.']\n",
            "[\"To further understand the effect of curriculum learning on the model, we train a regression model with all of the features except the curriculum learning features. This model achieves an INLINEFORM0 score of 0.291 (compared to the full model's score of 0.301). This indicates that curriculum learning is a factor in stability.\", 'Observation 2. POS is one of the biggest factors in stability. Table TABREF14 shows that many of the top weights belong to POS-related features (both primary and secondary POS). Table TABREF18 compares average stabilities for each primary POS. Here we see that the most stable POS are numerals, verbs, and determiners, while the least stable POS are punctuation marks, adpositions, and particles.', 'Observation 3. Stability within domains is greater than stability across domains. Table TABREF14 shows that many of the top factors are domain-related. Figure FIGREF19 shows the results of the regression model broken down by domain. This figure shows the highest stabilities appearing on the diagonal of the matrix, where the two embedding spaces both belong to the same domain. The stabilities are substantially lower off the diagonal.']\n",
            "['Overall, the regression model achieves a coefficient of determination ( INLINEFORM0 ) score of 0.301 on the training data, which indicates that the regression has learned a linear model that reasonably fits the training data given. Using the regression model, we can analyze the weights corresponding to each of the features being considered, shown in Table TABREF14 . These weights are difficult to interpret, because features have different distributions and ranges. However, we make several general observations about the stability of word embeddings.', 'Observation 1. Curriculum learning is important. This is evident because the top two features (by magnitude) of the regression model capture where the word first appears in the training data. Figure FIGREF15 shows trends between training data position and stability in the PTB. This figure contrasts word2vec with GloVe (which is order invariant).', 'Observation 2. POS is one of the biggest factors in stability. Table TABREF14 shows that many of the top weights belong to POS-related features (both primary and secondary POS). Table TABREF18 compares average stabilities for each primary POS. Here we see that the most stable POS are numerals, verbs, and determiners, while the least stable POS are punctuation marks, adpositions, and particles.']\n",
            "['Defining Stability', 'We define stability as the percent overlap between nearest neighbors in an embedding space. Given a word INLINEFORM0 and two embedding spaces INLINEFORM1 and INLINEFORM2 , take the ten nearest neighbors of INLINEFORM3 in both INLINEFORM4 and INLINEFORM5 . Let the stability of INLINEFORM6 be the percent overlap between these two lists of nearest neighbors. 100% stability indicates perfect agreement between the two embedding spaces, while 0% stability indicates complete disagreement. In order to find the ten nearest neighbors of a word INLINEFORM7 in an embedding space INLINEFORM8 , we measure distance between words using cosine similarity. This definition of stability can be generalized to more than two embedding spaces by considering the average overlap between two sets of embedding spaces. Let INLINEFORM12 and INLINEFORM13 be two sets of embedding spaces. Then, for every pair of embedding spaces INLINEFORM14 , where INLINEFORM15 and INLINEFORM16 , take the ten nearest neighbors of INLINEFORM17 in both INLINEFORM18 and INLINEFORM19 and calculate percent overlap. Let the stability be the average percent overlap over every pair of embedding spaces INLINEFORM20 .']\n",
            "['In addition to word and data properties, we encode features about the embedding algorithms. These include the different algorithms being used, as well as the different parameter settings of these algorithms. Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI. The choice of algorithm is represented in our feature vector as a bag-of-words.']\n",
            "['In addition to word and data properties, we encode features about the embedding algorithms. These include the different algorithms being used, as well as the different parameter settings of these algorithms. Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI. The choice of algorithm is represented in our feature vector as a bag-of-words.']\n",
            "['As we want to build task-specific NMT models, in this work we explore two data-selection algorithms that are classified as Transductive Algorithms (TA): Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA). These methods use the test set $S_{test}$ (the document to be translated) as the seed to retrieve sentences. In transductive learning BIBREF2 the goal is to identify the best training instances to learn how to classify a given test set. In order to select these sentences, the TAs search for those n-grams in the test set that are also present in the source side of the candidate sentences.']\n",
            "['As we want to build task-specific NMT models, in this work we explore two data-selection algorithms that are classified as Transductive Algorithms (TA): Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA). These methods use the test set $S_{test}$ (the document to be translated) as the seed to retrieve sentences. In transductive learning BIBREF2 the goal is to identify the best training instances to learn how to classify a given test set. In order to select these sentences, the TAs search for those n-grams in the test set that are also present in the source side of the candidate sentences.']\n",
            "['Nonetheless, if synthetic data are not in the same domain as the test set, it can also hurt the performance. For this reason, we explore an alternative approach to better use the artificially-generated training instances to improve NMT models. In particular, we propose that instead of blindly adding back-translated sentences into the training set they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model. By doing that, instead of increasing the number of training instances in a motivated manner, the generated sentences provide us with more chances of obtaining relevant parallel sentences (and still use smaller sets for fine-tuning).']\n",
            "['A popular technique used to create artificial data is the back-translation technique BIBREF0, BIBREF1. This consists of generating sentences in the source language by translating monolingual sentences in the target language. Then, these sentences in both languages are paired and can be used to augment the original parallel training set used to build better NMT models.']\n",
            "['Test sets: We evaluate the models with two test sets in different domains:', 'BIO test set: the Cochrane dataset from the WMT 2017 biomedical translation shared task BIBREF20.', 'NEWS test set: The test set provided in WMT 2015 News Translation Task.']\n",
            "['Test sets: We evaluate the models with two test sets in different domains:', 'BIO test set: the Cochrane dataset from the WMT 2017 biomedical translation shared task BIBREF20.', 'NEWS test set: The test set provided in WMT 2015 News Translation Task.']\n",
            "['For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value.']\n",
            "['For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value.']\n",
            "['The decoder is a recurrent neural network that predicts a target sequence INLINEFORM0 . Each word INLINEFORM1 is predicted based on a recurrent hidden state INLINEFORM2 , the previously predicted word INLINEFORM3 , and a context vector INLINEFORM4 . INLINEFORM5 is computed as a weighted sum of the annotations INLINEFORM6 . The weight of each annotation INLINEFORM7 is computed through an alignment model INLINEFORM8 , which models the probability that INLINEFORM9 is aligned to INLINEFORM10 . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation.']\n",
            "['The semantic tagset is language-neutral, abstracts over part-of-speech and named-entity classes, and includes fine-grained semantic information. The tagset consists of 80 semantic tags grouped in 13 coarse-grained classes. The tagset originated in the Parallel Meaning Bank (PMB) project BIBREF9 , where it contributes to compositional semantics and cross-lingual projection of semantic representations. Recent work has highlighted the utility of the tagset as a conduit for evaluating the semantics captured by vector representations BIBREF10 , or employed it in an auxiliary tagging task BIBREF4 , as we do in this work.']\n",
            "[\"We use the same evaluation metrics employed in BIBREF1. Content Fidelity (CF) is an information extraction (IE) approach used in BIBREF10 to measure model's ability to generate text containing factual records. That is, precision and recall (or number) of unique records extracted from the generated text $z$ via an IE model also appear in source recordset $x$. Style Preservation is used to measure how many stylistic properties of the reference are retained in the generated text. In this paper, we calculate BLEU score between the generated text and the reference to reflect model's ability on style preservation. Furthermore, in order to measure model's ability on content selection, we adopt another IE-based evaluation metric, named Content selection, (CS), which is used for data-to-text generation BIBREF10. It is measured in terms of precision and recall by comparing records in generated text $z$ with records in the auxiliary reference $y_{aux}$.\"]\n",
            "[\"We use the same evaluation metrics employed in BIBREF1. Content Fidelity (CF) is an information extraction (IE) approach used in BIBREF10 to measure model's ability to generate text containing factual records. That is, precision and recall (or number) of unique records extracted from the generated text $z$ via an IE model also appear in source recordset $x$. Style Preservation is used to measure how many stylistic properties of the reference are retained in the generated text. In this paper, we calculate BLEU score between the generated text and the reference to reflect model's ability on style preservation. Furthermore, in order to measure model's ability on content selection, we adopt another IE-based evaluation metric, named Content selection, (CS), which is used for data-to-text generation BIBREF10. It is measured in terms of precision and recall by comparing records in generated text $z$ with records in the auxiliary reference $y_{aux}$.\"]\n",
            "['We compare with the following baseline methods on the document-level text manipulation.', '(1) Rule-based Slot Filling Method (Rule-SF) is a straightforward way for text manipulation. Firstly, It masks the record information $x^{\\\\prime }$ in the $y^{\\\\prime }$ and build a mapping between $x$ and $x^{\\\\prime }$ through their data types. Afterwards, select the suitable records from $x$ to fill in the reference y with masked slots. The method is also used in sentence-level task BIBREF1.', '(2) Copy-based Slot Filling Method (Copy-SF) is a data-driven slot filling method. It is derived from BIBREF21, which first generates a template text with data slots to be filled and then leverages a delayed copy mechanism to fill in the slots with proper data records.', '(3) Conditional Copy based Data-To-Text (CCDT) is a classical neural model for data-to-text generation BIBREF10. (4) Hierarchical Encoder for Data-To-Text (HEDT) is also a data-to-text method, which adopts the same hierarchical encoder in our model.', '(5) Text Manipulation with Table Encoder (TMTE) extends sentence-level text editing method BIBREF1 by equipping a more powerful hierarchical table encoder.', '(6) Co-attention-based Method (Coatt): a variation of our model by replacing interactive attention with another co-attention model BIBREF22.', '(7) Ours w/o Interactive Attention (-InterAtt) is our model without interactive attention.', '(8) Ours w/o Back-translation (-BackT) is also a variation of our model by omitting back-translation loss.', 'In addition, for sentence-level task, we adopt the same baseline methods as the paper BIBREF1, including an attention-based Seq2Seq method with copy mechanism BIBREF23, a rule-based method, two style transfer methods, MAST BIBREF24 and AdvST BIBREF25, as well as their state-of-the-art method, abbreviate as S-SOTA.']\n",
            "['We compare with the following baseline methods on the document-level text manipulation.', '(1) Rule-based Slot Filling Method (Rule-SF) is a straightforward way for text manipulation. Firstly, It masks the record information $x^{\\\\prime }$ in the $y^{\\\\prime }$ and build a mapping between $x$ and $x^{\\\\prime }$ through their data types. Afterwards, select the suitable records from $x$ to fill in the reference y with masked slots. The method is also used in sentence-level task BIBREF1.', '(2) Copy-based Slot Filling Method (Copy-SF) is a data-driven slot filling method. It is derived from BIBREF21, which first generates a template text with data slots to be filled and then leverages a delayed copy mechanism to fill in the slots with proper data records.', '(3) Conditional Copy based Data-To-Text (CCDT) is a classical neural model for data-to-text generation BIBREF10. (4) Hierarchical Encoder for Data-To-Text (HEDT) is also a data-to-text method, which adopts the same hierarchical encoder in our model.', '(5) Text Manipulation with Table Encoder (TMTE) extends sentence-level text editing method BIBREF1 by equipping a more powerful hierarchical table encoder.', '(6) Co-attention-based Method (Coatt): a variation of our model by replacing interactive attention with another co-attention model BIBREF22.', '(7) Ours w/o Interactive Attention (-InterAtt) is our model without interactive attention.', '(8) Ours w/o Back-translation (-BackT) is also a variation of our model by omitting back-translation loss.', 'In addition, for sentence-level task, we adopt the same baseline methods as the paper BIBREF1, including an attention-based Seq2Seq method with copy mechanism BIBREF23, a rule-based method, two style transfer methods, MAST BIBREF24 and AdvST BIBREF25, as well as their state-of-the-art method, abbreviate as S-SOTA.']\n",
            "['The data set obtained from Pypestream is permitted to be used for the research purpose only, and for a security reason, we are not allowed to share the data set. It was once originaly used for creating a conversational intelligence system(chatbot) to support customer inqueries about a particular service. The data set is a two-column comma separated value format data with one column of \"sentence\" and the other column of \"class\". It contains 2,212 unique sentences of user expressions asking questions and aswering to the questions the chatbot asked to the users(see Table.TABREF9). The sentences are all in English without having any missspelled words, and labeled with 19 distinct classes that are identified and designed by humans. Additional data set that only contains the sentences was made for the purpose of this study by taking out the \"class\" column from the original data set.']\n",
            "[\"The particular algorithm of network community detection used in this study is Louvain method BIBREF2 which partitions a network into the number of nodes - every node is its own comunity, and from there, clusters the nodes in a way to maximize each cluster's modularity which indicates how strong is the connectivity between the nodes in the community. This means that, based on the cosine similarity scores - the networks edge weights, the algorithm clusters similar sentences together in a same community while the algorithm proceeds maximizing the connectivity strength amongst the nodes in each community. The network constructed with no threshold in place was detected to have 18 distinct communities with three single node communities. Based on the visualized network (see Figure.FIGREF13), it seemed that the network community detection method clustered the sentence network as good as the original data set with human labeled classes although the communities do not look quite distinct. However, based on the fact that it had three single node communities and the number of distinct communities is less than the number of classes in the human labeled data set, we suspected possible problems that would degrade the quality of the community detection for the use of training text classification models.\"]\n",
            "['The data set obtained from Pypestream is permitted to be used for the research purpose only, and for a security reason, we are not allowed to share the data set. It was once originaly used for creating a conversational intelligence system(chatbot) to support customer inqueries about a particular service. The data set is a two-column comma separated value format data with one column of \"sentence\" and the other column of \"class\". It contains 2,212 unique sentences of user expressions asking questions and aswering to the questions the chatbot asked to the users(see Table.TABREF9). The sentences are all in English without having any missspelled words, and labeled with 19 distinct classes that are identified and designed by humans. Additional data set that only contains the sentences was made for the purpose of this study by taking out the \"class\" column from the original data set.']\n",
            "['As originally introduced by BIBREF1, we compute the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). For example, the bias score of scientist is:', 'If the bias score during testing is greater than the one during training,', 'then the bias of man towards scientist has been amplified by the model while learning such representation, given training and testing datasets similarly distributed.']\n",
            "['As originally introduced by BIBREF1, we compute the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). For example, the bias score of scientist is:', 'We compute the bias amplification metric for all models, as defined in Section SECREF4, to study the effect of amplifying potential bias in text for different language generation models.']\n",
            "['We introduce a novel architecture that considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network.']\n",
            "['Neural Networks have proven to be useful for automating tasks such as question answering, system response, and language generation considering large textual datasets. In learning systems, bias can be defined as the negative consequences derived by the implicit association of patterns that occur in a high-dimensional space. In dialogue systems, these patterns represent associations between word embeddings that can be measured by a Cosine distance to observe male- and female-related analogies that resemble the gender stereotypes of the real world. We propose an automatic technique to mitigate bias in language generation models based on the use of an external memory in which word embeddings are associated to gender information, and they can be sparsely updated based on content-based lookup.']\n",
            "['Submission 3 BIBREF4 , ranked first, employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations.']\n",
            "[\"We measure the inter-annotator agreement between the annotators based on the Cohen's Kappa (cf. Table TABREF15 ) calculated on the first 200 tweets of the training set. According to BIBREF1 our score for Cohen's Kappa (0,70) indicates a strong agreement.\"]\n",
            "[\"In this paper, we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. We experiment with a number of auxiliary penalty terms to regularise the fine-tuning process for three modern RC models: QANet BIBREF10, decaNLP BIBREF11 and BERT BIBREF12. We observe that combining different auxiliary penalty terms results in the best performance, outperforming benchmark methods that require source data.\"]\n",
            "['To reduce the forgetting of source domain knowledge, we introduce auxiliary penalty terms to regularise the fine-tuning process. We favour this approach as it does not require storing data samples from the source domain. In general, there are two types of penalty: selective and non-selective. The former penalises the model when certain parameters diverge significantly from the source model, while the latter uses a pre-defined distance function to measure the change of all parameters.', 'For selective penalty, we use elastic weight consolidation (EWC: BIBREF8), which weighs the importance of a parameter based on its gradient when training the source model. For non-selective penalty, we explore L2 BIBREF7 and cosine distance. We detail the methods below.']\n",
            "['We compare our model with both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16.']\n",
            "['We compare our model with both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16.', 'Among these methods, FM, SVD, PMF, and NMF are rating-based collaborative filtering methods. DeepCoNN, D-ATT, MPCN, and HUITA are state-of-the-art methods that leverage the semantic information in reviews for improved performance. Specifically, DeepCoNN uses the same CNN module to learn user and item embeddings based on their reviews for recommendation. D-ATT extends DeepCoNN by adding a dual-attention layer at word-level before convolution. MPCN attends to informative reviews by several pointers. HUITA uses a symmetric hierarchical structure to infer user (item) embeddings using regular attention mechanisms. It is worth noting that all of the above review-based methods regard user reviews and item reviews as the same type of documents and process them in an identical way.']\n",
            "[\"We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews. Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user–item pairs into $80\\\\%$ training set, $10\\\\%$ validation set, and $10\\\\%$ testing set. When learning the representations for users and items, we only use their reviews from the training set, and none from the validation and testing sets. This ensures a practical scenario where we cannot include any future reviews into a user's (item's) history for model training.\"]\n",
            "[\"We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews. Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user–item pairs into $80\\\\%$ training set, $10\\\\%$ validation set, and $10\\\\%$ testing set. When learning the representations for users and items, we only use their reviews from the training set, and none from the validation and testing sets. This ensures a practical scenario where we cannot include any future reviews into a user's (item's) history for model training.\"]\n",
            "['Adversarial Examples. Adversarial examples are created to attack a neural network to make erroneous predictions. There are two main types of adversarial attacks which are called white-box and black-box. White-box attacks BIBREF32 have access to the model parameters, while black-box attacks BIBREF33 work only on the input and output. In this work, we utilize a white-box method working on the embedding level. In order to create adversarial examples, we utilize the formula used by BIBREF16, where the perturbations are created using gradient of the loss function. Assuming $p(y|x;\\\\theta )$ is the probability of label $y$ given the input $x$ and the model parameters $\\\\theta $, in order to find the adversarial examples the following minimization problem should be solved:', 'where $r$ denotes the perturbations on the input and $\\\\hat{\\\\theta }$ is a constant copy of $\\\\theta $ in order not to allow the gradients to propagate in the process of constructing the artificial examples. Solving the above minimization problem means that we are searching for the worst perturbations while trying to minimize the loss of the model. An approximate solution for Equation DISPLAY_FORM3 is found by linearizing $\\\\log p(y|x;\\\\theta )$ around $x$ BIBREF0. Therefore, the following perturbations are added to the input embeddings to create new adversarial sentences in the embedding space.']\n",
            "['Our model is depicted in Figure FIGREF1. As can be seen, we create adversarial examples from BERT embeddings using the gradient of the loss. Then, we feed the perturbed examples to the BERT encoder to calculate the adversarial loss. In the end, the backpropagation algorithm is applied to the sum of both losses.']\n",
            "['To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo, Baidu Zhidao, and Baidu Tieba. We filtered out short and meaningless replies like “...” and “Errr.” In total, the database contains 7 million query-reply pairs for retrieval.', 'For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query $q$ , we searched for a candidate reply $r^*$ by the retrieval component and obtained a tuple $\\\\langle q, r^*, r\\\\rangle $ . As a friendly reminder, $q$ and $r^*$ are the input of biseq2seq, whose output should approximate $r$ . We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.']\n",
            "['Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 .']\n",
            "['Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 .']\n",
            "['Additionally, the bottleneck of the LSTM approach is the training speed. The training process of the LSTM model by BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days to produce the claimed results.']\n",
            "[\"Most supervised approaches focus on extracting features from words in the context. Early approaches mostly depend on hand-crafted features. For example, IMS by BIBREF2 uses POS tags, surrounding words and collections of local words as features. These approaches are later improved by combining with word embedding features BIBREF0 , which better represents the words' semantic information in a real-value space. However, these methods neglect the valuable positional information between the words in the sequence BIBREF3 . The bi-directional Long-Short-Term-Memory (LSTM) approach by BIBREF3 provides one way to leverage the order of words. Recently, BIBREF4 improved the performance by pre-training a LSTM language model with a large unlabelled corpus, and using this model to generate sense vectors for further WSD predictions. However, LSTM significantly increases the computational complexity during the training process.\", 'Table TABREF6 presents the micro F1 scores from different models. Note that we use a corpus with 0.8 billion words and vocabulary of 100,000 words when training the language model, comparing with BIBREF4 using 100 billion words and vocabulary of 1,000,000 words. The context abstraction using the language model is the most crucial step. The sizes of the training corpus and vocabulary significantly affect the performance of this process, and consequently the final WSD results. However, BIBREF4 did not publish the 100 billion words corpus used for training their LSTM language model.']\n",
            "['A language model is trained with large unlabelled corpus by BIBREF4 in order to overcome the shortage of WSD training data. A language model represents the probability distribution of a given sequence of words, and it is commonly used in predicting the subsequent word given preceding sequence. BIBREF5 proposed a FOFE-based neural network language model by feeding FOFE code of preceding sequence into FFNN. WSD is different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Hence, we build a pseudo language model that uses both preceding and succeeding sequence to accommodate the purpose of WSD tasks.']\n",
            "['Table 1 shows PTB test set BPC among our baseline models, our proposed models and several published results. Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task.', 'Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.']\n",
            "['We presented a new RNN architecture by combining the gating mechanism and tensor product concepts. Our proposed architecture can learn long-term dependencies from temporal and sequential data using gating units as well as more powerful interaction between the current input and previous hidden layers by introducing tensor product operations. From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. In the future, we will investigate the possibility of combining our model with other stacked RNNs architecture, such as Gated Feedback RNN (GFRNN). We would also like to explore other possible tensor operations and integrate them with our RNN architecture. By applying these ideas together, we expect to gain further performance improvement. Last, for further investigation we will apply our proposed models to other temporal and sequential tasks, such as speech recognition and video recognition.']\n",
            "['Previously in Sections \"Experiment Settings\" and \"Recursive Neural Tensor Network\" , we discussed that the gating mechanism concept can helps RNNs learn long-term dependencies from sequential input data and that adding more powerful interaction between the input and hidden layers simultaneously with the tensor product operation in a bilinear form improves neural network performance and expressiveness. By using tensor product, we increase our model expressiveness by using second-degree polynomial interactions, compared to first-degree polynomial interactions on standard dot product followed by addition in common RNNs architecture. Therefore, in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values. The calculation is parameterized by tensor weight. To construct a GRURNTN, we defined the formulation as:', 'As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). To construct an LSTMRNTN, we defined its formulation:']\n",
            "['However, standard RecNNs have several limitations, where two vectors only implicitly interact with addition before applying a nonlinear activation function on them BIBREF12 and standard RecNNs are not able to model very long-term dependency on tree structures. Zhu et al. BIBREF20 proposed the gating mechanism into standard RecNN model to solve the latter problem. For the former limitation, the RecNN performance can be improved by adding more interaction between the two input vectors. Therefore, a new architecture called a Recursive Neural Tensor Network (RecNTN) tried to overcome the previous problem by adding interaction between two vectors using a tensor product, which is connected by tensor weight parameters. Each slice of the tensor weight can be used to capture the specific pattern between the left and right child vectors. For RecNTN, value $p_1$ from Eq. 13 and is defined by:', '$$p_1 &=& f\\\\left( \\\\begin{bmatrix} x_1 & x_2 \\\\end{bmatrix} W_{tsr}^{[1:d]} \\\\begin{bmatrix} x_1 \\\\\\\\ x_2 \\\\end{bmatrix} + \\\\begin{bmatrix} x_1 & x_2 \\\\end{bmatrix} W + b \\\\right) \\\\\\\\ p_2 &=& f\\\\left( \\\\begin{bmatrix} p_1 & x_3 \\\\end{bmatrix} W_{tsr}^{[1:d]} \\\\begin{bmatrix} p_1 \\\\\\\\ x_3 \\\\end{bmatrix} + \\\\begin{bmatrix} p_1 & x_3 \\\\end{bmatrix} W + b \\\\right)$$ (Eq. 15)', 'where $W_{tsr}^{[1:d]} \\\\in \\\\mathbb {R}^{2d \\\\times 2d \\\\times d}$ is the tensor weight to map the tensor product between two children vectors. Each slice $W_{tsr}^{[i]}$ is a matrix $\\\\mathbb {R}^{2d \\\\times 2d}$ . For more details, we visualize the calculation for $p_1$ in Fig. 5 .']\n",
            "[\"In this section, we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN. Fig. 8 shows performance comparisons from every model based on the validation set's BPC per epoch. In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch.\", 'Table 1 shows PTB test set BPC among our baseline models, our proposed models and several published results. Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task.', \"In this section, we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN. Fig. 9 compares the performance from every models based on the validation set's PPL per epoch. In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models.\", 'Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.', 'We presented a new RNN architecture by combining the gating mechanism and tensor product concepts. Our proposed architecture can learn long-term dependencies from temporal and sequential data using gating units as well as more powerful interaction between the current input and previous hidden layers by introducing tensor product operations. From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. In the future, we will investigate the possibility of combining our model with other stacked RNNs architecture, such as Gated Feedback RNN (GFRNN). We would also like to explore other possible tensor operations and integrate them with our RNN architecture. By applying these ideas together, we expect to gain further performance improvement. Last, for further investigation we will apply our proposed models to other temporal and sequential tasks, such as speech recognition and video recognition.']\n",
            "['We train a Bidirectional Encoding model BIBREF2 , which has achieved state-of-the-art results for Twitter stance detection on our dataset. The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction. To reduce the sequence length, we use the same context window that was presented to annotators for training the LSTM. We use pretrained GloVe embeddings BIBREF13 and tune hyperparameters on a validation set. The best model achieves a test accuracy of INLINEFORM0 and a macro-averaged test F1 score of INLINEFORM1 . It significantly outperforms baselines such as a bag-of-n-grams (accuracy: INLINEFORM2 ; F1: INLINEFORM3 ).']\n",
            "['We train a Bidirectional Encoding model BIBREF2 , which has achieved state-of-the-art results for Twitter stance detection on our dataset. The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction. To reduce the sequence length, we use the same context window that was presented to annotators for training the LSTM. We use pretrained GloVe embeddings BIBREF13 and tune hyperparameters on a validation set. The best model achieves a test accuracy of INLINEFORM0 and a macro-averaged test F1 score of INLINEFORM1 . It significantly outperforms baselines such as a bag-of-n-grams (accuracy: INLINEFORM2 ; F1: INLINEFORM3 ).']\n",
            "[\"The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance. In particular, 47.67% examples have been annotated with `neutral', 21.9% with `against', 19.05% with `in favour', and 11.38% with `unrelated`. We use 70% of examples for training, 20% for validation, and 10% for testing according to a stratified split. As we expect to encounter novel and unknown entities in the wild, we ensure that entities do not overlap across splits and that we only test on unseen entities.\"]\n",
            "['We collected datasets of news articles in English and German language from the news agency Reuters (Table TABREF13 ). After a data cleaning step, which was deleting meta information like author and editor name from the article, title, body and date were stored in a local database and imported to a Pandas data frame BIBREF12 . The English corpus has a dictionary of length 106.848, the German version has a dictionary of length 163.788.']\n",
            "['The previous state-of-the-art Dynamic-RNN model from BIBREF4 ouchi-tsuboi:2016:EMNLP2016 maintains speaker embeddings to track each speaker status, which dynamically changes across time steps. It then produces the context embedding from the speaker embeddings and selects the addressee and response based on embedding similarity. However, this model updates only the sender embedding, not the embeddings of the addressee or observers, with the corresponding utterance, and it selects the addressee and response separately. In this way, it only models who says what and fails to capture addressee information. Experimental results show that the separate selection process often produces inconsistent addressee-response pairs.']\n",
            "['The previous state-of-the-art Dynamic-RNN model from BIBREF4 ouchi-tsuboi:2016:EMNLP2016 maintains speaker embeddings to track each speaker status, which dynamically changes across time steps. It then produces the context embedding from the speaker embeddings and selects the addressee and response based on embedding similarity. However, this model updates only the sender embedding, not the embeddings of the addressee or observers, with the corresponding utterance, and it selects the addressee and response separately. In this way, it only models who says what and fails to capture addressee information. Experimental results show that the separate selection process often produces inconsistent addressee-response pairs.']\n",
            "[\"We use our dictionaries to train and evaluate three of the best performing BLI models BIBREF3, BIBREF4, BIBREF5 on all 40 language pairs. To paint a complete picture of the models' generalization ability we propose a new experimental paradigm in which we independently control for four different variables: the word form's frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words BIBREF6. Our findings also contradict the strong empirical claims made elsewhere in the literature BIBREF4, BIBREF2, BIBREF5, BIBREF7, as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure FIGREF2, which also highlights the skew of existing dictionaries towards more frequent words. As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs.z\"]\n",
            "[\"We use our dictionaries to train and evaluate three of the best performing BLI models BIBREF3, BIBREF4, BIBREF5 on all 40 language pairs. To paint a complete picture of the models' generalization ability we propose a new experimental paradigm in which we independently control for four different variables: the word form's frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words BIBREF6. Our findings also contradict the strong empirical claims made elsewhere in the literature BIBREF4, BIBREF2, BIBREF5, BIBREF7, as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure FIGREF2, which also highlights the skew of existing dictionaries towards more frequent words. As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs.z\"]\n",
            "['In our final experiment we demonstrate that improving morphological generalization has the potential to improve BLI results. We show that enforcing a simple, hard morphological constraint at training time can lead to performance improvements at test time—both on the standard BLI task and the controlled for lexeme BLI. We adapt the self-learning models of BIBREF4 and BIBREF5 so that at each iteration they can align two words only if they share the same morphosyntactic category. Note that this limits the training data only to word forms present in UniMorph, as those are the only ones for which we have a gold tag. The results, a subset of which we present in Table TABREF35, show that the constraint, despite its simplicity and being trained on less data, leads to performance improvements for every Romance language pair and many of the Slavic language pairs. We take this as evidence that properly modelling morphology will have a role to play in BLI.']\n",
            "['Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction', 'Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Introduction']\n",
            "['Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction', 'Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Introduction']\n",
            "[\"Motivated by traditional stylometry and the growing interest in matching user accounts across Internet services, we created models for Digital Stylometry, which fuses traditional stylometry techniques with big-data driven social informatics methods used commonly in analyzing social networks. Our models use linguistic and temporal activity patterns of users on different accounts to match accounts belonging to the same person. We evaluated our models on $11,224$ accounts belonging to $5,612$ distinct users on two of the largest social media networks, Twitter and Facebook. The only information that was used in our models were the time and the linguistic content of posts by the users. We intentionally did not use any other information, especially the potentially personally identifiable information that was explicitly provided by the user, such as the screen name, birthday or location. This is in accordance with traditional stylometry techniques, since people could misstate, omit, or lie about this information. Also, we wanted to show that there are implicit clues about the identities of users in the content (language) and context (time) of the users' interactions with social networks that can be used to link their accounts across different services.\"]\n",
            "['For the purposes of this paper, we focused on matching accounts between two of the largest social networks: Twitter and Facebook. In order to proceed with our study, we needed a sizeable (few thousand) number of English speaking users with accounts on both Twitter and Facebook. We also needed to know the precise matching between the Twitter and Facebook accounts for our ground truth.', \"To that end, we crawled publicly available, English-language, Google Plus accounts using the Google Plus API and scraped links to the users' other social media profiles. (Note that one of the reasons why we used Twitter and Facebook is that they were two of the most common sites linked to on Google Plus). We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth in order to limit selection bias in our data collection.\", 'We discarded all users who did not link to an account for both Twitter and Facebook and those whose accounts on either of these sites were not public. We then used the APIs of Twitter and Facebook to collect posts made by the users on these sites. We only collected the linguistic content and the date and time at the which the posts were made. For technical and privacy reasons, we did not collect any information from the profile of the users, such as the location, screen name, or birthday.']\n",
            "[\"To that end, we crawled publicly available, English-language, Google Plus accounts using the Google Plus API and scraped links to the users' other social media profiles. (Note that one of the reasons why we used Twitter and Facebook is that they were two of the most common sites linked to on Google Plus). We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth in order to limit selection bias in our data collection.\"]\n",
            "['Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. “Cristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.“Ronaldo\", “CR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.']\n",
            "['Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. “Cristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.“Ronaldo\", “CR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.']\n",
            "['The combination of Topic Modeling and Sentiment Analysis has been attempted before: one example is a model called TSM - Topic-Sentiment Mixture Model BIBREF3 that can be applied to any Weblog to determine a correlation between topic and sentiment. Another similar model has been proposed proposed BIBREF4 in which the topic extraction is achieved using LDA, similarly to the model that will be presented. Our work distinguishes from previous work by relying on daily entity-centric aggregations of tweets to create a meta-document which will be used as input for topic modeling and sentiment analysis.', 'A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles.']\n",
            "['Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. “Cristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.“Ronaldo\", “CR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.', 'A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as “the\" or “a\";', 'A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles.']\n",
            "['Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. “Cristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.“Ronaldo\", “CR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.', 'A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as “the\" or “a\";', 'Every word with less than three characters is removed, except some whitelisted words that can actually be meaningful (e.g. “PSD\" may refer to a portuguese political party);']\n",
            "[\"Thus, when compared to the slightly nonlinear scaling of total amount of words, not all words follow the growth homogeneously with this same exponent. Though a significant amount remains in the linear or inconclusive range according to the statistical model test, most words are sensitive to city size and exhibit a super- or sublinear scaling. Those that fit the linear model the best, correspond to a kind of 'core-Twitter' vocabulary, which has a lot in common with the most common words of the English language, but also shows some Twitter-specific elements. A visible group of words that are amongst the most super- or sublinearly scaling words are related to the abundance or lack of the elements of urban lifestyle (e.g. deer, fitness). Thus, the imprint of the physical environment appears in a quantifiable way in the growths of word occurrences as a function of urban populations. Swearwords and slang, that are quite prevalent in this type of corpus BIBREF7 , BIBREF6 , appear at both ends of the regime that suggests that some specific forms of swearing disappear with urbanization, but the share of overall swearing on Twitter grows with city size. The peak consisting of Spanish words at the superlinear end of the exponent distribution marks the stronger presence of the biggest non-English speaking ethnicity in bigger urban areas. This is confirmed by fitting the scaling relationship to the Hispanic or Latino population BIBREF53 of the MSA areas ( INLINEFORM0 , see SI), which despite the large error, is very superlinear.\"]\n",
            "['Semantic Features', 'Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.', 'Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.', 'Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.', \"Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.\", 'N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.', 'Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.', 'Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.', \"Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.\", \"Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.\", 'Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.']\n",
            "[\"Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories.\"]\n",
            "[\"Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories.\"]\n",
            "['We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific. All of our classifiers are evaluated using 20-fold cross validation. Table TABREF9 shows the performance of our five classifiers trained and evaluated on all of the data. We report the F1 score for each class. As shown in Table TABREF9 , the logistic regression was the performing classifier with a weighted average F1 score of INLINEFORM0 . Thus we picked logistic regression as our classier and the rest of the results reported will be for LR only. Table TABREF10 shows the average performance of the LR classifier for Twitter-wide, type and topic specific classifiers.']\n",
            "[\"The topic-specific classifiers' average performance was better than that of the type-specific classifiers ( INLINEFORM0 and INLINEFORM1 respectively) which was in turn marginally better than the performance of the Twitter-wide classifier ( INLINEFORM2 ). This confirms our earlier hypothesis that the more granular type and topic specific classifiers would be superior to a more general Twitter-wide classifier.\"]\n",
            "['We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "[\"Given the diversity of topics talked about on Twitter, we wanted to explore topic and type dependent speech act classifiers. We used Zhao et al.'s BIBREF7 definitions for topic and type. A topic is a subject discussed in one or more tweets (e.g., Boston Marathon bombings, Red Sox, etc). The type characterizes the nature of the topic, these are: Entity-oriented, Event-oriented topics, and Long-standing topics (topics about subjects that are commonly discussed).\", 'We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "['We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "['We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "['We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "['We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).']\n",
            "['We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.', 'Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.', 'Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.', 'Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.', \"Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.\", 'N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.']\n",
            "['We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.', 'Syntactic Features', 'Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.', 'Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.', \"Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.\", \"Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.\", 'Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.']\n",
            "['To make our work easier and compatible to more available singing content, we collected 130 music-removed (or vocal-only) English songs from www.youtube.com so as to consider only the vocal line.The music-removing processes are conducted by the video owners, containing the original vocal recordings by the singers and vocal elements for remix purpose.', 'After initial test by speech recognition system trained with LibriSpeech BIBREF20 , we dropped 20 songs, with WERs exceeding 95%. The remaining 110 pieces of music-removed version of commercial English popular songs were produced by 15 male singers, 28 female singers and 19 groups. The term group means by more than one person. No any further preprocessing was performed on the data, so the data preserves many characteristics of the vocal extracted from commercial polyphonic music, such as harmony, scat, and silent parts. Some pieces also contain overlapping verses and residual background music, and some frequency components may be truncated. Below this database is called vocal data here.']\n",
            "['To make our work easier and compatible to more available singing content, we collected 130 music-removed (or vocal-only) English songs from www.youtube.com so as to consider only the vocal line.The music-removing processes are conducted by the video owners, containing the original vocal recordings by the singers and vocal elements for remix purpose.']\n",
            "['In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.']\n",
            "['In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.']\n",
            "['Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 . For character-level embedding, we used one hot encoding with 69 characters (68 ASCII characters plus one unknown character). Both word embedding and character embedding matrix were fixed during the training. After algorithm SECREF12 was applied, the remaining out-of-vocabulary words were initialized as zero vectors. We used Stanford PTBTokenizer BIBREF32 on the Ubuntu corpus. The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus. For the ensemble model, we use the average prediction output of models with different runs. On both corpuses, the dimension of word2vec vectors generated on the training set is 100.', 'On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (RNN, CNN, LSTM, BiLSTM, Dual-Encoder) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 .']\n",
            "['Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 . For character-level embedding, we used one hot encoding with 69 characters (68 ASCII characters plus one unknown character). Both word embedding and character embedding matrix were fixed during the training. After algorithm SECREF12 was applied, the remaining out-of-vocabulary words were initialized as zero vectors. We used Stanford PTBTokenizer BIBREF32 on the Ubuntu corpus. The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus. For the ensemble model, we use the average prediction output of models with different runs. On both corpuses, the dimension of word2vec vectors generated on the training set is 100.']\n",
            "[\"2-way classification b/w satire and trusted articles: We use the satirical and trusted news articles from LUN-train for training, and from LUN-test as the development set. We evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our classifier on an out of domain dataset. We don't use SLN for training purposes because it just contains 360 examples which is too little for training our model and we want to have an unseen test set. The best performing model on SLN is used to evaluate the performance on RPN.\", '4-way classification b/w satire, propaganda, hoax and trusted articles: We split the LUN-train into a 80:20 split to create our training and development set. We use the LUN-test as our out of domain test set.']\n",
            "[\"Table TABREF20 shows the quantitative results for the two way classification between satirical and trusted news articles. Our proposed GAT method with 2 attention heads outperforms SoTA. The semantic similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper BIBREF2 reports a 10-fold cross validation number on SLN. We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set. The SoTA paper BIBREF10 on RPN reports a 5-fold cross validation accuracy of 91%. These results indicate the generalizability of our proposed model across datasets. We also present results of four way classification in Table TABREF21. All of our proposed methods outperform SoTA on both the in-domain and out of domain test set.\"]\n",
            "['We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,']\n",
            "['We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,']\n",
            "['We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,', 'CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer BIBREF11 with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.', 'LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.', 'BERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) BIBREF4 for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document.']\n",
            "['We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,', 'CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer BIBREF11 with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.', 'LSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.', 'BERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) BIBREF4 for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document.']\n",
            "['We modify our model accordingly for every research question, based on changes in the input representation. To benchmark the performance of our deep learning models, we compare them against a logistic regression (LR) baseline that learns on one-hot representations of the Java tokens extracted from the commit diffs. For all of our models, we employ dropout on the fully-connected layer for regularization. We use Adam BIBREF25 for optimization, with a learning rate of 0.001, and batch size of 16 for randomly initialized embeddings and 8 for pre-trained embeddings.', 'For RQ1, we use a hierarchical CNN (H-CNN) with either randomly-initialized or pre-trained word embeddings in order to extract features from the commit diff. We represent the commit diff as a concatenation of 300-dimensional vectors for each corresponding token from that diff. This resultant matrix is then passed through three temporal convolutional layers in parallel, with filter windows of size 3, 5, and 7. A temporal max-pooling operation is applied to these feature maps to retain the feature with the highest value in every map. We also present a regularized version of this model (henceforth referred to as HR-CNN) with embedding dropout applied on the inputs, and DropBlock on the activations of the convolutional layers.']\n",
            "['For RQ1, we use a hierarchical CNN (H-CNN) with either randomly-initialized or pre-trained word embeddings in order to extract features from the commit diff. We represent the commit diff as a concatenation of 300-dimensional vectors for each corresponding token from that diff. This resultant matrix is then passed through three temporal convolutional layers in parallel, with filter windows of size 3, 5, and 7. A temporal max-pooling operation is applied to these feature maps to retain the feature with the highest value in every map. We also present a regularized version of this model (henceforth referred to as HR-CNN) with embedding dropout applied on the inputs, and DropBlock on the activations of the convolutional layers.']\n",
            "['For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.']\n",
            "['For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.']\n",
            "['In the sampling layer, we sample continuous sub-strings from the embedding layer, which is also called span. Because we know the exact label of each sample in the training phase, so we can train the model in a particular way. For those negative samples, which means each span does not belong to any entity class, we randomly sampling them rather than enumerate them all. This is a simple but effective way to improve both performance and efficiency. For those ground truth, we keep them all. In this way, we can obtain a balanced span set: $S = S_{neg} \\\\cup S_{pos} $. In which $S_{neg} = \\\\lbrace s^{\\\\prime }_1, s^{\\\\prime }_2, \\\\dots , s^{\\\\prime }_p\\\\rbrace $, $S_{pos} = \\\\lbrace s_1, s_2, \\\\dots , s_q\\\\rbrace $. Both $s$ and $s^{\\\\prime }$ is consist of $\\\\lbrace \\\\mathbf {e}_i ,\\\\dots ,\\\\mathbf {e}_j\\\\rbrace $, $i$ and $j$ are the start and end index of the span. $p$ is a hyper-parameter: the negative sample number. $q$ is the positive sample number. We further explore the effect of different $p$ in the experiment section.', 'Span extractor is responsible to extract a span representation from embeddings. In previous work BIBREF8, endpoint features, content attention, and span length embedding are concatenated to represent a span. We perform a simple max-pooling to extract span representation because those features are implicitly included in self-attention layers of transformers. Formally, each element in the span vector is:']\n",
            "[\"To balance the positive and negative samples and reduce the search space, we remove the pruner and modify the model by under-sampling. Furthermore, because there is a multi-head self-attention mechanism in transformers and they can capture interactions between tokens, we don't need more attention or LSTM network in span extractors. So we simplify the origin network architecture and extract span representation by a simple pooling layer. We call the final scientific named entity recognizer SEPT.\", 'Experiments demonstrate that even simplified architecture achieves the same performance and SEPT achieves a new state of the art result compared to existing transformer-based systems.']\n",
            "['We discover that performance improvement is mainly supported by the pre-trained external resources, which is very helpful for such a small dataset. In ELMo model, SCIIE achieves almost 3.0% F1 higher than BiLSTM. But in SciBERT, the performance becomes similar, which is only a 0.5% gap.', 'SEPT still has an advantage comparing to the same transformer-based models, especially in the recall.']\n",
            "['The paper makes three main contributions. First, we introduce a novel dataset consisting of 1,268 short video clips paired with sets of actions mentioned in the video transcripts, as well as manual annotations of whether the actions are visible or not. The dataset includes a total of 14,769 actions, 4,340 of which are visible. Second, we propose a set of strong baselines to determine whether an action is visible or not. Third, we introduce a multimodal neural architecture that combines information drawn from visual and linguistic clues, and show that it improves over models that rely on one modality at a time.']\n",
            "['The paper makes three main contributions. First, we introduce a novel dataset consisting of 1,268 short video clips paired with sets of actions mentioned in the video transcripts, as well as manual annotations of whether the actions are visible or not. The dataset includes a total of 14,769 actions, 4,340 of which are visible. Second, we propose a set of strong baselines to determine whether an action is visible or not. Third, we introduce a multimodal neural architecture that combines information drawn from visual and linguistic clues, and show that it improves over models that rely on one modality at a time.']\n",
            "['The paper makes three main contributions. First, we introduce a novel dataset consisting of 1,268 short video clips paired with sets of actions mentioned in the video transcripts, as well as manual annotations of whether the actions are visible or not. The dataset includes a total of 14,769 actions, 4,340 of which are visible. Second, we propose a set of strong baselines to determine whether an action is visible or not. Third, we introduce a multimodal neural architecture that combines information drawn from visual and linguistic clues, and show that it improves over models that rely on one modality at a time.']\n",
            "['Concreteness. We label as visible all the actions that have a concreteness score above a certain threshold, and label as non-visible the remaining ones. We fine tune the threshold on our validation set; for fine tuning, we consider threshold values between 3 and 5. Table TABREF20 shows the results obtained for this baseline.', 'Feature-based Classifier. For our second set of baselines, we run a classifier on subsets of all of our features. We use an SVM BIBREF50 , and perform five-fold cross-validation across the train and validation sets, fine tuning the hyper-parameters (kernel type, C, gamma) using a grid search. We run experiments with various combinations of features: action GloVe embeddings; POS embeddings; embeddings of sentence-level context (Context INLINEFORM0 ) and action-level context (Context INLINEFORM1 ); concreteness score. The combinations that perform best during cross-validation on the combined train and validation sets are shown in Table TABREF20 .', 'LSTM and ELMo. We also consider an LSTM model BIBREF36 that takes as input the tokenized action sequences padded to the length of the longest action. These are passed through a trainable embedding layer, initialized with GloVe embeddings, before the LSTM. The LSTM output is then passed through a feed forward network of fully connected layers, each followed by a dropout layer BIBREF51 at a rate of 50%. We use a sigmoid activation function after the last hidden layer to get an output probability distribution. We fine tune the model on the validation set for the number of training epochs, batch size, size of LSTM, and number of fully-connected layers.', 'We build a similar model that embeds actions using ELMo (composed of 2 bi-LSTMs). We pass these embeddings through the same feed forward network and sigmoid activation function. The results for both the LSTM and ELMo models are shown in Table TABREF20 .', 'Yolo Object Detection. Our final baseline leverages video information from the YOLO9000 object detector. This baseline builds on the intuition that many visible actions involve visible objects. We thus label an action as visible if it contains at least one noun similar to objects detected in its corresponding miniclip. To measure similarity, we compute both the Wu-Palmer (WUP) path-length-based semantic similarity BIBREF52 and the cosine similarity on the GloVe word embeddings. For every action in a miniclip, each noun is compared to all detected objects and assigned a similarity score. As in our concreteness baseline, the action is assigned the highest score of its corresponding nouns. We use the validation data to fine tune the similarity threshold that decides if an action is visible or not. The results are reported in Table TABREF20 . Examples of actions that contain one or more words similar to detected objects by Yolo can be seen in Figure FIGREF18 .']\n",
            "['Feature-based Classifier. For our second set of baselines, we run a classifier on subsets of all of our features. We use an SVM BIBREF50 , and perform five-fold cross-validation across the train and validation sets, fine tuning the hyper-parameters (kernel type, C, gamma) using a grid search. We run experiments with various combinations of features: action GloVe embeddings; POS embeddings; embeddings of sentence-level context (Context INLINEFORM0 ) and action-level context (Context INLINEFORM1 ); concreteness score. The combinations that perform best during cross-validation on the combined train and validation sets are shown in Table TABREF20 .', 'LSTM and ELMo. We also consider an LSTM model BIBREF36 that takes as input the tokenized action sequences padded to the length of the longest action. These are passed through a trainable embedding layer, initialized with GloVe embeddings, before the LSTM. The LSTM output is then passed through a feed forward network of fully connected layers, each followed by a dropout layer BIBREF51 at a rate of 50%. We use a sigmoid activation function after the last hidden layer to get an output probability distribution. We fine tune the model on the validation set for the number of training epochs, batch size, size of LSTM, and number of fully-connected layers.', 'We build a similar model that embeds actions using ELMo (composed of 2 bi-LSTMs). We pass these embeddings through the same feed forward network and sigmoid activation function. The results for both the LSTM and ELMo models are shown in Table TABREF20 .', 'Yolo Object Detection. Our final baseline leverages video information from the YOLO9000 object detector. This baseline builds on the intuition that many visible actions involve visible objects. We thus label an action as visible if it contains at least one noun similar to objects detected in its corresponding miniclip. To measure similarity, we compute both the Wu-Palmer (WUP) path-length-based semantic similarity BIBREF52 and the cosine similarity on the GloVe word embeddings. For every action in a miniclip, each noun is compared to all detected objects and assigned a similarity score. As in our concreteness baseline, the action is assigned the highest score of its corresponding nouns. We use the validation data to fine tune the similarity threshold that decides if an action is visible or not. The results are reported in Table TABREF20 . Examples of actions that contain one or more words similar to detected objects by Yolo can be seen in Figure FIGREF18 .']\n",
            "['Our goal is to identify which of the actions extracted from the transcripts are visually depicted in the videos. We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible.']\n",
            "['Our goal is to identify which of the actions extracted from the transcripts are visually depicted in the videos. We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible.']\n",
            "['Segment Videos into Miniclips. The length of our collected videos varies from two minutes to twenty minutes. To ease the annotation process, we split each video into miniclips (short video sequences of maximum one minute). Miniclips are split to minimize the chance that the same action is shown across multiple miniclips. This is done automatically, based on the transcript timestamp of each action. Because YouTube transcripts have timing information, we are able to line up each action with its corresponding frames in the video. We sometimes notice a gap of several seconds between the time an action occurs in the transcript and the time it is shown in the video. To address this misalignment, we first map the actions to the miniclips using the time information from the transcript. We then expand the miniclip by 15 seconds before the first action and 15 seconds after the last action. This increases the chance that all actions will be captured in the miniclip.']\n",
            "['We have randomly selected 150 problems out of the RTE corpus which were marked as “YES” (i.e. entailment holds). The problems were not further selected nor doctored by us. The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. Similarly, if they considered that there was no entailment in the problem, they were given prompted to (optionally) give an argument for their judgement.']\n",
            "['We have randomly selected 150 problems out of the RTE corpus which were marked as “YES” (i.e. entailment holds). The problems were not further selected nor doctored by us. The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. Similarly, if they considered that there was no entailment in the problem, they were given prompted to (optionally) give an argument for their judgement.', 'Additionally, collecting all the inputs received and using our best judgments, we have put together a test set of 150 problems comprised of the original problems, a new judgement (“yes” or “no”), and added missing hypotheses (if “yes” is a reasonable option).']\n",
            "['We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU. Following BIBREF10 , we evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct-1 and Distinct-2. In this paper, we define a new metric, originality, that is defined as the ratio of generated responses that do not appear in the training set. Here, “appear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines. We ask the same three native speakers to do a side-by-side comparison BIBREF15 on the 1,000 contexts. Given a context and two responses generated by different models, we ask annotators to decide which response is better (Ties are permitted).']\n",
            "['Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results.']\n",
            "['Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results.']\n",
            "['Prior work BIBREF11 has figured out how to edit prototype in an unconditional setting, but it cannot be applied to the response generation directly. In this paper, we propose a prototype editing method in a conditional setting. Our idea is that differences between responses strongly correlates with differences in their contexts (i.e. if a word in prototype context is changed, its related words in the response are probably modified in the editing.). We realize this idea by designing a context-aware editing model that is built upon a encoder-decoder model augmented with an editing vector. The edit vector is computed by the weighted average of insertion word embeddings and deletion word embeddings. Larger weights mean that the editing model should pay more attention on corresponding words in revision. For instance, in Table TABREF1 , we wish words like “dessert\", “Tofu\" and “vegetables\" get larger weights than words like “and\" and “ at\". The encoder learns the prototype representation with a gated recurrent unit (GRU), and feeds the representation to a decoder together with the edit vector. The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.']\n",
            "['Prior work BIBREF11 has figured out how to edit prototype in an unconditional setting, but it cannot be applied to the response generation directly. In this paper, we propose a prototype editing method in a conditional setting. Our idea is that differences between responses strongly correlates with differences in their contexts (i.e. if a word in prototype context is changed, its related words in the response are probably modified in the editing.). We realize this idea by designing a context-aware editing model that is built upon a encoder-decoder model augmented with an editing vector. The edit vector is computed by the weighted average of insertion word embeddings and deletion word embeddings. Larger weights mean that the editing model should pay more attention on corresponding words in revision. For instance, in Table TABREF1 , we wish words like “dessert\", “Tofu\" and “vegetables\" get larger weights than words like “and\" and “ at\". The encoder learns the prototype representation with a gated recurrent unit (GRU), and feeds the representation to a decoder together with the edit vector. The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.']\n",
            "['Prior work BIBREF11 has figured out how to edit prototype in an unconditional setting, but it cannot be applied to the response generation directly. In this paper, we propose a prototype editing method in a conditional setting. Our idea is that differences between responses strongly correlates with differences in their contexts (i.e. if a word in prototype context is changed, its related words in the response are probably modified in the editing.). We realize this idea by designing a context-aware editing model that is built upon a encoder-decoder model augmented with an editing vector. The edit vector is computed by the weighted average of insertion word embeddings and deletion word embeddings. Larger weights mean that the editing model should pay more attention on corresponding words in revision. For instance, in Table TABREF1 , we wish words like “dessert\", “Tofu\" and “vegetables\" get larger weights than words like “and\" and “ at\". The encoder learns the prototype representation with a gated recurrent unit (GRU), and feeds the representation to a decoder together with the edit vector. The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.']\n",
            "['Inspired by this idea, we formulate the response generation process as follows. Given a conversational context INLINEFORM0 , we first retrieve a similar context INLINEFORM1 and its associated response INLINEFORM2 from a pre-defined index, which are called prototype context and prototype response respectively. Then, we calculate an edit vector by concatenating the weighted average results of insertion word embeddings (words in prototype context but not in current context) and deletion word embeddings (words in current context but not in prototype context). After that, we revise the prototype response conditioning on the edit vector. We further illustrate how our idea works with an example in Table TABREF1 . It is obvious that the major difference between INLINEFORM3 and INLINEFORM4 is what the speaker eats, so the phrase “raw green vegetables\" in INLINEFORM5 should be replaced by “desserts\" in order to adapt to the current context INLINEFORM6 . We hope that the decoder language model could remember the collocation of “desserts\" and “bad for health\", so as to replace “beneficial\" with “bad\" in the revised response. The new paradigm does not only inherits the fluency and informativeness advantages from retrieval results, but also enjoys the flexibility of generation results. Hence, our edit-based model is better than previous retrieval-based and generation-based models. The edit-based model can solve the “safe response\" problem of generative models by leveraging existing responses, and is more flexible than retrieval-based models, because it does not highly depend on the index and is able to edit a response to fit current context.']\n",
            "['A good prototype selector INLINEFORM0 plays an important role in the prototype-then-edit paradigm. We use different strategies to select prototypes for training and testing. In testing, as we described above, we retrieve a context-response pair INLINEFORM1 from a pre-defined index for context INLINEFORM2 according to the similarity of INLINEFORM3 and INLINEFORM4 . Here, we employ Lucene to construct the index and use its inline algorithm to compute the context similarity.']\n",
            "['Datasets. We use 10 datasets. For answer extraction datasets in which a reader chooses a text span in a given context, we use (1) CoQA BIBREF17, (2) DuoRC BIBREF18, (3) HotpotQA (distractor) BIBREF19, (4) SQuAD v1.1 BIBREF0, and (5) SQuAD v2.0 BIBREF20. For multiple choice datasets in which a reader chooses a correct option from multiple options, we use (6) ARC (Challenge) BIBREF21, (7) MCTest BIBREF22, (8) MultiRC BIBREF23, (9) RACE BIBREF24, and (10) SWAG BIBREF25. For the main analysis, we applied our ablation methods to development sets. We included SWAG because its formulation can be viewed as a multiple-choice MRC task and we would like to analyze the reasons for the high performance reported for the baseline model on this dataset BIBREF3. For preprocessing the datasets, we use CoreNLP BIBREF26. We specify further details in Appendix B.']\n",
            "['Datasets. We use 10 datasets. For answer extraction datasets in which a reader chooses a text span in a given context, we use (1) CoQA BIBREF17, (2) DuoRC BIBREF18, (3) HotpotQA (distractor) BIBREF19, (4) SQuAD v1.1 BIBREF0, and (5) SQuAD v2.0 BIBREF20. For multiple choice datasets in which a reader chooses a correct option from multiple options, we use (6) ARC (Challenge) BIBREF21, (7) MCTest BIBREF22, (8) MultiRC BIBREF23, (9) RACE BIBREF24, and (10) SWAG BIBREF25. For the main analysis, we applied our ablation methods to development sets. We included SWAG because its formulation can be viewed as a multiple-choice MRC task and we would like to analyze the reasons for the high performance reported for the baseline model on this dataset BIBREF3. For preprocessing the datasets, we use CoreNLP BIBREF26. We specify further details in Appendix B.']\n",
            "['Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\\\sigma _4$ vocabulary anonymization, we train the model after the anonymization. For ARC, MCTest, and MultiRC, we fine-tuned a model that had already been trained on RACE to see the performance gained by transfer learning BIBREF27. We report the hyperparameters of our models in Appendix C. Although we trained the baseline model on the original training set, it is assumed that the upper-bound performance can be achieved by a model trained on the modified training set. Therefore, in Section SECREF16, we also see the extent to which the performance improves when the model is trained on the modified training set.']\n",
            "['Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\\\sigma _4$ vocabulary anonymization, we train the model after the anonymization. For ARC, MCTest, and MultiRC, we fine-tuned a model that had already been trained on RACE to see the performance gained by transfer learning BIBREF27. We report the hyperparameters of our models in Appendix C. Although we trained the baseline model on the original training set, it is assumed that the upper-bound performance can be achieved by a model trained on the modified training set. Therefore, in Section SECREF16, we also see the extent to which the performance improves when the model is trained on the modified training set.']\n",
            "['To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted experiments on three NMT tasks, English-to-German translation (En-De) with a large dataset, English-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two datasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset for WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The source and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013 for validation and the newstest 2014 as our test set. We report the results on the test set.', 'For En-Vi, we trained our model on the dataset in IWSLT 2015 BIBREF20. The dataset consists of around 133K sentence pairs from translated TED talks. The vocabulary size for source language is around 17,200 and that for target language is around 7,800. We used tst2012 for validation, and tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT 2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences. Following BIBREF21, we used the same test set with around 7K sentences. The data were preprocessed with byte-pair encoding BIBREF22. The vocabulary size is 14,000.', 'We evaluated our approach on the image captioning task. Image captioning is a task that combines image understanding and language generation. We conducted experiments on the Microsoft COCO 2014 dataset BIBREF23. It contains 123,287 images, each of which is paired 5 with descriptive sentences. We report the results and evaluate the image captioning model on the MSCOCO 2014 test set for image captioning. Following previous works BIBREF24, BIBREF25, we used the publicly-available splits provided by BIBREF26. The validation set and test set both contain 5,000 images.', 'Enwiki8 is large-scale dataset for character-level language modeling. It contains 100M bytes of unprocessed Wikipedia texts. The inputs include Latin alphabets, non-Latin alphabets, XML markups and special characters. The vocabulary size 205 tokens, including one for unknown characters. We used the same preprocessing method following BIBREF33. The training set contains 90M bytes of data, and the validation set and the test set contains 5M respectively.']\n",
            "['We construct a dataset containing 6,138 logical reasoning questions sourced from open websites and books. In the original problems, there are five answer options in which only one is right. To comply with fair use of law, we shuffle the order of answer options and randomly delete one of the wrong options for each data point, which results in four options with one right option and three wrong options. Furthermore, similar to ImageNet dataset, ReClor is available for non-commercial research purpose only. We are also hosting a public evaluation server on EvalAI BIBREF37 to benchmark progress on Reclor.']\n",
            "['A typical example of logical reasoning questions is shown in Table TABREF5. Similar to the format of multiple-choice reading comprehension datasets BIBREF10, BIBREF5, it contains a context, a question and four options with only one right answer. To answer the question in this example, readers need to identify the logical connections between the lines to pinpoint the conflict, then understand each of the options and select an option that solves the conflict. Human minds need extensive training and practice to get used to complex reasoning, and it will take immense efforts for crowdsourcing workers to design such logical reasoning questions. Inspired by the datasets extracted from standardized examinations BIBREF5, BIBREF12, we build a dataset by selecting such logical reasoning questions from standardized exams such as GMAT and LSAT . We finally collect 6,138 pieces of logical reasoning questions, which constitute a Reading Comprehension dataset requiring logical reasoning (ReClor).']\n",
            "['As mentioned earlier, biases prevalently exist in human-annotated datasets BIBREF16, BIBREF17, BIBREF18, BIBREF42, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner BIBREF43. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\\\small \\\\textsc {BASE}}$, XLNet$_{\\\\small \\\\textsc {BASE}}$ and RoBERTa$_{\\\\small \\\\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of $(25\\\\%)^{4}=0.39\\\\%$ to be guessed right consecutively for four times. Then we unite the sets of data points that are consistently predicted right by each model, because intuitively different models may learn different biases of the dataset. The above process is formulated as the following expression,']\n",
            "['Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11.']\n",
            "['Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11.']\n",
            "['The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck. For example, in a report of a football game, it cannot transform “1-3” to score if there are no keywords like “score” or “game” close to it.']\n",
            "['Text Normalization (TN) is a process to transform non-standard words (NSW) into spoken-form words (SFW) for disambiguation. In Text-To-Speech (TTS), text normalization is an essential procedure to normalize unreadable numbers, symbols or characters, such as transforming “$20” to “twenty dollars” and “@” to “at”, into words that can be used in speech synthesis. The surrounding context is the determinant for ambiguous cases in TN. For example, the context will decide whether to read “2019” as year or a number, and whether to read “10:30” as time or the score of a game. In Mandarin, some cases depend on language habit instead of rules- “2” can either be read as “r” or “ling” and “1” as “y” or “yo”.']\n",
            "['In this paper, we consider name variants from the perspective of a NER application and analyze an existing named entity-annotated tweet dataset in Turkish described in BIBREF5, in order to further annotate the included named entities with respect to a proprietary name variant categorization. The original dataset includes named annotations for eight types: PERSON, LOCATION, ORGANIZATION, DATE, TIME, MONEY, PERCENT, and MISC BIBREF5. However, in this study, we target only at the first three categories which amounts to a total of 980 annotations in 670 tweets in Turkish. We further annotate these 980 names with respect to a name variant categorization that we propose and try to present a rough estimate of the extent at which different named entity variants are used as named entities in Turkish tweets. The resulting annotations of named entities as different name variants are also made publicly available for research purposes. We believe that both the analysis described in the paper and the publicly-shared annotations (i.e., a tweet dataset annotated for name variants) will help improve research on NER, name disambiguation, and name linking on Turkish social media posts.']\n",
            "['In this study, we analyze the basic named entities (of type PERSON, LOCATION, and ORGANIZATION, henceforth, PLOs) in the annotated dataset compiled in BIBREF5, with respect to their being well-formed canonical names or name variants. The dataset includes a total of 1.322 named entity annotations, however, 980 of them are PLOs (457 PERSON, 282 LOCATION, and 241 ORGANIZATION names) and are the main focus of this paper. These 980 PLOs were annotated within a total of 670 tweets.']\n",
            "['In this study, we analyze the basic named entities (of type PERSON, LOCATION, and ORGANIZATION, henceforth, PLOs) in the annotated dataset compiled in BIBREF5, with respect to their being well-formed canonical names or name variants. The dataset includes a total of 1.322 named entity annotations, however, 980 of them are PLOs (457 PERSON, 282 LOCATION, and 241 ORGANIZATION names) and are the main focus of this paper. These 980 PLOs were annotated within a total of 670 tweets.']\n",
            "['In this paper, we consider name variants from the perspective of a NER application and analyze an existing named entity-annotated tweet dataset in Turkish described in BIBREF5, in order to further annotate the included named entities with respect to a proprietary name variant categorization. The original dataset includes named annotations for eight types: PERSON, LOCATION, ORGANIZATION, DATE, TIME, MONEY, PERCENT, and MISC BIBREF5. However, in this study, we target only at the first three categories which amounts to a total of 980 annotations in 670 tweets in Turkish. We further annotate these 980 names with respect to a name variant categorization that we propose and try to present a rough estimate of the extent at which different named entity variants are used as named entities in Turkish tweets. The resulting annotations of named entities as different name variants are also made publicly available for research purposes. We believe that both the analysis described in the paper and the publicly-shared annotations (i.e., a tweet dataset annotated for name variants) will help improve research on NER, name disambiguation, and name linking on Turkish social media posts.']\n",
            "['In this work, we investigate knowledge distillation in the context of neural machine translation. We note that NMT differs from previous work which has mainly explored non-recurrent models in the multi-class prediction setting. For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions. With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network. This sequence-level approximation leads to a simple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher network.']\n",
            "['The large sizes of neural machine translation systems make them an ideal candidate for knowledge distillation approaches. In this section we explore three different ways this technique can be applied to NMT.', 'Word-Level Knowledge Distillation', 'Sequence-Level Knowledge Distillation', 'Sequence-Level Interpolation']\n",
            "['To test out these approaches, we conduct two sets of NMT experiments: high resource (English INLINEFORM0 German) and low resource (Thai INLINEFORM1 English).', 'The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is INLINEFORM3 (which performed better than INLINEFORM4 , INLINEFORM5 models), and the student model is INLINEFORM6 . Other training details mirror Luong2015.']\n",
            "['The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is INLINEFORM3 (which performed better than INLINEFORM4 , INLINEFORM5 models), and the student model is INLINEFORM6 . Other training details mirror Luong2015.']\n",
            "['Table 1 summarizes the results on the test data along with the results from the literature. We can see that joint EL and RE performs better than the default pipelined approach, and outperforms most semantic parsing based models, except BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing. In addition, inference on unstructured data helps the default model. The joint EL and RE combined with inference on unstructured data further improves the default pipelined model by 9.2% (from 44.1% to 53.3%), and achieves a new state-of-the-art result beating the previous reported best result of yih-EtAl:2015:ACL-IJCNLP (with one-tailed t-test significance of $p < 0.05$ ).', 'We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction.']\n",
            "['fig:qaframework gives an overview of our method for the question “who did shaq first play for”. We have two main steps: (1) inference on Freebase (KB-QA box); and (2) further inference on Wikipedia (Answer Refinement box). Let us take a close look into step 1. Here we perform entity linking to identify a topic entity in the question and its possible Freebase entities. We employ a relation extractor to predict the potential Freebase relations that could exist between the entities in the question and the answer entities. Later we perform a joint inference step over the entity linking and relation extraction results to find the best entity-relation configuration which will produce a list of candidate answer entities. In the step 2, we refine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones.']\n",
            "['Table 1 summarizes the results on the test data along with the results from the literature. We can see that joint EL and RE performs better than the default pipelined approach, and outperforms most semantic parsing based models, except BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing. In addition, inference on unstructured data helps the default model. The joint EL and RE combined with inference on unstructured data further improves the default pipelined model by 9.2% (from 44.1% to 53.3%), and achieves a new state-of-the-art result beating the previous reported best result of yih-EtAl:2015:ACL-IJCNLP (with one-tailed t-test significance of $p < 0.05$ ).']\n",
            "['General Model', 'Given a type embedding vector INLINEFORM0 and a featurizer INLINEFORM1 that takes entity INLINEFORM2 and its context INLINEFORM3 , we employ the logistic regression (as shown in fig:arch) to model the probability of INLINEFORM4 assigned INLINEFORM5 (i.e., INLINEFORM6 ) DISPLAYFORM0', 'and we seek to learn a type embedding matrix INLINEFORM0 and a featurizer INLINEFORM1 such that DISPLAYFORM0']\n",
            "['The state-of-the-art approach BIBREF8 for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features BIBREF4 , BIBREF6 or distributed representations BIBREF9 , it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, ignoring their mutual connection; (2) the attention on context is computed solely upon the context, considering no alignment to the entity; (3) document-level contexts which could be useful in classification are not exploited; and (4) hand-crafted features heavily rely on system or human annotations.']\n",
            "['The state-of-the-art approach BIBREF8 for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context. These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold. While this approach outperforms previous approaches which only use sparse binary features BIBREF4 , BIBREF6 or distributed representations BIBREF9 , it has a few drawbacks: (1) the representations of left and right contexts are learnt independently, ignoring their mutual connection; (2) the attention on context is computed solely upon the context, considering no alignment to the entity; (3) document-level contexts which could be useful in classification are not exploited; and (4) hand-crafted features heavily rely on system or human annotations.']\n",
            "[\"We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .\"]\n",
            "[\"We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .\"]\n",
            "[\"We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .\"]\n",
            "['Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW BIBREF11, BIBREF28, BIBREF29. Its validation and test sets are taken from Turkcorpus BIBREF9, where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the source sentences while keeping as much meaning as possible. Hence, no sentence splitting, minimal structural simplification and little content reduction occurs in this test set BIBREF9.']\n",
            "['Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW BIBREF11, BIBREF28, BIBREF29. Its validation and test sets are taken from Turkcorpus BIBREF9, where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the source sentences while keeping as much meaning as possible. Hence, no sentence splitting, minimal structural simplification and little content reduction occurs in this test set BIBREF9.']\n",
            "['On the other hand SARI compares the predicted simplification with both the source and the target references. It is an average of F1 scores for three $n$-gram operations: additions, keeps and deletions. For each operation, these scores are then averaged for all $n$-gram orders (from 1 to 4) to get the overall F1 score.']\n",
            "['On the other hand SARI compares the predicted simplification with both the source and the target references. It is an average of F1 scores for three $n$-gram operations: additions, keeps and deletions. For each operation, these scores are then averaged for all $n$-gram orders (from 1 to 4) to get the overall F1 score.']\n",
            "['In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia BIBREF11, BIBREF12.', 'Phrase-Based MT system with candidate reranking. Dissimilar candidates are favored based on their Levenshtein distance to the source.', 'BIBREF33', 'Deep semantics sentence representation fed to a monolingual MT system.', 'Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by choosing relevant parameters; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-the-box Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI BIBREF9 on the WikiLarge benchmark BIBREF10, a +1.42 gain over previous scores, without requiring any external resource or modified training objective.', 'Syntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI.', 'Seq2Seq trained with reinforcement learning, combined with a lexical simplification model.', 'Lately, SS has mostly been tackled using Seq2Seq MT models BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.', 'Seq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks.', 'Standard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI.', 'BIBREF35', 'Seq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI.', 'Seq2Seq integrating the simple PPDB simplification database BIBREF36 as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words.']\n",
            "['The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.', 'Speech recordings of 88 PD patients and 88 HC speakers from Germany are considered BIBREF17. The participants performed four speech task: the rapid repetition of /pa-ta-ka/, 5 sentences, one text with 81 words, and a monologue.', 'A total of 100 native Czech speakers (50 PD, 50 HC) were considered BIBREF18. The speech tasks performed by the participants include the rapid repetition of the syllables /pa-ta-ka/, a read text with 80 words, and a monologue.']\n",
            "['The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.', 'Speech recordings of 88 PD patients and 88 HC speakers from Germany are considered BIBREF17. The participants performed four speech task: the rapid repetition of /pa-ta-ka/, 5 sentences, one text with 81 words, and a monologue.', 'A total of 100 native Czech speakers (50 PD, 50 HC) were considered BIBREF18. The speech tasks performed by the participants include the rapid repetition of the syllables /pa-ta-ka/, a read text with 80 words, and a monologue.']\n",
            "['We further investigate the performance gain from two different patient teacher designs: PKD-Last vs. PKD-Skip. Results of both PKD variants on the GLUE benchmark (with BERT$_6$ as the student) are summarized in Table TABREF23. Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last. Presumably, this might be due to the fact that distilling information across every $k$ layers captures more diverse representations of richer semantics from low-level to high-level, while focusing on the last $k$ layers tends to capture relatively homogeneous semantic information.']\n",
            "['We further investigate the performance gain from two different patient teacher designs: PKD-Last vs. PKD-Skip. Results of both PKD variants on the GLUE benchmark (with BERT$_6$ as the student) are summarized in Table TABREF23. Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last. Presumably, this might be due to the fact that distilling information across every $k$ layers captures more diverse representations of richer semantics from low-level to high-level, while focusing on the last $k$ layers tends to capture relatively homogeneous semantic information.']\n",
            "['We evaluate our proposed approach on Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension tasks. For Sentiment Classification, we test on Stanford Sentiment Treebank (SST-2) BIBREF3. For Paraphrase Similarity Matching, we use Microsoft Research Paraphrase Corpus (MRPC) BIBREF39 and Quora Question Pairs (QQP) datasets. For Natural Language Inference, we evaluate on Multi-Genre Natural Language Inference (MNLI) BIBREF4, QNLI BIBREF20, and Recognizing Textual Entailment (RTE).']\n",
            "[\"We evaluate the proposed approach on several NLP tasks, including Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension. Experiments on seven datasets across these four tasks demonstrate that the proposed Patient-KD approach achieves superior performance and better generalization than standard knowledge distillation methods BIBREF6, with significant gain in training efficiency and storage reduction while maintaining comparable model accuracy to original large models. To the authors' best knowledge, this is the first known effort for BERT model compression.\"]\n",
            "['We generate 100k examples to train on, and evaluate on 1000 held-out examples. We train our models for 200k steps, batch size of 32 and perform no model selection. The table below shows that the deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points.']\n",
            "['To demonstrate the utility of our annotated corpus, we explore two machine learning approaches for extracting actions and entities: a maximum entropy model and a neural network tagging model. We also present experiments for relation classification. We use the standard precision, recall and F INLINEFORM0 metrics to evaluate and compare the performance.']\n",
            "['To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 . The history of previously emitted characters is encapsulated in a recurrent state INLINEFORM2 : DISPLAYFORM0']\n",
            "[\"We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts. A term depending on the transcript length was proposed for both CTC BIBREF3 and seq2seq BIBREF11 networks, but its usage was reported to be difficult because beam search was looping over parts of the recording and additional constraints were needed BIBREF11 . To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than INLINEFORM0 : DISPLAYFORM0\", 'The coverage criterion prevents looping over the utterance because once the cumulative attention bypasses the threshold INLINEFORM0 a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost. In our implementation, the coverage is recomputed at each beam search iteration using all attention weights produced up to this step.', 'Label Smoothing Prevents Overconfidence', 'A elegant solution to model overconfidence was problem proposed for the Inception image recognition architecture BIBREF15 . For the purpose of computing the training cost the ground-truth label distribution is smoothed, with some fraction of the probability mass assigned to classes other than the correct one. This in turn prevents the model from learning to concentrate all probability mass on a single token. Additionally, the model receives more training signal because the error function cannot easily saturate.']\n",
            "['We record validation loss of the model checkpoints and plot them in Figure FIGREF47. Similar to the machine translation tasks, the learning rate warm-up stage can be removed for the Pre-LN model. The Pre-LN model can be trained faster. For example, the Post-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achieves similar validation loss at 700k updates, which suggests there is a 40% speed-up rate. Note that $T_{warmup}$ (10k) is far less than the acceleration (200k) which suggests the Pre-LN Transformer is easier to optimize using larger learning rates. We also evaluate different model checkpoints on the downstream task MRPC and RTE (more details can be found in the supplementary material). The experiments results are plotted in Figure FIGREF48 and FIGREF49. We can see that the Pre-LN model also converges faster on the downstream tasks.']\n",
            "['We record validation loss of the model checkpoints and plot them in Figure FIGREF47. Similar to the machine translation tasks, the learning rate warm-up stage can be removed for the Pre-LN model. The Pre-LN model can be trained faster. For example, the Post-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achieves similar validation loss at 700k updates, which suggests there is a 40% speed-up rate. Note that $T_{warmup}$ (10k) is far less than the acceleration (200k) which suggests the Pre-LN Transformer is easier to optimize using larger learning rates. We also evaluate different model checkpoints on the downstream task MRPC and RTE (more details can be found in the supplementary material). The experiments results are plotted in Figure FIGREF48 and FIGREF49. We can see that the Pre-LN model also converges faster on the downstream tasks.']\n",
            "['Experiments ::: Experiment Settings ::: Machine Translation', 'We conduct our experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. For the IWSLT14 De-En task, we use the same model configuration as in Section 3. For the WMT14 En-De task, we use the Transformer base setting. More details can be found in the supplementary material.', 'For training the Pre-LN Transformer, we remove the learning rate warm-up stage. On the IWSLT14 De-En task, we set the initial learning rate to be $5e^{-4}$ and decay the learning rate at the 8-th epoch by 0.1. On the WMT14 En-De task, we run two experiments in which the initial learning rates are set to be $7e^{-4}/1.5e^{-3}$ respectively. Both learning rates are decayed at the 6-th epoch followed by the inverse square root learning rate scheduler.', 'Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)', 'We follow BIBREF8 to use English Wikipedia corpus and BookCorpus for pre-training. As the dataset BookCorpus BIBREF40 is no longer freely distributed. We follow the suggestions from BIBREF8 to crawl and collect BookCorpus on our own. The concatenation of two datasets contains roughly 3.4B words in total, which is comparable with the data corpus used in BIBREF8. We randomly split documents into one training set and one validation set. The training-validation ratio for pre-training is 199:1.', 'We use base model configuration in our experiments. Similar to the translation task, we train the Pre-LN BERT without the warm-up stage and compare it with the Post-LN BERT. We follow the same hyper-parameter configuration in BIBREF8 to train the Post-LN BERT using 10k warm-up steps with $\\\\text{lr}_{max}=1e^{-4}$. For the Pre-LN BERT, we use linear learning rate decay starting from $3e^{-4}$ without the warm-up stage. We have tried to use a larger learning rate (such as $3e^{-4}$) for the Post-LN BERT but found the optimization diverged.']\n",
            "['We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.']\n",
            "['We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.', \"Recently, we release Baidu Speech Translation Corpus (BSTC) for open research . This dataset covers speeches in a wide range of domains, including IT, economy, culture, biology, arts, etc. We transcribe the talks carefully, and have professional translators to produce the English translations. This procedure is extremely difficult due to the large number of domain-specific terminologies, speech redundancies and speakers' accents. We expect that this dataset will help the researchers to develop robust NMT models on the speech translation. In summary, there are many features that distinguish this dataset to the previously related resources:\", 'The test dataset includes interpretations produced by simultaneous interpreters with professional experience. This dataset contributes an essential resource for the comparison between translation and interpretation.', 'We randomly extract several talks from the dataset, and divide them into the development and test set. In Table TABREF34 , we summarize the statistics of our dataset. The average number of utterances per talk is 152.6 in the training set, 59.75 in the dev set, and 162.5 in the test set.']\n",
            "['To evaluate model performance, we apply our trained models to rewrite the ill-formed questions in TEST and treat the well-formed question in each pair as the reference sentence. We then compute BLEU-4 BIBREF28, ROUGE-1, ROUGE-2, ROUGE-L BIBREF29, and METEOR BIBREF30 scores. As a baseline, we also evaluate the original ill-formed question using the automatic metrics.']\n",
            "['To evaluate model performance, we apply our trained models to rewrite the ill-formed questions in TEST and treat the well-formed question in each pair as the reference sentence. We then compute BLEU-4 BIBREF28, ROUGE-1, ROUGE-2, ROUGE-L BIBREF29, and METEOR BIBREF30 scores. As a baseline, we also evaluate the original ill-formed question using the automatic metrics.']\n",
            "['To understand the quality of the question rewriting examples in the MQR dataset, we ask human annotators to judge the quality of the questions in the DEV and TEST splits (abbreviated as DEVTEST onward). Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:', 'Is the question grammatically correct?', 'Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.', 'Is the question an explicit question, rather than a search query, a command, or a statement?', 'The annotators were asked to annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13. We consider all “How to” questions (“How to unlock GT90 in Gran Turismo 2?”) as grammatical. Although it is not a complete sentence, this kind of question is quite common in our dataset and therefore we choose to treat it as grammatically correct.']\n",
            "['To understand the quality of the question rewriting examples in the MQR dataset, we ask human annotators to judge the quality of the questions in the DEV and TEST splits (abbreviated as DEVTEST onward). Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:', 'Is the question grammatically correct?', 'Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.', 'Is the question an explicit question, rather than a search query, a command, or a statement?']\n",
            "['To understand the quality of the question rewriting examples in the MQR dataset, we ask human annotators to judge the quality of the questions in the DEV and TEST splits (abbreviated as DEVTEST onward). Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:', 'Is the question grammatically correct?', 'Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.', 'Is the question an explicit question, rather than a search query, a command, or a statement?', 'The annotators were asked to annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13. We consider all “How to” questions (“How to unlock GT90 in Gran Turismo 2?”) as grammatical. Although it is not a complete sentence, this kind of question is quite common in our dataset and therefore we choose to treat it as grammatically correct.']\n",
            "['The annotators were asked to annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13. We consider all “How to” questions (“How to unlock GT90 in Gran Turismo 2?”) as grammatical. Although it is not a complete sentence, this kind of question is quite common in our dataset and therefore we choose to treat it as grammatically correct.', 'FLOAT SELECTED: Table 2: Examples given to annotators for binary question quality scores.']\n",
            "['We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.']\n",
            "['The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.']\n",
            "['The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.']\n",
            "['This work is a first attempt towards using contemporary neural machine translation (NMT) techniques to perform machine translation for Nigerian Pidgin, establishing solid baselines that will ease and spur future work. We evaluate the performance of supervised and unsupervised neural machine translation models using word-level and the subword-level tokenization of BIBREF3.']\n",
            "['All baseline models were trained using the Transformer architecture of BIBREF7. We experiment with both word-level and Byte Pair Encoding (BPE) subword-level tokenization methods for the supervised models. We learned 4000 byte pair encoding tokens, following the findings of BIBREF5. For the unuspervised model, we experiment with only word-level tokenization.']\n",
            "['All baseline models were trained using the Transformer architecture of BIBREF7. We experiment with both word-level and Byte Pair Encoding (BPE) subword-level tokenization methods for the supervised models. We learned 4000 byte pair encoding tokens, following the findings of BIBREF5. For the unuspervised model, we experiment with only word-level tokenization.']\n",
            "['The supervised translation models seem to perform better at longer example translations than the unsupervised example.']\n",
            "['We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7. This could include not just a statement about factuality, but any free text that journalists decided to add, e.g., links to biographies or behavioral analysis of the opponents and moderators. We converted this to binary annotation about whether a particular sentence was annotated for factuality by a given source. Whenever one or more annotations were about part of a sentence, we selected the entire sentence, and when an annotation spanned over multiple sentences, we selected each of them.', 'Ultimately, we ended up with a dataset of four debates, with a total of 5,415 sentences. The agreement between the sources was low as Table TABREF8 shows: only one sentence was selected by all nine sources, 57 sentences by at least five, 197 by at least three, 388 by at least two, and 880 by at least one. The reason for this is that the different media aimed at annotating sentences according to their own editorial line, rather than trying to be exhaustive in any way. This suggests that the task of predicting which sentence would contain check-worthy claims will be challenging. Thus, below we focus on a ranking task rather than on absolute predictions. Moreover, we predict which sentence would be selected (i) by at least one of the media, or (ii) by a specific medium.']\n",
            "['Ultimately, we ended up with a dataset of four debates, with a total of 5,415 sentences. The agreement between the sources was low as Table TABREF8 shows: only one sentence was selected by all nine sources, 57 sentences by at least five, 197 by at least three, 388 by at least two, and 880 by at least one. The reason for this is that the different media aimed at annotating sentences according to their own editorial line, rather than trying to be exhaustive in any way. This suggests that the task of predicting which sentence would contain check-worthy claims will be challenging. Thus, below we focus on a ranking task rather than on absolute predictions. Moreover, we predict which sentence would be selected (i) by at least one of the media, or (ii) by a specific medium.']\n",
            "['We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.']\n",
            "['We model the problem as a ranking task, and we train both Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) obtaining state-of-the-art results. We also analyze the relevance of the specific feature groups and we show that modeling the context yields a significant boost in performance. Finally, we also analyze whether we can learn to predict which facts are check-worthy with respect to each of the individual media sources, thus capturing their biases. It is worth noting that while trained on political debates, many features of our model can be potentially applied to other kinds of information sources, e.g., interviews and news.']\n",
            "['The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.']\n",
            "['The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.', 'More importantly, both the SVM and the FNN versions of our system consistently outperform all three versions of ClaimBuster on all measures. This means that the extra information coded in our model, mainly more linguistic, structural, and contextual features, has an important contribution to the overall performance.']\n",
            "['We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7. This could include not just a statement about factuality, but any free text that journalists decided to add, e.g., links to biographies or behavioral analysis of the opponents and moderators. We converted this to binary annotation about whether a particular sentence was annotated for factuality by a given source. Whenever one or more annotations were about part of a sentence, we selected the entire sentence, and when an annotation spanned over multiple sentences, we selected each of them.']\n",
            "['New dataset: We build a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice-presidential debates, which we gathered from nine reputable sources such as CNN, NPR, and PolitiFact, and which we release to the research community.']\n",
            "['We run experiments on INLINEFORM0 , a 3 gigabyte English Wikipedia corpus, and train word2vec skipgram (W2V, BIBREF1 ) and fastText skipgram (FTX, BIBREF2 ) models on INLINEFORM1 and its derivatives. We randomly generate a permutation INLINEFORM2 on the alphabet and learn a transduction INLINEFORM3 (details below). In Table TABREF8 (left), the columns “method”, INLINEFORM4 and INLINEFORM5 indicate the method used (W2V or FTX) and whether experiments in a row were run on INLINEFORM6 , INLINEFORM7 or INLINEFORM8 . The values of “whitespace” are: (i) ORIGINAL (whitespace as in the original), (ii) SUBSTITUTE (what INLINEFORM9 outputs as whitespace is used as whitespace, i.e., INLINEFORM10 becomes the new whitespace) and (iii) RANDOM (random segmentation with parameters INLINEFORM11 , INLINEFORM12 , INLINEFORM13 ). Before random segmentation, whitespace is replaced with “@” – this character occurs rarely in INLINEFORM14 , so that the effect of conflating two characters (original “@” and whitespace) can be neglected. The random segmenter then indicates boundaries by whitespace – unambiguously since it is applied to text that contains no whitespace.']\n",
            "['Evaluation. We evaluate the three models on an entity typing task, similar to BIBREF14 , but based on an entity dataset released by xie16entitydesc2 in which each entity has been assigned one or more types from a set of 50 types. For example, the entity “Harrison Ford” has the types “actor”, “celebrity” and “award winner” among others. We extract mentions from FACC (http://lemurproject.org/clueweb12/FACC1) if an entity has a mention there or we use the Freebase name as the mention otherwise. This gives us a data set of 54,334, 6085 and 6747 mentions in train, dev and test, respectively. Each mention is annotated with the types that its entity has been assigned by xie16entitydesc2. The evaluation has a strong cross-domain aspect because of differences between FACC and Wikipedia, the training corpus for our representations. For example, of the 525 mentions in dev that have a length of at least 5 and do not contain lowercase characters, more than half have 0 or 1 occurrences in the Wikipedia corpus, including many like “JOHNNY CARSON” that are frequent in other case variants.']\n",
            "[\"This paper is the first in a series of works to build a framework for the audit of the demographic attributes of ImageNet and other large image datasets. The main contributions of this work include the introduction of a model-driven demographic annotation pipeline for apparent age and gender, analysis of said annotation models and the presentation of annotations for each image in the training set of the ILSVRC 2012 subset of ImageNet (1.28M images) and the `person' hierarchical synset of ImageNet (1.18M images).\"]\n",
            "[\"We evaluate the training set of the ILSVRC 2012 subset of ImageNet (1000 synsets) and the `person' hierarchical synset of ImageNet (2833 synsets) with the proposed methodology. Face detections that receive a confidence score of 0.9 or higher move forward to the annotation phase. Statistics for both datasets are presented in Tables TABREF7 and TABREF10 . In these preliminary annotations, we find that females comprise only 41.62% of images in ILSVRC and 31.11% in the `person' subset of ImageNet, and people over the age of 60 are almost non-existent in ILSVRC, accounting for 1.71%.\"]\n",
            "['Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where “Yedda+R” suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that “Yedda+R” has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The “Yedda+R” gives 16.47% time reduction in annotating 100 sentences.']\n",
            "['Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where “Yedda+R” suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that “Yedda+R” has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The “Yedda+R” gives 16.47% time reduction in annotating 100 sentences.']\n",
            "['Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where “Yedda+R” suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that “Yedda+R” has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The “Yedda+R” gives 16.47% time reduction in annotating 100 sentences.']\n",
            "['Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where “Yedda+R” suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that “Yedda+R” has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The “Yedda+R” gives 16.47% time reduction in annotating 100 sentences.']\n",
            "['Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 . Besides, many tools BIBREF6 , BIBREF4 require a complex system configuration on either local device or server, which is not friendly to new users.']\n",
            "['Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 . Besides, many tools BIBREF6 , BIBREF4 require a complex system configuration on either local device or server, which is not friendly to new users.']\n",
            "['We used three ontologies: (i) the Wine Ontology, one of the most commonly used examples of owl ontologies; (ii) the m-piro ontology, which describes a collection of museum exhibits, was originally developed in the m-piro project BIBREF27 , was later ported to owl, and accompanies Naturalowl BIBREF11 ; and (iii) the Disease Ontology, which describes diseases, including their symptoms, causes etc.']\n",
            "['We used three ontologies: (i) the Wine Ontology, one of the most commonly used examples of owl ontologies; (ii) the m-piro ontology, which describes a collection of museum exhibits, was originally developed in the m-piro project BIBREF27 , was later ported to owl, and accompanies Naturalowl BIBREF11 ; and (iii) the Disease Ontology, which describes diseases, including their symptoms, causes etc.']\n",
            "['To avoid variance in training and provide a fair comparison, 3 ensembles of each model were trained and evaluated on both dev and test sets. As shown in Table 5 , the DFN model has a 1.6% performance gain over the basic model (without DF and MR). This performance boost was contributed by both multi-step reasoning and dynamic fusion. When omitting DF or MR alone, the performance of DFN model dropped by 1.1% and 1.2%, respectively.']\n",
            "['In this paper, we find that a large part of the improvement also stems from a certain pruning of the data used to train the model. The KDG system generates its training data using an algorithm proposed by BIBREF3 . This algorithm applies a pruning step (discussed in Section SECREF3 ) to eliminate spurious training data entries. We find that without this pruning of the training data, accuracy of the KDG model drops to 36.3%. We consider this an important finding as the pruning step not only accounts for a large fraction of the improvement in the state-of-the-art KDG model but may also be relevant to training other models. In what follows, we briefly discuss the pruning algorithm, how we identified its importance for the KDG model, and its relevance to further work.']\n",
            "['Pruning algorithm', \"A key challenge in generating consistent logical forms is that many of them are spurious, i.e., they do not represent the question's meaning. For instance, a spurious logical form for the question “which country won the highest number of gold medals” would be one which simply selects the country in the first row of the table. This logical form leads to the correct answer only because countries in the table happen to be sorted in descending order.\", 'BIBREF3 propose a separate algorithm for pruning out spurious logical forms using fictitious tables. Specifically, for each question-table instance in the dataset, fictitious tables are generated, and answers are crowdsourced on them. A logical form that fails to obtain the correct answer on any fictitious table is filtered out. The paper presents an analysis over 300 questions revealing that the algorithm eliminated 92.1% of the spurious logical forms.']\n",
            "['In the experiments presented thus far we had at our disposal training sets with documents similar to the documents for which we inferred binary codes. One could ask a question, if it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we needed to hash documents that are not associated with any available domain-specific corpus? One solution could be to train the model with a big generic text corpus, that covers a wide variety of domains. BIBREF21 evaluated this approach for real-valued paragraph vectors, with promising results. It is not obvious, however, whether short binary codes would also perform well in similar settings. To shed light on this question we trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets. The results are presented in Table TABREF14 and in Figure FIGREF11 . The model trained on an unrelated text corpus gives lower retrieval precision than models with domain-specific training sets, which is not surprising. However, it still performs remarkably well, indicating that the semantics it captured can be useful for different text collections. Importantly, these results were obtained without domain-specific finetuning.']\n",
            "['To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.', 'In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.']\n",
            "['To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.']\n",
            "['The data set for the coding of death certificates is called the CépiDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate.']\n",
            "['The data set for the coding of death certificates is called the CépiDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate.']\n",
            "['In this paper, we describe our approach and present the results for our participation in the task 1, i.e. multilingual information extraction, of the CLEF eHealth 2018 challenge BIBREF0 . More precisely, this task consists in automatically coding death certificates using the International Classification of Diseases, 10th revision (ICD-10) BIBREF1 .']\n",
            "['In this paper, we describe our approach and present the results for our participation in the task 1, i.e. multilingual information extraction, of the CLEF eHealth 2018 challenge BIBREF0 . More precisely, this task consists in automatically coding death certificates using the International Classification of Diseases, 10th revision (ICD-10) BIBREF1 .']\n",
            "['We built Doc2Vec embeddings BIBREF27 on Swedish online data from 2018 crawled by Trendiction and manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative). Doc2Vec requires us to define a notion of source. For the data crawled by Trendiction, we take the source to be the domain name of the document, e.g. www.wikipedia.se, whereas for the manually scraped party platforms, we assign it the appropriate party identifier. The model was trained using the Gensim package BIBREF28 with embedding dimension 100 and a context window of size 8.']\n",
            "['We built Doc2Vec embeddings BIBREF27 on Swedish online data from 2018 crawled by Trendiction and manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative). Doc2Vec requires us to define a notion of source. For the data crawled by Trendiction, we take the source to be the domain name of the document, e.g. www.wikipedia.se, whereas for the manually scraped party platforms, we assign it the appropriate party identifier. The model was trained using the Gensim package BIBREF28 with embedding dimension 100 and a context window of size 8.']\n",
            "['First, lets compare the agreements in terms of two variants of INLINEFORM0 : INLINEFORM1 (interval) and INLINEFORM2 (nominal). The difference between the two measures is that INLINEFORM3 assigns four times higher cost to extreme disagreements (between the negative and positive classes) than INLINEFORM4 . A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans. The results in Table TABREF37 , column two, show that INLINEFORM5 always yields higher agreement than INLINEFORM6 , except for Spanish. We compute the average relative agreement gains by ignoring the Albanian and Spanish datasets (which have poor annotation quality), and Emojis (which are already subsumed by the 13 language datasets). We observe that the average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8 . This gives a strong indication that the sentiment classes are perceived as ordered by the annotators.']\n",
            "['First, lets compare the agreements in terms of two variants of INLINEFORM0 : INLINEFORM1 (interval) and INLINEFORM2 (nominal). The difference between the two measures is that INLINEFORM3 assigns four times higher cost to extreme disagreements (between the negative and positive classes) than INLINEFORM4 . A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans. The results in Table TABREF37 , column two, show that INLINEFORM5 always yields higher agreement than INLINEFORM6 , except for Spanish. We compute the average relative agreement gains by ignoring the Albanian and Spanish datasets (which have poor annotation quality), and Emojis (which are already subsumed by the 13 language datasets). We observe that the average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8 . This gives a strong indication that the sentiment classes are perceived as ordered by the annotators.']\n",
            "['(6) How many posts should be labeled with sentiment for training? We cannot provide conclusive answers here. It seems that 20,000 high-quality annotations already provide reasonable performance. The peak performance depends on the inter-annotator agreement and we estimate that around 100,000 annotations are needed. However, more important than sheer quantity is the quality, and domain- and topic-specific coverage of the posts, as demonstrated on several use-cases.']\n",
            "[\"A different pattern from the above can be observed in Fig FIGREF13 for the Polish dataset. After a slow improvement of the classifier's performance, the peak is reached at around 150,000 labeled tweets, and afterwards the performance remains stable and is even slightly decreasing. The maximum INLINEFORM0 is 0.536, close to the inter-annotator agreement of 0.571. At the same point, at 150,000 tweets, another performance measure, INLINEFORM1 , also peaks at its maximum value, even above the corresponding inter-annotator agreement. These results suggest that beyond a certain point, when the classifier's performance is “close enough” to the inter-annotator agreement, it does not pay off to further label tweets by sentiment. This is valid, however, only until a considerably new topic occurs.\"]\n",
            "[\"Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6 is a generalization of several specialized agreement measures. It works for any number of annotators, and is applicable to different variable types and metrics (e.g., nominal, ordered, interval, etc.). INLINEFORM1 is defined as follows: INLINEFORM2\"]\n",
            "['In general, the agreement can be estimated between any two methods of generating data. One of the main ideas of this work is to use the same measures to estimate the agreement between the human annotators as well as the agreement between the results of automated classification and the “gold standard”. There are different measures of agreement, and to get robust estimates we apply four well-known measures from the fields of inter-rater agreement and machine learning.', \"Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6 is a generalization of several specialized agreement measures. It works for any number of annotators, and is applicable to different variable types and metrics (e.g., nominal, ordered, interval, etc.). INLINEFORM1 is defined as follows: INLINEFORM2\", 'F score ( INLINEFORM0 ) is an instance of a well-known effectiveness measure in information retrieval BIBREF22 . We use an instance specifically designed to evaluate the 3-class sentiment classifiers BIBREF23 . INLINEFORM1 is defined as follows: INLINEFORM2', 'Accuracy ( INLINEFORM0 ) is a common, and the simplest, measure of performance of the model which measures the agreement between the model and the “gold standard”. INLINEFORM1 is defined in terms of the observed disagreement INLINEFORM2 : INLINEFORM3', 'Accuracy within 1 ( INLINEFORM0 ) is a special case of accuracy within n BIBREF24 . It assumes ordered classes and extends the range of predictions considered correct to the INLINEFORM1 neighbouring class values. In our case, INLINEFORM2 considers as incorrect only misclassifications from negative to positive and vice-versa: INLINEFORM3']\n",
            "['Fig FIGREF43 gives the results of the Friedman-Nemenyi test for the six classifiers trained in this study. We focus on two evaluation measures that take the ordering of sentiment classes into account: INLINEFORM0 and INLINEFORM1 . There are two classifiers which are in the group of top indistinguishable classifiers in both cases: ThreePlaneSVM (ranked 3rd) and TwoPlaneSVMbin (ranked 4th and 1st). We decided to interpret and discuss all the results in this paper using the TwoPlaneSVMbin classifier, since it is explicitly designed for ordered classes.']\n",
            "[\"We constructed and evaluated six different classification models for each labeled language dataset. The results for the application datasets are extracted from the original papers. Our classifiers are all based on Support Vector Machines (SVM) BIBREF12 , and for reference we also constructed a Naive Bayes classifier BIBREF13 . Detailed results are in the Classification models performance subsection in Methods. When comparing the classifiers' performance with the Friedman-Nemenyi test BIBREF14 , BIBREF15 , it turns out that there is no statistically significant difference between most of them (see the Friedman-Nemenyi test subsection in Methods). For subsequent analyses and comparisons, we selected the TwoPlaneSVMbin classifier that is always in the group of top classifiers according to two most relevant evaluation measures.\"]\n",
            "['We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match. This model only returns the top match, so only top-1 recall is evaluated, and top-3 recall is not applicable. Both neural models outperform the baseline, but by far the best performing model is BERT with fine-tuning, which retrieves the correct match for nearly 90% of queries (Table TABREF21).']\n",
            "['We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match. This model only returns the top match, so only top-1 recall is evaluated, and top-3 recall is not applicable. Both neural models outperform the baseline, but by far the best performing model is BERT with fine-tuning, which retrieves the correct match for nearly 90% of queries (Table TABREF21).']\n",
            "[\"The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent).\"]\n",
            "[\"The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent).\"]\n",
            "['We conduct our summarization experiments using the anonymized CNN/DailyMail corpus BIBREF20. We follow the data preprocessing steps in see2017get to obtain 287K training examples, 13K validation examples, and 11K test examples.', 'We use the US Petition dataset from BIBREF3. In total we have 1K petitions with over 12M signatures after removing petitions that have less than 150 signatures. We use the same train/dev/test split of 80/10/10 as subramanian2018content.']\n",
            "['We use the US Petition dataset from BIBREF3. In total we have 1K petitions with over 12M signatures after removing petitions that have less than 150 signatures. We use the same train/dev/test split of 80/10/10 as subramanian2018content.', 'We conduct our summarization experiments using the anonymized CNN/DailyMail corpus BIBREF20. We follow the data preprocessing steps in see2017get to obtain 287K training examples, 13K validation examples, and 11K test examples.', 'We re-implemented the RST Parser in PyTorch and were able to reproduce the results reported in the original paper. We train the parser on the same data (385 documents from the Wall Street Journal), based on the configuration recommended in the paper.']\n",
            "['Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20), we calculated the average length of generated summaries for PG$+$Cov and M1-latent, and found that they are of length 55.2 and 64.4 words respectively. This suggests that although discourse information helps the summarization model overall (based on consistent improvement in F1), the negative side effect is that the summaries tend to be longer and potentially more repetitive.', 'We present the test results in Table TABREF27. We tune the models based on the development set using MAE, and find that most converge after 8K–10K iterations of training. We are able to reproduce the performance of the baseline model (“CNN w/ GloVe”), and find that once again, adding the shallow discourse features improves results.']\n",
            "['The downstream tasks for evaluation include semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13 . After unsupervised training, the encoder is fixed, and applied as a representation extractor on the 10 tasks.']\n",
            "['The downstream tasks for evaluation include semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13 . After unsupervised training, the encoder is fixed, and applied as a representation extractor on the 10 tasks.']\n",
            "['To compare the effect of different corpora, we also trained two models on Amazon Book Review dataset (without ratings) which is the largest subset of the Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus.']\n",
            "['We evaluated the social impact of our system by interviewing individuals involved in government ($n=5$). We designed a discussion guide based on user experience research interview standards to speak with politicians in relevant jurisdictions BIBREF18. Participants had varying levels of prior awareness of the ParityBOT project. Our participants included 3 women candidates, each from a different major political party in the 2019 Alberta provincial election, and 2 men candidates at different levels of government representing Alberta areas. The full discussion guide for qualitative assessment is included in Appdx SECREF27. All participants provided informed consent to their anonymous feedback being included in this paper.']\n",
            "['We evaluated the social impact of our system by interviewing individuals involved in government ($n=5$). We designed a discussion guide based on user experience research interview standards to speak with politicians in relevant jurisdictions BIBREF18. Participants had varying levels of prior awareness of the ParityBOT project. Our participants included 3 women candidates, each from a different major political party in the 2019 Alberta provincial election, and 2 men candidates at different levels of government representing Alberta areas. The full discussion guide for qualitative assessment is included in Appdx SECREF27. All participants provided informed consent to their anonymous feedback being included in this paper.']\n",
            "['For validation, we found the most relevant features and set an abusive prediction threshold by using a dataset of 20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22. Each entry in our featurized dataset is composed of 24 features and a class label of hateful or not hateful. The dataset is shuffled and randomly split into training (80%) and testing (20%) sets matching the class balance ($25.4\\\\%$ hateful) of the full dataset. We use Adaptive Synthetic (ADASYN) sampling to resample and balance class proportions in the dataset BIBREF23.']\n",
            "['For validation, we found the most relevant features and set an abusive prediction threshold by using a dataset of 20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22. Each entry in our featurized dataset is composed of 24 features and a class label of hateful or not hateful. The dataset is shuffled and randomly split into training (80%) and testing (20%) sets matching the class balance ($25.4\\\\%$ hateful) of the full dataset. We use Adaptive Synthetic (ADASYN) sampling to resample and balance class proportions in the dataset BIBREF23.']\n",
            "['ParityBOT sent positivitweets composed by volunteers. These tweets expressed encouragement, stated facts about women in politics, and aimed to inspire and uplift the community. Volunteers submitted many of these positivitweets through an online form. Volunteers were not screened and anyone could access the positivitweet submission form. However, we mitigate the impact of trolls submitting hateful content, submitter bias, and ill-equipped submitters by reviewing, copy editing, and fact checking each tweet. Asking for community contribution in this way served to maximize limited copywriting resources and engage the community in the project.']\n",
            "['The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Perspective API uses machine learning models to score the perceived impact a tweet might have BIBREF10. The outputs from these models (i.e. 17 from Perspective, 3 from HateSonar, and 4 from VADER) are combined into a single feature vector for each tweet (see Appdx SECREF10). No user features are included in the tweet analysis models. While these features may improve classification accuracy they can also lead to potential bias BIBREF13.']\n",
            "['The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Perspective API uses machine learning models to score the perceived impact a tweet might have BIBREF10. The outputs from these models (i.e. 17 from Perspective, 3 from HateSonar, and 4 from VADER) are combined into a single feature vector for each tweet (see Appdx SECREF10). No user features are included in the tweet analysis models. While these features may improve classification accuracy they can also lead to potential bias BIBREF13.']\n",
            "['In BIBREF33 , significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \"minimal time lag\" was obtained in this way. In my opinion, another possible explanation is that a word in word sequence may more statistically depend on the following context than previous one. After all, a number of words are determined by its following words instead of previous ones in some natural languages. Take the articles in English as examples, indefinite article \"an\" is used when the first syllable of next word is a vowel while \"a\" is preposed before words starting with consonant. What\\'s more, if a noun is qualified by an attributive clause, definite article \"the\" should be used before the noun. These examples illustrate that words in a word sequence depends on their following words sometimes. To verify this hypothesis further, an experiment is performed here in which the word order of every input sentence is reversed, and the probability of word sequence INLINEFORM0 is evaluated as following: INLINEFORM1']\n",
            "['Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like: INLINEFORM0', 'where, INLINEFORM0 is the output of standard language model, INLINEFORM1 is the probability evaluated using caching, and INLINEFORM2 is a constant, INLINEFORM3 .']\n",
            "['Another type of caching has been proposed as a speed-up technique for RNNLMs BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 . The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In BIBREF32 , four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability INLINEFORM0 and word sequence INLINEFORM1 ; history INLINEFORM2 and its corresponding hidden state vector; history INLINEFORM3 and the denominator of the softmax function for classes; history INLINEFORM4 , class index INLINEFORM5 and the denominator of the softmax function for words. In BIBREF32 , around 50-fold speed-up was reported with this caching technique in speech recognition but, unfortunately, it only works for prediction and cannot be applied during training.']\n",
            "['Since this study focuses on NNLM itself and does not aim at raising a state of the art language model, the techniques of combining neural network language models with other kind of language models, like N-gram based language models, maximum entropy (ME) language models and etc., will not be included. The rest of this paper is organized as follows: In next section, the basic neural network language models - feed-forward neural network language model (FNNLM), recurrent neural network language model (RNNLM) and long-short term memory (LSTM) RNNLM, will be introduced, including the training and evaluation of these models. In the third section, the details of some important NNLM techniques, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), will be described, and experiments will be performed on them to examine their advantages and disadvantages separately. The limits of NNLM, mainly about the aspects of model architecture and knowledge representation, will be explored in the fourth section. A further work section will also be given to represent some further researches on NNLM. In last section, a conclusion about the findings in this paper will be made.']\n",
            "['The experiment results are showed in Table TABREF9 which suggest that, on a small corpus likes the Brown Corpus, RNNLM and LSTM-RNN did not show a remarkable advantage over FNNLM, instead a bit higher perplexity was achieved by LSTM-RNNLM. Maybe more data is needed to train RNNLM and LSTM-RNNLM because longer dependencies are taken into account by RNNLM and LSTM-RNNLM when predicting next word. LSTM-RNNLM with bias terms or direct connections was also evaluated here. When the direct connections between input layer and output layer of LSTM-RNN are enabled, a slightly higher perplexity but shorter training time were obtained. An explanation given for this phenomenon by BIBREF10 is that direct connections provide a bit more capacity and faster learning of the \"linear\" part of mapping from inputs to outputs but impose a negative effect on generalization. For bias terms, no significant improvement on performance was gained by adding bias terms which was also observed on RNNLM by BIBREF16 . In the rest of this paper, all studies will be performed on LSTM-RNNLM with neither direct connections nor bias terms, and the result of this model in Table TABREF9 will be used as the baseline for the rest studies.']\n",
            "['Various architectures of neural network language models are described and a number of improvement techniques are evaluated in this paper, but there are still something more should be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for addressing overfitting, character level neural network language model and ect. In addition, the experiments in this paper are all performed on Brown Corpus which is a small corpus, and different results may be obtained when the size of corpus becomes larger. Therefore, all the experiments in this paper should be repeated on a much larger corpus.']\n",
            "['Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations ([tab:sst2]Table tab:sst2). As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.']\n",
            "['We follow BIBREF18 by computing BLEU BIBREF19 between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 BIBREF20 and a random sample of 5000 sentences from TBC as references.', 'We also evaluate the perplexity of a trained language model on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model BIBREF21 pretrained on WikiText-103.', 'Following BIBREF22 , we compute self-BLEU: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.', 'We also evaluate the percentage of $n$ -grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique $n$ -grams implies higher BLEU.']\n",
            "['We follow BIBREF18 by computing BLEU BIBREF19 between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 BIBREF20 and a random sample of 5000 sentences from TBC as references.', 'We also evaluate the perplexity of a trained language model on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model BIBREF21 pretrained on WikiText-103.']\n",
            "[\"Table TABREF22 shows SA-WERs for two different diarization configurations and two different experiment setups. In the first setup, we assumed all attendees were invited to the meetings and therefore their face and voice signatures were available in advance. In the second setup, we used precomputed face and voice signatures for 50% of the attendees and the other speakers were treated as `guests'. A diarization system using only face identification and SSL may be regarded as a baseline as this approach was widely used in previous audio-visual diarization studies BIBREF33, BIBREF34, BIBREF35. The results show that the use of speaker identification substantially improved the speaker attribution accuracy. The SA-WERs were improved by 11.6% and 6.0% when the invited/guest ratios were 100/0 and 50/50, respectively. The small differences between the SA-WERs from Table TABREF22 and the WER from Table TABREF22 indicate very accurate speaker attribution.\"]\n",
            "['We also compare our method with several alternative VQA methods including (1) LSTM-Att BIBREF7 , a LSTM model with spatial attention; (2) MemAUG BIBREF33 : a memory-augmented model for VQA; (3) MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling; (4) MLAN BIBREF11 : an advanced multi-level attention model.']\n",
            "['In this section, we conduct extensive experiments to evaluate performance of our proposed model, and compare it with its variants and the alternative methods. We specifically implement the evaluation on a public benchmark dataset (Visual7W) BIBREF7 for the close-domain VQA task, and also generate numerous arbitrary question-answers pairs automatically to evaluate the performance on open-domain VQA. In this section, we first briefly review the dataset and the implementation details, and then report the performance of our proposed method comparing with several baseline models on both close-domain and open-domain VQA tasks.', 'We train and evaluate our model on a public available large-scale visual question answering datasets, the Visual7W dataset BIBREF7 , due to the diversity of question types. Besides, since there is no public available open-domain VQA dataset for evaluation now, we automatically build a collection of open-domain visual question-answer pairs to examine the potentiality of our model for answering open-domain visual questions.']\n",
            "['Given an image, we apply the Fast-RCNN BIBREF27 to detect the visual objects of the input image, and extract keywords of the corresponding questions with syntax analysis. Based on these information, we propose to learn a mechanism to retrieve the candidate knowledge by querying the large-scale knowledge graph, yielding a subgraph of relevant knowledge to facilitate the question answering. During the past years, a substantial amount of large-scale knowledge bases have been developed, which store common sense and factual knowledge in a machine readable fashion. In general, each piece of structured knowledge is represented as a triple $(subject, rel, object)$ with $subject$ and $object$ being two entities or concepts, and $rel$ corresponding to the specific relationship between them. In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.']\n",
            "['Fusion', 'We now propose a new method seeking to take advantage of both previously described ones. It is based on the assumption that the content- and graph-based features convey different information. Therefore, they could be complementary, and their combination could improve the classification performance. We experiment with three different fusion strategies, which are represented in the right-hand part of Figure FIGREF1 .', 'The first strategy follows the principle of Early Fusion. It consists in constituting a global feature set containing all content- and graph-based features from Sections SECREF2 and SECREF3 , then training a SVM directly using these features. The rationale here is that the classifier has access to the whole raw data, and must determine which part is relevant to the problem at hand.', 'The second strategy is Late Fusion, and we proceed in two steps. First, we apply separately both methods described in Sections SECREF2 and SECREF3 , in order to obtain two scores corresponding to the output probability of each message to be abusive given by the content- and graph-based methods, respectively. Second, we fetch these two scores to a third SVM, trained to determine if a message is abusive or not. This approach relies on the assumption that these scores contain all the information the final classifier needs, and not the noise present in the raw features.', 'Finally, the third fusion strategy can be considered as Hybrid Fusion, as it seeks to combine both previous proposed ones. We create a feature set containing the content- and graph-based features, like with Early Fusion, but also both scores used in Late Fusion. This whole set is used to train a new SVM. The idea is to check whether the scores do not convey certain useful information present in the raw features, in which case combining scores and features should lead to better results.']\n",
            "['We now propose a new method seeking to take advantage of both previously described ones. It is based on the assumption that the content- and graph-based features convey different information. Therefore, they could be complementary, and their combination could improve the classification performance. We experiment with three different fusion strategies, which are represented in the right-hand part of Figure FIGREF1 .', 'The first strategy follows the principle of Early Fusion. It consists in constituting a global feature set containing all content- and graph-based features from Sections SECREF2 and SECREF3 , then training a SVM directly using these features. The rationale here is that the classifier has access to the whole raw data, and must determine which part is relevant to the problem at hand.', 'The second strategy is Late Fusion, and we proceed in two steps. First, we apply separately both methods described in Sections SECREF2 and SECREF3 , in order to obtain two scores corresponding to the output probability of each message to be abusive given by the content- and graph-based methods, respectively. Second, we fetch these two scores to a third SVM, trained to determine if a message is abusive or not. This approach relies on the assumption that these scores contain all the information the final classifier needs, and not the noise present in the raw features.', 'Finally, the third fusion strategy can be considered as Hybrid Fusion, as it seeks to combine both previous proposed ones. We create a feature set containing the content- and graph-based features, like with Early Fusion, but also both scores used in Late Fusion. This whole set is used to train a new SVM. The idea is to check whether the scores do not convey certain useful information present in the raw features, in which case combining scores and features should lead to better results.']\n",
            "['The supervision is described under each task, e.g., there are three (conflicting) sources for the Intent task. A task requires labels at the appropriate granularity (singleton, sequence, or set) and type (multiclass or bitvector). The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes.']\n",
            "[\"(3) Weak Supervision Applications have access to supervision of varying quality and combining this contradictory and incomplete supervision is a major challenge. Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0, which have studied how to combine supervision in theory and in software. Here, we describe two novel observations from building production applications: (1) we describe the shift to applications which are constructed almost entirely with weakly supervised data due to cost, privacy, and cold-start issues, and (2) we observe that weak supervision may obviate the need for popular methods like transfer learning from massive pretrained models, e.g., BERT BIBREF8–on some production workloads, which suggests that a deeper trade-off study may be illuminating.\"]\n",
            "[\"Overton provides the engineer with abstractions that allow them to build, maintain, and monitor their application by manipulating data files–not custom code. Inspired by relational systems, supervision (data) is managed separately from the model (schema). Akin to traditional logical independence, Overton's schema provides model independence: serving code does not change even when inputs, parameters, or resources of the model change. The schema changes very infrequently–many production services have not updated their schema in over a year.\", 'In retrospect, the following three choices of Overton were the most important in meeting the above challenges.', '(1) Code-free Deep Learning In Overton-based systems, engineers focus exclusively on fine-grained monitoring of their application quality and improving supervision–not tweaking deep learning models. An Overton engineer does not write any deep learning code in frameworks like TensorFlow. To support application quality improvement, we use a technique, called model slicing BIBREF3. The main idea is to allow the developer to identify fine-grained subsets of the input that are important to the product, e.g., queries about nutrition or queries that require sophisticated disambiguation. The system uses developer-defined slices as a guide to increase representation capacity. Using this recently developed technique led to state-of-the-art results on natural language benchmarks including GLUE and SuperGLUE BIBREF4.', \"(2) Multitask Learning Overton was built to natively support multitask learning BIBREF5, BIBREF6, BIBREF7 so that all model tasks are concurrently predicted. A key benefit is that Overton can accept supervision at whatever granularity (for whatever task) is available. Overton models often perform ancillary tasks like part-of-speech tagging or typing. Intuitively, if a representation has captured the semantics of a query, then it should reliably perform these ancillary tasks. Typically, ancillary tasks are also chosen either to be inexpensive to supervise. Ancillary task also allow developers to gain confidence in the model's predictions and have proved to be helpful for aids for debugging errors.\", \"(3) Weak Supervision Applications have access to supervision of varying quality and combining this contradictory and incomplete supervision is a major challenge. Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0, which have studied how to combine supervision in theory and in software. Here, we describe two novel observations from building production applications: (1) we describe the shift to applications which are constructed almost entirely with weakly supervised data due to cost, privacy, and cold-start issues, and (2) we observe that weak supervision may obviate the need for popular methods like transfer learning from massive pretrained models, e.g., BERT BIBREF8–on some production workloads, which suggests that a deeper trade-off study may be illuminating.\"]\n",
            "['Overton takes as input a schema whose design goal is to support rich applications from modeling to automatic deployment. In more detail, the schema has two elements: (1) data payloads similar to a relational schema, which describe the input data, and (2) model tasks, which describe the tasks that need to be accomplished. The schema defines the input, output, and coarse-grained data flow of a deep learning model. Informally, the schema defines what the model computes but not how the model computes it: Overton does not prescribe architectural details of the underlying model (e.g., Overton is free to embed sentences using an LSTM or a Transformer) or hyperparameters, like hidden state size. Additionally, sources of supervision are described as data–not in the schema–so they are free to rapidly evolve.']\n",
            "['The schema is changed infrequently, and many engineers who use Overton simply select an existing schema. Applications are customized by providing supervision in a data file that conforms to the schema, described next.']\n",
            "[\"Overton provides the engineer with abstractions that allow them to build, maintain, and monitor their application by manipulating data files–not custom code. Inspired by relational systems, supervision (data) is managed separately from the model (schema). Akin to traditional logical independence, Overton's schema provides model independence: serving code does not change even when inputs, parameters, or resources of the model change. The schema changes very infrequently–many production services have not updated their schema in over a year.\"]\n",
            "['We evaluate configurations of our proposed architecture across three measures. The first is a modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014. It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release. Eight of the fifteen slots are used in evaluation. Similarly, the second evaluation measure we present is standard precision, recall, and F INLINEFORM1 , specifically for null values.', 'We also evaluate the models using mean reciprocal rank (MRR). When calculating the F INLINEFORM0 -based evaluation measure we decode the model by taking the single highest-scoring value for each slot. However, this does not necessarily reflect the quality of the overall value ranking produced by the model. For this reason we include MRR, defined as: DISPLAYFORM0']\n",
            "['We evaluate configurations of our proposed architecture across three measures. The first is a modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014. It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release. Eight of the fifteen slots are used in evaluation. Similarly, the second evaluation measure we present is standard precision, recall, and F INLINEFORM1 , specifically for null values.', 'We also evaluate the models using mean reciprocal rank (MRR). When calculating the F INLINEFORM0 -based evaluation measure we decode the model by taking the single highest-scoring value for each slot. However, this does not necessarily reflect the quality of the overall value ranking produced by the model. For this reason we include MRR, defined as: DISPLAYFORM0']\n",
            "['We evaluate on four categories of architecture:', 'reschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:', 'Reschke CRF: a conditional random field model.', 'Reschke Noisy-OR: a sequence tagger with a \"Noisy-OR\" form of aggregation that discourages the model from predicting the same value for multiple slots.', 'Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN BIBREF17 , a learning-to-search framework.']\n",
            "['reschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:', 'Reschke CRF: a conditional random field model.', 'Reschke Noisy-OR: a sequence tagger with a \"Noisy-OR\" form of aggregation that discourages the model from predicting the same value for multiple slots.', 'Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN BIBREF17 , a learning-to-search framework.']\n",
            "['The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles. Of these events, 40 are reserved for training, and 40 for testing, with the average cluster containing more than 2,000 mentions. Gold labels for each cluster are derived from Wikipedia infoboxes and cover up to 15 slots, of which 8 are used in evaluation (Figure TABREF54 ).']\n",
            "['The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles. Of these events, 40 are reserved for training, and 40 for testing, with the average cluster containing more than 2,000 mentions. Gold labels for each cluster are derived from Wikipedia infoboxes and cover up to 15 slots, of which 8 are used in evaluation (Figure TABREF54 ).']\n",
            "['The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles. Of these events, 40 are reserved for training, and 40 for testing, with the average cluster containing more than 2,000 mentions. Gold labels for each cluster are derived from Wikipedia infoboxes and cover up to 15 slots, of which 8 are used in evaluation (Figure TABREF54 ).']\n",
            "['The performances of our final model and other baseline models are illustrated in Table TABREF34. In the baseline unsupervised NMT models, subword-level NMT outperforms word-level NMT by around a 1.5 BLEU score. Although the unsupervised PBSMT model is worse than the subword-level NMT model, leveraging generated pseudo-parallel data from the PBSMT model to fine-tune the subword-level NMT model can still boost its performance. However, this pseudo-parallel data from the PBSMT model can not improve the word-level NMT model since the large percentage of OOV words limits its performance. After applying unknown words replacement to the word-level NMT model, the performance improves by a BLEU score of around 2. Using the Czech language model to re-score helps the model improve by around a 0.3 BLEU score each time. We also use this language model to create an ensemble of the best word-level and subword-level NMT model and achieve the best performance.', 'FLOAT SELECTED: Table 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our approach. Except the result that is listed on the last line, all results are under the condition that the translations are post-processed without patch-up.']\n",
            "['The quotes are fixed to keep them the same as the source sentences.', 'For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses BIBREF22 is applied to convert the translations to the real cases.', 'From our observation, the ensemble NMT model lacks the ability to translate name entities correctly. We find that words with capital characters are named entities, and those named entities in the source language may have the same form in the target language. Hence, we capture and copy these entities at the end of the translation if they does not exist in our translation.']\n",
            "['The performances of our final model and other baseline models are illustrated in Table TABREF34. In the baseline unsupervised NMT models, subword-level NMT outperforms word-level NMT by around a 1.5 BLEU score. Although the unsupervised PBSMT model is worse than the subword-level NMT model, leveraging generated pseudo-parallel data from the PBSMT model to fine-tune the subword-level NMT model can still boost its performance. However, this pseudo-parallel data from the PBSMT model can not improve the word-level NMT model since the large percentage of OOV words limits its performance. After applying unknown words replacement to the word-level NMT model, the performance improves by a BLEU score of around 2. Using the Czech language model to re-score helps the model improve by around a 0.3 BLEU score each time. We also use this language model to create an ensemble of the best word-level and subword-level NMT model and achieve the best performance.']\n",
            "['We test several state-of-the-art (BERT-based) models and find that they are still substantially (20%) below human performance. Our contributions are thus (1) the dataset, containing 3864 richly annotated questions plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions.']\n",
            "['We test several state-of-the-art (BERT-based) models and find that they are still substantially (20%) below human performance. Our contributions are thus (1) the dataset, containing 3864 richly annotated questions plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions.']\n",
            "['The QuaRTz task is to answer the questions given the corpus $K$ of qualitative background knowledge. We also examine a “no knowledge” (questions only) task and a “perfect knowledge” task (each question paired with the qualitative sentence $K_i$ it was based on). We report results using two baselines and several strong models built with BERT-large BIBREF9 as follows:', '1. Random: always 50% (2-way MC).', '2. BERT-Sci: BERT fine-tuned on a large, general set of science questions BIBREF5.', '3. BERT (IR): This model performs the full task. First, a sentence $K_i$ is retrieved from $K$ using $Q_i$ as a search query. This is then supplied to BERT as [CLS] $K_i$ [SEP] question-stem [SEP] answer-option [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using QuaRTz (only).', '4. BERT (IR upper bound): Same, but using the ideal (annotated) $K_i$ rather than retrieved $K_i$.', '5. BERT-PFT (no knowledge): BERT first fine-tuned (“pre-fine-tuned”) on the RACE dataset BIBREF10, BIBREF11, and then fine-tuned on QuaRTz (questions only, no $K$, both train and test). Questions are supplied as [CLS] question-stem [SEP] answer-option [SEP].', '6. BERT-PFT (IR): Same as BERT (IR), except starting with the pre-fine-tuned BERT.']\n",
            "['1. The dataset is hard. Our best model, BERT-PFT (IR), scores only 73.7, over 20 points behind human performance (95.0), suggesting there are significant linguistic and semantic challenges to overcome (Section SECREF7).']\n",
            "['QuaRTz was constructed as follows. First, 400 sentences expressing general qualitative relations were manually extracted by the authors from a large corpus using keyword search (“increase”, “faster”, etc.). Examples ($K_i$) are in Table TABREF3.', 'Second, crowdworkers were shown a seed sentence $K_i$, and asked to annotate the two properties being compared using the template below, illustrated using $K_2$ from Table TABREF3:', '[vskip=0mm,leftmargin=3mm]', '\"The smaller its mass is, the greater its acceleration for a given amount of force.\"', 'They were then asked to author a situated, 2-way multiple-choice (MC) question that tested this relationship, guided by multiple illustrations. Examples of their questions ($Q_i$) are in Table TABREF3.', 'Third, a second set of workers was shown an authored question, asked to validate its answer and quality, and asked to annotate how the properties of $K_i$ identified earlier were expressed. To do this, they filled a second template, illustrated for $Q_2$:', 'Finally these workers were asked to generate a new question by “flipping” the original so the answer switched. Flipping means inverting comparatives (e.g., “more” $\\\\rightarrow $ “less”), values, and other edits as needed to change the answer, e.g.,']\n",
            "['Second, crowdworkers were shown a seed sentence $K_i$, and asked to annotate the two properties being compared using the template below, illustrated using $K_2$ from Table TABREF3:', '[vskip=0mm,leftmargin=3mm]', '\"The smaller its mass is, the greater its acceleration for a given amount of force.\"']\n",
            "['One of the challenges for model development in this task is the limited amount of training data. To use the data efficiently, we developed the Span-Attribute Tagging (SAT) model which performs inference in a hierarchical manner by identifying the clinically relevant span using a BIO scheme and classifying the spans into specific labels BIBREF19. The span is represented using the latent representation from a bidirectional encoder, and as such has the capacity to capture the relevant context information. This latent contextual representation is used to infer the entity label and the status of the symptom as experienced or not. This is a trainable mechanism to infer status in contrast to ad hoc methods applied in negation tasks in previous work in clinical domain BIBREF20.']\n",
            "['This paper describes a novel task for extracting clinical concepts from provider-patient conversations. We describe in detail the ontologies and the annotation guidelines for developing a corpus. Using this corpus, we trained a state-of-the-art Span-Attribute Tagging (SAT) model and report results that highlight the relative difficulties of the different tasks. Further through human error analyses of the errors, we provide insights into the weakness of the current models and opportunities to improve it. Our experiments and analyses demonstrate that several entities such as medications, symptoms, conditions and certain attributes can be extracted with sufficiently high accuracy to support practical applications and we hope our results will spur further research on this important topic.']\n",
            "['Success Rate As shown in Table 2, the SL model achieves the lowest task success rate. Model trained with SL on dialog corpus has limited capabilities in capturing the change in state, and thus may not be able to generalize well to unseen dialog situations during simulation. RL efficiently improves the dialog task success rate, as it enables the dialog agent to explore strategies that are not in the training corpus. The agent-update-only models using REINFORCE and A2C achieve similar results, outperforming the baseline model by 14.9% and 15.3% respectively. The jointly optimized models improved the performance further over the agent-update-only models. Model using A2C for joint policy optimization achieves the best task success rate.']\n",
            "['Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn. At the INLINEFORM0 th turn of a dialog, the dialog agent takes in (1) the previous agent output encoding INLINEFORM1 , (2) the user input encoding INLINEFORM2 , (3) the retrieved KB result encoding INLINEFORM3 , and updates its internal state conditioning on the previous agent state INLINEFORM4 . With the updated agent state INLINEFORM5 , the dialog agent emits (1) a system action INLINEFORM6 , (2) an estimation of the belief state, and (3) a pointer to an entity in the retrieved query results. These outputs are then passed to an NLG module to generate the final agent response.', 'Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance.']\n",
            "['Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn. At the INLINEFORM0 th turn of a dialog, the dialog agent takes in (1) the previous agent output encoding INLINEFORM1 , (2) the user input encoding INLINEFORM2 , (3) the retrieved KB result encoding INLINEFORM3 , and updates its internal state conditioning on the previous agent state INLINEFORM4 . With the updated agent state INLINEFORM5 , the dialog agent emits (1) a system action INLINEFORM6 , (2) an estimation of the belief state, and (3) a pointer to an entity in the retrieved query results. These outputs are then passed to an NLG module to generate the final agent response.', 'Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance.']\n",
            "[\"Four different language pairs have been selected for the experiments. The datasets' size varies from tens of thousands to millions of sentences to test the regularizers' ability to improve translation over a range of low-resource and high-resource language pairs.\", 'De-En: The German-English dataset (de-en) has been taken from the WMT18 news translation shared task. The training set contains over 5M sentence pairs collected from the Europarl, CommonCrawl and Newscommentary parallel corpora. As validation and test sets, we have used the newstest2017 and the newstest2018 datasets, respectively. We consider this dataset as a high-resource case.', 'En-Fr: The English-French dataset (en-fr) has been sourced from the IWSLT 2016 translation shared task. This corpus contains translations of TED talks of very diverse topics. The training data provided by the organizers consist of $219,777$ translations which allow us to categorize this dataset as low/medium-resource. Following Denkowski and Neubig BIBREF41, the validation set has been formed by merging the 2013 and 2014 test sets from the same shared task, and the test set has been formed with the 2015 and 2016 test sets.', 'Cs-En: The Czech-English dataset (cs-en) is also from the IWSLT 2016 TED talks translation task. However, this dataset is approximately half the size of en-fr as its training set consists of $114,243$ sentence pairs. Again following Denkowski and Neubig BIBREF41), the validation set has been formed by merging the 2012 and 2013 test sets, and the test set by merging the 2015 and 2016 test sets. We regard this dataset as low-resource.', 'Eu-En: The Basque-English dataset (eu-en) has been collected from the WMT16 IT-domain translation shared task. This is the smallest dataset, with only $89,413$ sentence pairs in the training set. However, only $2,000$ sentences in the training set have been translated by human annotators. The remaining sentence pairs are translations of IT-domain short phrases and Wikipedia titles. Therefore, we consider this dataset as extremely low-resource. It must be said that translations in the IT domain are somehow easier than in the news domain, as this domain is very specific and the wording of the sentences are less varied. For this dataset, we have used the validation and test sets ($1,000$ sentences each) provided in the shared task.']\n",
            "['Experiments ::: Datasets', \"Four different language pairs have been selected for the experiments. The datasets' size varies from tens of thousands to millions of sentences to test the regularizers' ability to improve translation over a range of low-resource and high-resource language pairs.\", 'De-En: The German-English dataset (de-en) has been taken from the WMT18 news translation shared task. The training set contains over 5M sentence pairs collected from the Europarl, CommonCrawl and Newscommentary parallel corpora. As validation and test sets, we have used the newstest2017 and the newstest2018 datasets, respectively. We consider this dataset as a high-resource case.', 'En-Fr: The English-French dataset (en-fr) has been sourced from the IWSLT 2016 translation shared task. This corpus contains translations of TED talks of very diverse topics. The training data provided by the organizers consist of $219,777$ translations which allow us to categorize this dataset as low/medium-resource. Following Denkowski and Neubig BIBREF41, the validation set has been formed by merging the 2013 and 2014 test sets from the same shared task, and the test set has been formed with the 2015 and 2016 test sets.', 'Cs-En: The Czech-English dataset (cs-en) is also from the IWSLT 2016 TED talks translation task. However, this dataset is approximately half the size of en-fr as its training set consists of $114,243$ sentence pairs. Again following Denkowski and Neubig BIBREF41), the validation set has been formed by merging the 2012 and 2013 test sets, and the test set by merging the 2015 and 2016 test sets. We regard this dataset as low-resource.', 'Eu-En: The Basque-English dataset (eu-en) has been collected from the WMT16 IT-domain translation shared task. This is the smallest dataset, with only $89,413$ sentence pairs in the training set. However, only $2,000$ sentences in the training set have been translated by human annotators. The remaining sentence pairs are translations of IT-domain short phrases and Wikipedia titles. Therefore, we consider this dataset as extremely low-resource. It must be said that translations in the IT domain are somehow easier than in the news domain, as this domain is very specific and the wording of the sentences are less varied. For this dataset, we have used the validation and test sets ($1,000$ sentences each) provided in the shared task.']\n",
            "['As our language model we used ULMFiT BIBREF21. ULMFiT is an NLP transfer learning algorithm that we picked due to its straightforward implementation in the fast.ai library, and its promising results on small datasets. As the basis of our ULMFiT model we built a Swedish language model on a large corpus of Swedish text retrieved from the Swedish Wikipedia and the aforementioned forums Flashback and Familjeliv. We then used our annotated samples to fine-tune the language model, resulting in a classifier for the Big Five factors.']\n",
            "['As our language model we used ULMFiT BIBREF21. ULMFiT is an NLP transfer learning algorithm that we picked due to its straightforward implementation in the fast.ai library, and its promising results on small datasets. As the basis of our ULMFiT model we built a Swedish language model on a large corpus of Swedish text retrieved from the Swedish Wikipedia and the aforementioned forums Flashback and Familjeliv. We then used our annotated samples to fine-tune the language model, resulting in a classifier for the Big Five factors.']\n",
            "['Several regression models were tested from the scikit-learn framework BIBREF20, such as RandomForestRegressor, LinearSVR, and KNeighborsRegressor. The Support Vector Machine Regression yielded the lowest MAE and MSE while performing a cross validated grid search for all the models and a range of hyperparameters.']\n",
            "['Several regression models were tested from the scikit-learn framework BIBREF20, such as RandomForestRegressor, LinearSVR, and KNeighborsRegressor. The Support Vector Machine Regression yielded the lowest MAE and MSE while performing a cross validated grid search for all the models and a range of hyperparameters.']\n",
            "['Throughout our experiments we use only one training algorithm (subsection sec:data), and two performance measures (subsection sec:measures). During training, the performance of the trained model can be estimated only on the in-sample data. However, there are different estimation procedures which yield these approximations. In machine learning, a standard procedure is cross-validation, while for time-ordered data, sequential validation is typically used. In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc). The goal is to find the in-sample estimation procedure that best approximates the out-of-sample gold standard. The error an estimation procedure makes is defined as the difference to the gold standard.', 'In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:', 'seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,', 'seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,', 'seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.', \"The Twitter data shares some characteristics of time series and some of static data. A time series is an array of observations at regular or equidistant time points, and the observations are in general dependent on previous observations BIBREF0 . On the other hand, Twitter data is time-ordered, but the observations are short texts posted by Twitter users at any time and frequency. It can be assumed that original Twitter posts are not directly dependent on previous posts. However, there is a potential indirect dependence, demonstrated in important trends and events, through influential users and communities, or individual user's habits. These long-term topic drifts are typically not taken into account by the sentiment analysis models.\"]\n",
            "['Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc. In this paper we focus on sentiment classification of Twitter data. Construction of sentiment classifiers is a standard text mining task, but here we address the question of how to properly evaluate them as there is no settled way to do so. Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data. The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters. We collected a large set of 1.5 million tweets in 13 European languages. We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations. The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set). We find no significant difference between the best cross-validation and sequential validation. However, we observe that all cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it. Standard cross-validation with random selection of examples is significantly worse than the blocked cross-validation, and should not be used to evaluate classifiers in time-ordered data scenarios.', 'In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:', 'seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,', 'seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,', 'seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.']\n",
            "['First, we apply 10-fold cross-validation where the training:test set ratio is always 9:1. Cross-validation is stratified when the fold partitioning is not completely random, but each fold has roughly the same class distribution. We also compare standard random selection of examples to the blocked form of cross-validation BIBREF16 , BIBREF11 , where each fold is a block of consecutive tweets. We use the following abbreviations for cross-validations:', 'xval(9:1, strat, block) - 10-fold, stratified, blocked;', 'xval(9:1, no-strat, block) - 10-fold, not stratified, blocked;', 'xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.']\n",
            "['First, we apply 10-fold cross-validation where the training:test set ratio is always 9:1. Cross-validation is stratified when the fold partitioning is not completely random, but each fold has roughly the same class distribution. We also compare standard random selection of examples to the blocked form of cross-validation BIBREF16 , BIBREF11 , where each fold is a block of consecutive tweets. We use the following abbreviations for cross-validations:', 'xval(9:1, strat, block) - 10-fold, stratified, blocked;', 'xval(9:1, no-strat, block) - 10-fold, not stratified, blocked;', 'xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.']\n",
            "['The complexity of Twitter data raises some challenges on how to perform such estimations, as, to the best of our knowledge, there is currently no settled approach to this. Sentiment classes are typically ordered and unbalanced, and the data itself is time-ordered. Taking these properties into account is important for the selection of appropriate estimation procedures.']\n",
            "['In the paper we address the task of sentiment analysis of Twitter data. The task encompasses identification and categorization of opinions (e.g., negative, neutral, or positive) written in quasi-natural language used in Twitter posts. We focus on estimation procedures of the predictive performance of machine learning models used to address this task. Performance estimation procedures are key to understand the generalization ability of the models since they present approximations of how these models will behave on unseen data. In the particular case of sentiment analysis of Twitter data, high volumes of content are continuously being generated and there is no immediate feedback about the true class of instances. In this context, it is fundamental to adopt appropriate estimation procedures in order to get reliable estimates about the performance of the models.']\n",
            "Precision@5: 0.12\n",
            "Recall@5: 0.44\n",
            "Accuracy@5: 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Define a function to retrieve the most relevant paragraphs from a new paper\n",
        "def retrieve_from_paper(question, paper_text, tfidf_vectorizer, k):\n",
        "    # Split the paper into paragraphs\n",
        "    paragraphs = [p for p in paper_text if p.strip()]  # Filter out empty paragraphs\n",
        "\n",
        "    # Transform each paragraph using the fitted TF-IDF vectorizer\n",
        "    paragraph_tfidf = tfidf_vectorizer.transform(paragraphs)\n",
        "\n",
        "    # Transform the question\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "\n",
        "    # Calculate cosine similarity between question and each paragraph\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "\n",
        "    # Rank paragraphs by similarity\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices[:k]]\n",
        "\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Evaluation metrics calculation for a range of k\n",
        "def calculate_metrics_for_k_range(test_data, tfidf_vectorizer, k_values):\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    accuracy_scores = []\n",
        "\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "\n",
        "        for index, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']\n",
        "            relevant_paragraphs = row['relevant_paragraphs']  # Expected relevant paragraphs (list of strings)\n",
        "\n",
        "            # Retrieve the top-k most relevant paragraphs\n",
        "            ranked_paragraphs = retrieve_from_paper(question, full_paper, tfidf_vectorizer, k)\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]  # Get only the text of the paragraphs\n",
        "\n",
        "            # Calculate Precision@k\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "\n",
        "            # Calculate Recall@k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "\n",
        "            # Calculate Accuracy@k (1 if at least one relevant paragraph is in top-k, else 0)\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "\n",
        "        # Average the metrics over all questions\n",
        "        precision_at_k /= total_questions\n",
        "        recall_at_k /= total_questions\n",
        "        accuracy_at_k /= total_questions\n",
        "\n",
        "        # Append scores for the current k\n",
        "        precision_scores.append(precision_at_k)\n",
        "        recall_scores.append(recall_at_k)\n",
        "        accuracy_scores.append(accuracy_at_k)\n",
        "\n",
        "    # Plotting the metrics across different values of k\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, precision_scores, label='Precision@k', marker='o')\n",
        "    plt.plot(k_values, recall_scores, label='Recall@k', marker='o')\n",
        "    plt.plot(k_values, accuracy_scores, label='Accuracy@k', marker='o')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Metrics Across Different Values of k')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `test_data` is your test DataFrame with columns 'question', 'full_paper', and 'relevant_paragraphs'\n",
        "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Define the range of k values you want to test\n",
        "calculate_metrics_for_k_range(test_data, tfidf_vectorizer, k_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1tZx_efSfa0_",
        "outputId": "2ba8990e-017a-4162-ae77-2c3f30710fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25"
      ],
      "metadata": {
        "id": "93AxxbzVhIvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to retrieve the most relevant paragraphs within each paper independently using BM25\n",
        "def retrieve_from_paper_bm25(question, paper_paragraphs, k=5):\n",
        "    # Tokenize each paragraph in the paper\n",
        "    tokenized_paragraphs = [paragraph.split() for paragraph in paper_paragraphs]\n",
        "\n",
        "    # Initialize BM25 for this specific paper\n",
        "    bm25 = BM25Okapi(tokenized_paragraphs)\n",
        "\n",
        "    # Tokenize the question\n",
        "    tokenized_question = question.split()\n",
        "\n",
        "    # Get BM25 scores for each paragraph\n",
        "    scores = bm25.get_scores(tokenized_question)\n",
        "\n",
        "    # Rank paragraphs by score\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    ranked_paragraphs = [(paper_paragraphs[i], scores[i]) for i in ranked_indices]\n",
        "\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Evaluation metrics calculation for a range of k\n",
        "def calculate_metrics_for_k_range_bm25(test_data, k_values):\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    accuracy_scores = []\n",
        "\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "\n",
        "        for index, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']  # A list of paragraphs for this test paper\n",
        "            relevant_paragraphs = row['relevant_paragraphs']  # Expected relevant paragraphs (list of strings)\n",
        "\n",
        "            # Retrieve the top-k most relevant paragraphs for the question within this paper\n",
        "            try:\n",
        "                ranked_paragraphs = retrieve_from_paper_bm25(question, full_paper, k=k)\n",
        "            except ValueError as e:\n",
        "                print(f\"Error in row {index}: {e}\")\n",
        "                continue  # Skip this entry if there is a mismatch\n",
        "\n",
        "            # Get only the text of the retrieved paragraphs\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]\n",
        "\n",
        "            # Calculate Precision@k\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "\n",
        "            # Calculate Recall@k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "\n",
        "            # Calculate Accuracy@k (1 if at least one relevant paragraph is in top-k, else 0)\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "\n",
        "        # Average the metrics over all questions\n",
        "        precision_at_k /= total_questions\n",
        "        recall_at_k /= total_questions\n",
        "        accuracy_at_k /= total_questions\n",
        "\n",
        "        # Append scores for the current k\n",
        "        precision_scores.append(precision_at_k)\n",
        "        recall_scores.append(recall_at_k)\n",
        "        accuracy_scores.append(accuracy_at_k)\n",
        "\n",
        "    # Plotting the metrics across different values of k\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, precision_scores, label='Precision@k', marker='o')\n",
        "    plt.plot(k_values, recall_scores, label='Recall@k', marker='o')\n",
        "    plt.plot(k_values, accuracy_scores, label='Accuracy@k', marker='o')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Metrics Across Different Values of k (BM25)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `test_data` is your test DataFrame with columns 'question', 'full_paper', and 'relevant_paragraphs'\n",
        "# 'full_paper' should be a list of paragraphs for each paper\n",
        "# 'relevant_paragraphs' should be a list of strings representing the relevant paragraphs\n",
        "test_data = test_data  # Replace with your actual test dataset\n",
        "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Define the range of k values you want to test\n",
        "calculate_metrics_for_k_range_bm25(test_data, k_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "_1eXCNl_lGe8",
        "outputId": "5a5e95f2-168c-4009-b588-2bc43a6bc794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Load the trained TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Define a function to retrieve relevant paragraphs with TF-IDF\n",
        "def retrieve_from_paper_tfidf(question, paper_text, tfidf_vectorizer, k):\n",
        "    paragraphs = [p for p in paper_text if p.strip()]  # Filter out empty paragraphs\n",
        "    paragraph_tfidf = tfidf_vectorizer.transform(paragraphs)\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices[:k]]\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Define a function to retrieve relevant paragraphs with BM25\n",
        "def retrieve_from_paper_bm25(question, paper_paragraphs, k=5):\n",
        "    tokenized_paragraphs = [paragraph.split() for paragraph in paper_paragraphs]\n",
        "    bm25 = BM25Okapi(tokenized_paragraphs)\n",
        "    tokenized_question = question.split()\n",
        "    scores = bm25.get_scores(tokenized_question)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    ranked_paragraphs = [(paper_paragraphs[i], scores[i]) for i in ranked_indices]\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Calculate metrics for TF-IDF\n",
        "def calculate_metrics_for_k_range_tfidf(test_data, tfidf_vectorizer, k_values):\n",
        "    precision_scores, recall_scores, accuracy_scores = [], [], []\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "        for _, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']\n",
        "            relevant_paragraphs = row['relevant_paragraphs']\n",
        "            ranked_paragraphs = retrieve_from_paper_tfidf(question, full_paper, tfidf_vectorizer, k)\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "        precision_scores.append(precision_at_k / total_questions)\n",
        "        recall_scores.append(recall_at_k / total_questions)\n",
        "        accuracy_scores.append(accuracy_at_k / total_questions)\n",
        "    return precision_scores, recall_scores, accuracy_scores\n",
        "\n",
        "# Calculate metrics for BM25\n",
        "def calculate_metrics_for_k_range_bm25(test_data, k_values):\n",
        "    precision_scores, recall_scores, accuracy_scores = [], [], []\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "        for _, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']\n",
        "            relevant_paragraphs = row['relevant_paragraphs']\n",
        "            try:\n",
        "                ranked_paragraphs = retrieve_from_paper_bm25(question, full_paper, k=k)\n",
        "            except ValueError as e:\n",
        "                print(f\"Error in row {_}: {e}\")\n",
        "                continue\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "        precision_scores.append(precision_at_k / total_questions)\n",
        "        recall_scores.append(recall_at_k / total_questions)\n",
        "        accuracy_scores.append(accuracy_at_k / total_questions)\n",
        "    return precision_scores, recall_scores, accuracy_scores\n",
        "\n",
        "# Combined plotting function for TF-IDF and BM25\n",
        "def plot_combined_metrics(test_data, tfidf_vectorizer, k_values):\n",
        "    # Calculate metrics for TF-IDF\n",
        "    tfidf_precision, tfidf_recall, tfidf_accuracy = calculate_metrics_for_k_range_tfidf(test_data, tfidf_vectorizer, k_values)\n",
        "\n",
        "    # Calculate metrics for BM25\n",
        "    bm25_precision, bm25_recall, bm25_accuracy = calculate_metrics_for_k_range_bm25(test_data, k_values)\n",
        "\n",
        "    # Plotting the metrics\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Precision@k Plot\n",
        "    plt.plot(k_values, tfidf_precision, label='TF-IDF Precision@k', marker='o', color='blue')\n",
        "    plt.plot(k_values, bm25_precision, label='BM25 Precision@k', marker='o', color='green')\n",
        "\n",
        "    # Recall@k Plot\n",
        "    plt.plot(k_values, tfidf_recall, label='TF-IDF Recall@k', marker='o', color='orange')\n",
        "    plt.plot(k_values, bm25_recall, label='BM25 Recall@k', marker='o', color='red')\n",
        "\n",
        "    # Accuracy@k Plot\n",
        "    plt.plot(k_values, tfidf_accuracy, label='TF-IDF Accuracy@k', marker='o', color='purple')\n",
        "    plt.plot(k_values, bm25_accuracy, label='BM25 Accuracy@k', marker='o', color='brown')\n",
        "\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Metrics Across Different Values of k (TF-IDF vs BM25)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `test_data` is your test DataFrame with columns 'question', 'full_paper', and 'relevant_paragraphs'\n",
        "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Define the range of k values you want to test\n",
        "plot_combined_metrics(test_data, tfidf_vectorizer, k_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "JXmseh1AocY_",
        "outputId": "a03fe704-a4fc-4402-ca04-201036e33581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fairer Comparison"
      ],
      "metadata": {
        "id": "noDScmFHw7Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to initialize TF-IDF for each paper and retrieve relevant paragraphs\n",
        "def retrieve_from_paper_tfidf_independent(question, paper_text, k):\n",
        "    # Initialize TF-IDF vectorizer for this paper independently\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    paragraphs = [p for p in paper_text if p.strip()]  # Filter out empty paragraphs\n",
        "    paragraph_tfidf = tfidf_vectorizer.fit_transform(paragraphs)\n",
        "\n",
        "    # Transform the question using the same vectorizer\n",
        "    question_vec = tfidf_vectorizer.transform([question])\n",
        "    similarities = cosine_similarity(question_vec, paragraph_tfidf).flatten()\n",
        "\n",
        "    # Rank paragraphs by similarity\n",
        "    ranked_indices = similarities.argsort()[::-1]\n",
        "    ranked_paragraphs = [(paragraphs[i], similarities[i]) for i in ranked_indices[:k]]\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Define a function to retrieve relevant paragraphs with BM25\n",
        "def retrieve_from_paper_bm25(question, paper_paragraphs, k=5):\n",
        "    tokenized_paragraphs = [paragraph.split() for paragraph in paper_paragraphs]\n",
        "    bm25 = BM25Okapi(tokenized_paragraphs)\n",
        "    tokenized_question = question.split()\n",
        "    scores = bm25.get_scores(tokenized_question)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    ranked_paragraphs = [(paper_paragraphs[i], scores[i]) for i in ranked_indices]\n",
        "    return ranked_paragraphs\n",
        "\n",
        "# Calculate metrics for independent TF-IDF on each paper\n",
        "def calculate_metrics_for_k_range_tfidf_independent(test_data, k_values):\n",
        "    precision_scores, recall_scores, accuracy_scores = [], [], []\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "        for _, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']\n",
        "            relevant_paragraphs = row['relevant_paragraphs']\n",
        "            ranked_paragraphs = retrieve_from_paper_tfidf_independent(question, full_paper, k)\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "        precision_scores.append(precision_at_k / total_questions)\n",
        "        recall_scores.append(recall_at_k / total_questions)\n",
        "        accuracy_scores.append(accuracy_at_k / total_questions)\n",
        "    return precision_scores, recall_scores, accuracy_scores\n",
        "\n",
        "# Calculate metrics for BM25\n",
        "def calculate_metrics_for_k_range_bm25(test_data, k_values):\n",
        "    precision_scores, recall_scores, accuracy_scores = [], [], []\n",
        "    for k in k_values:\n",
        "        total_questions = len(test_data)\n",
        "        precision_at_k = 0\n",
        "        recall_at_k = 0\n",
        "        accuracy_at_k = 0\n",
        "        for _, row in test_data.iterrows():\n",
        "            question = row['question']\n",
        "            full_paper = row['full_paper']\n",
        "            relevant_paragraphs = row['relevant_paragraphs']\n",
        "            try:\n",
        "                ranked_paragraphs = retrieve_from_paper_bm25(question, full_paper, k=k)\n",
        "            except ValueError as e:\n",
        "                print(f\"Error in row {_}: {e}\")\n",
        "                continue\n",
        "            retrieved_paragraphs = [para[0] for para in ranked_paragraphs]\n",
        "            relevant_retrieved = [para for para in retrieved_paragraphs if para in relevant_paragraphs]\n",
        "            precision_at_k += len(relevant_retrieved) / k\n",
        "            recall_at_k += len(relevant_retrieved) / len(relevant_paragraphs) if len(relevant_paragraphs) > 0 else 0\n",
        "            if any(para in relevant_paragraphs for para in retrieved_paragraphs):\n",
        "                accuracy_at_k += 1\n",
        "        precision_scores.append(precision_at_k / total_questions)\n",
        "        recall_scores.append(recall_at_k / total_questions)\n",
        "        accuracy_scores.append(accuracy_at_k / total_questions)\n",
        "    return precision_scores, recall_scores, accuracy_scores\n",
        "\n",
        "# Combined plotting function for independent TF-IDF and BM25\n",
        "def plot_combined_metrics(test_data, k_values):\n",
        "    # Calculate metrics for independent TF-IDF\n",
        "    tfidf_precision, tfidf_recall, tfidf_accuracy = calculate_metrics_for_k_range_tfidf_independent(test_data, k_values)\n",
        "\n",
        "    # Calculate metrics for BM25\n",
        "    bm25_precision, bm25_recall, bm25_accuracy = calculate_metrics_for_k_range_bm25(test_data, k_values)\n",
        "\n",
        "    # Plotting the metrics\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Precision@k Plot\n",
        "    plt.plot(k_values, tfidf_precision, label='TF-IDF Precision@k', marker='o', color='blue')\n",
        "    plt.plot(k_values, bm25_precision, label='BM25 Precision@k', marker='o', color='green')\n",
        "\n",
        "    # Recall@k Plot\n",
        "    plt.plot(k_values, tfidf_recall, label='TF-IDF Recall@k', marker='o', color='orange')\n",
        "    plt.plot(k_values, bm25_recall, label='BM25 Recall@k', marker='o', color='red')\n",
        "\n",
        "    # Accuracy@k Plot\n",
        "    plt.plot(k_values, tfidf_accuracy, label='TF-IDF Accuracy@k', marker='o', color='purple')\n",
        "    plt.plot(k_values, bm25_accuracy, label='BM25 Accuracy@k', marker='o', color='brown')\n",
        "\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Metrics Across Different Values of k (TF-IDF vs BM25)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `test_data` is your test DataFrame with columns 'question', 'full_paper', and 'relevant_paragraphs'\n",
        "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Define the range of k values you want to test\n",
        "plot_combined_metrics(test_data, k_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "ajMurUSew67u",
        "outputId": "4119123d-22f5-4722-9320-1ac48625b1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Passage Retrieval"
      ],
      "metadata": {
        "id": "ZNB-IaPGxbYS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFmKEqGYyD3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}